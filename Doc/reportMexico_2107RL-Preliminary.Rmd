---
output: 
  bookdown::pdf_document2:
    number_sections: yes
    toc: no
    includes: 
      in_header: yaml/header.tex
  bookdown::word_document2:
    reference_docx: template/report_template_DRAFT_Rmarkdown.docx
csl: csl/ices-journal-of-marine-science.csl
bibliography: bib/ast_bib.bib
---
```{r load-libraries, echo=F, error=F, message=F, warning=F}
# Install and load pacman (library management package)
if (!require("pacman")) install.packages("pacman")

# Install and load required packages from CRAN ---------------------------------
pacman::p_load(tidyverse,swfscMisc,readr,pander,kableExtra,bookdown,mapview,rgeos,
               knitr,ggmap,maps,readxl,RSQLite,shadowtext,xml2,sf,odbc,ggspatial,
               maptools,png,grid,gridExtra,cowplot,flextable,fs,magick,ftExtra,
               stringr,xtable,devtools,gdata,reshape2,lubridate,rworldmap,
               scatterpie,forcats,here,viridis,rnaturalearth,rworldxtra)

# Install and load required packages from Github -------------------------------
# surveyR
pacman::p_load_gh("kstierhoff/surveyR")
# atm
pacman::p_load_gh("kstierhoff/atm")
# rnaturalearth data
pacman::p_load_gh("ropenscilabs/rnaturalearthdata")
pacman::p_load_gh("ropenscilabs/rnaturalearthhires")

# Define method of table generation (whether kable or xtable) for best formatting
doc.type <- knitr::opts_knit$get('rmarkdown.pandoc.to')
doc.name <- knitr::current_input()
if (is.null(doc.type)) {doc.type <- "html"}
if (is.null(doc.name)) {doc.name <- "reportMexico.Rmd"}

# global knitr chunk options
knitr::opts_chunk$set(echo = F, warning = F, message = F,progress = T,
                      fig.align = 'center',dev = "png",
                      dev.args = list(type = "cairo"),dpi = 150)

# determine global knitr table format
if (doc.type == "latex") {
  knitr.format <- "latex"
} else {
  knitr.format <- "html" 
}

# set global knitr table format
options(knitr.table.format = knitr.format,knitr.kable.NA = '')

# global pander options
panderOptions('table.style','rmarkdown'); panderOptions('table.split.table', Inf); panderOptions('digits', 6);
panderOptions('round', 6); panderOptions('keep.trailing.zeros', T); panderOptions('missing', "")

# Register Google Maps API
register_google(key = google_map_api)
```

```{r user-input,include=FALSE}
# Get project name from directory
prj.name <- last(unlist(str_split(here(), "/")))

# Get all settings files
settings.files <- dir(here("Doc/settings"))

# Source survey settings file
prj.settings <- settings.files[str_detect(settings.files, paste0("settings_", prj.name, ".R"))]
source(here("Doc/settings", prj.settings))

# Define ggplot theme
theme_set(theme_bw())
```

```{r controls, echo=F, error=F, message=F, warning=F}
# Processing instructions (T/F)
copy.files      <- F # copy files from source
copy.bib        <- F # copy files from source
overwrite.csv   <- T # overwrite CSV files when copying
overwrite.files <- T # overwrite files when copying
download.hab    <- F # download habitat map
save.figs       <- F # draw plots and maps, or use existing
get.db          <- F # import database data
process.scs     <- F # process SCS logs, or load processed data
get.nav         <- F # download nave data from ERDDAP
process.csv     <- F # process CSV files from Echoview
process.csv.all <- F # Process all CSV files (F = only new files)
process.cal     <- F # process calibration results from previous surveys for time series plots
process.zmux    <- F # process impedance data
resize.map      <- F # Resize map during survey; if T, uses anticipated bounds of survey area

calc.raw.size   <- F # computer RAW file size, or use existing
```

```{r copy-bib,include=F}
if (copy.bib) {
  # Update bibliography and CSL
  file_copy("//swc-storage1/ast1/LITERATURE/Rmarkdown/csl/ices-journal-of-marine-science.csl",
            "csl", overwrite = T)
  file_copy("//swc-storage1/AST1/LITERATURE/Rmarkdown/bib/ast_bib.bib",
            "bib", overwrite = T)
}
```

```{r copy-files, include=F}
if (copy.files) {
  # Create data directories
  dir_create(here("Data", c("Backscatter","CUFES","Trawl")))
  dir_create(here("Data/Backscatter", nasc.vessels))
  
  # Copy trawl Access database
  haul.db <- dir_ls(file.path(survey.dir[survey.vessel.primary], "DATA/BIOLOGICAL/HAUL"),
                    regexp = trawl.db.access)
  file_copy(haul.db, here("Data/Trawl"), overwrite = overwrite.files)
  
  # Copy CUFES files
  cufes.file <- dir_ls(file.path(survey.dir[survey.vessel.primary], "DATA/BIOLOGICAL/CUFES"), 
                       regexp = cufes.db.sqlite)
  file_copy(cufes.file, here("Data/CUFES"), overwrite = overwrite.files)
  
  # Copy CSV files for CPS
  csv.files.cps <- dir_ls(file.path(survey.dir[survey.vessel.primary], 
                                    nasc.dir[survey.vessel.primary]), 
                          regexp = nasc.pattern.cps[survey.vessel.primary],
                          ignore.case = TRUE)
  file_copy(csv.files.cps, here("Data/Backscatter", survey.vessel.primary), 
            overwrite = overwrite.csv)
}
```

```{r download-habitat-maps}
if (download.hab) {
  source(here("Code/plot_sardine_habitat.R"))
}
```

```{r process-cal-all}
if (process.cal) {
  source(here("Code/extractCalAll.R"))
} else {
  load(here("Data/Calibration/cal_results_all.Rdata"))
}

if (save.figs) {
  source(here("Code/plot_CalTimeSeries.R"))
}
```  

```{r plot-cal-fm}
if (process.cal) {
  if (save.figs) {
    # Plot FM calibration results
    source(here("Code/plot_CalFM_2107RL.R"))
  }  
}
```

```{r process-cal}
# Get results from current survey ---------------------------------------------
cal.res <- cal.res.all %>% 
  mutate(cal_date = mdy(cal_date)) %>% 
  filter(between(cal_date,
                 ymd(cal.plot.date) - days(cal.window),
                 ymd(cal.plot.date) + days(cal.window))) %>% 
  arrange(txdr_freq)


# create data frame for calibration parameters
cal.params <- cal.res %>% 
  select(txdr_freq,txdr_type,txdr_sn,gpt_power,gpt_pd,
         txdr_gain,txdr_sa_corr,gpt_rcr_bw,gpt_si,
         txdr_2way_ba,env_alpha,txdr_alon_ang_sens,
         txdr_athw_ang_sens,txdr_alon_ba,txdr_athw_ba,
         txdr_alon_oa,txdr_athw_oa,target_ts) 

# add noise estimates
# if noise isn't measured, enter -999 in the User Inputs, else use noise estimates
if (is.na(cal.noise[survey.vessel.primary])) { 
  cal.params$noise <- rep("N/A", length(cal.files))
}else{
  cal.params$noise <- unlist(cal.noise[survey.vessel.primary])
}

# create data frame for beam model results
bm.res <- cal.res %>% 
  select(txdr_freq,bm_txdr_gain,bm_sa_corr,dev_bm_rms,
         bm_alon_ba,bm_athw_ba,bm_alon_oa,bm_athw_oa) 

# create a data frame for echosounder settings
echo.settings <- cal.res %>% 
  select(txdr_freq,gpt_pd,gpt_si,gpt_rcr_bw,gpt_power,txdr_z,env_alpha) 

# create names for cal parameters and beam model results
names(cal.params) <- c("Frequency","Model","Serial Number","Transmit Power ($p_\\mathrm{et}$)",
                       "Pulse Duration ($\\tau$)","On-axis Gain ($G_0$)",
                       "$S_\\mathrm{a}$ Correction ($S_\\mathrm{a}\\mathrm{corr}$)",
                       "Bandwidth ($W_\\mathrm{f}$)","Sample Interval",
                       "Eq. Two-way Beam Angle ($\\mathrm{\\Psi}$)",
                       "Absorption Coefficient ($\\alpha_\\mathrm{f}$)",
                       "Angle Sensitivity Along. ($\\mathrm{\\Lambda}_{\\alpha}$)",
                       "Angle Sensitivity Athw. ($\\mathrm{\\Lambda}_{\\beta}$)",
                       "3-dB Beamwidth Along. ($\\alpha_\\mathrm{-3dB}$)",
                       "3-dB Beamwidth Athw. ($\\beta_\\mathrm{-3dB}$)",
                       "Angle Offset Along. ($\\alpha_{0}$)","Angle Offset Athw. ($\\beta_{0}$)",
                       "Theoretical TS ($TS_\\mathrm{theory}$)",
                       "Ambient Noise")

names(bm.res) <- c("Frequency","On-axis Gain ($G_0$)",
                   "$S_\\mathrm{a}$ Correction ($S_\\mathrm{a}\\mathrm{corr}$)","RMS",
                   "3-dB Beamwidth Along. ($\\alpha_\\mathrm{-3dB}$)",
                   "3-dB Beamwidth Athw. ($\\beta_\\mathrm{-3dB}$)",
                   "Angle Offset Along. ($\\alpha_{0}$)",
                   "Angle Offset Athw. ($\\beta_{0}$)")

names(echo.settings) <- c("Frequency","Pulse Duration ($\\mu s$)","Sample Interval (m)",
                          "Bandwidth (Hz)","Transmit Power (W)",
                          "Transducer Depth (m)","Absorption Coefficient (dB km$\\^{-1}$)")

# cast output by frequency and transducer model number
param.output  <- suppressMessages(dcast(melt(cal.params, id.vars = "Frequency"), variable~Frequency))
bm.output     <- suppressMessages(dcast(melt(bm.res, id.vars = "Frequency"), variable~Frequency))

# add a column with parameter units
param.units <- data.frame(Units = c(" "," ","W","ms","dB re 1","dB re 1","Hz","m","dB re 1 sr","dB km$^{-1}$",
                                    "Elec.$^\\circ$/Geom.$^\\circ$","Elec.$^\\circ$/Geom.$^\\circ$",
                                    "deg","deg","deg","deg","dB re 1 m$^{2}$","dB re 1 W"))

# add a column with beam model units
bm.units <- data.frame(Units = c("dB re 1","dB re 1","dB","deg","deg","deg","deg"))

# add units to output and arrange columns
param.output <- bind_cols(param.output, param.units) %>% 
  select(variable, Units, everything()) %>% 
  rename("Frequency ($f$, kHz)" = variable)

bm.output <- bind_cols(bm.output, bm.units) %>% 
  select(variable, Units, everything()) %>% 
  rename("Frequency ($f$, kHz)" = variable)

# combine results data frames
all.output <- rbind(param.output, bm.output) %>% 
  rename(" " = "Frequency ($f$, kHz)")

# save output to .Rdata and CSV
save(all.output,
     file = here("Output/cal_output_table.Rdata"))

write.csv(all.output,
          file = here("Output/cal_output_table.csv"), 
          quote = F, row.names = F)
```

```{r process-cal-carnage}
if ("LBC" %in% cal.vessels) {
  
  # Create output directory ------------------------------------------------------
  dir_create(here("Output/Calibration/LBC"))
  
  # Calibration directory -----------------------------------------------
  dir.lbc <- here("Data/Calibration/LBC")
  
  # Get calibration files
  cal.files.lbc <- sort(c(list.files(dir.lbc,pattern = ".txt", 
                                     full.names = TRUE),
                          list.files(dir.lbc,pattern = ".xml", 
                                     full.names = TRUE)))
  
  # set survey info
  survey.vessel.temp   <- "LBC"
  survey.name.temp     <- survey.name
  cal.group.temp       <- cal.group
  
  # Create data frames for storing results
  cal.res.lbc   <- data.frame()
  cal.info.lbc  <- data.frame()
  cal.pings.lbc <- data.frame()
  
  # process all calibration results in CALIBRATION directory
  for (i in cal.files.lbc) {
    # Extract calibration results, information, and pings
    cal <- extract_cal(i, survey.vessel.temp, survey.name.temp, cal.group.temp)
    
    # combine all results
    cal.res.lbc   <- bind_rows(cal.res.lbc,   cal$cal.res)
    cal.info.lbc  <- bind_rows(cal.info.lbc,  cal$cal.info)
    cal.pings.lbc <- bind_rows(cal.pings.lbc, cal$cal.pings)
  }
  
  # Format results
  cal.res.lbc   <- arrange(cal.res.lbc, mdy(cal_date), txdr_freq)
  cal.info.lbc  <- arrange(cal.info.lbc, mdy_hms(date_time))
  cal.pings.lbc <- arrange(cal.pings.lbc, date_time)
  
  # write all results to file
  save(cal.res.lbc, cal.info.lbc, cal.pings.lbc, cal.files,
       file = here("Data/Calibration/LBC/cal_results_lbc.Rdata"))
  
  # Get results from current survey ---------------------------------------------
  
  # create data frame for calibration parameters
  cal.params.lbc <- cal.res.lbc %>% 
    select(txdr_freq,txdr_type,txdr_sn,gpt_power,gpt_pd,
           txdr_gain,txdr_sa_corr,gpt_rcr_bw,gpt_si,
           txdr_2way_ba,env_alpha,txdr_alon_ang_sens,
           txdr_athw_ang_sens,txdr_alon_ba,txdr_athw_ba,
           txdr_alon_oa,txdr_athw_oa,target_ts) 
  
  # add noise estimates
  # if noise isn't measured, enter -999 in the User Inputs, else use noise estimates
  if (is.na(cal.noise["LBC"])) { 
    cal.params.lbc$noise <- rep("N/A", length(cal.files.lbc))
  } else {
    cal.params.lbc$noise <- unlist(cal.noise[cal.vessels])
  }
  
  # create data frame for beam model results
  bm.res.lbc <- cal.res.lbc %>% 
    select(txdr_freq,bm_txdr_gain,bm_sa_corr,dev_bm_rms,
           bm_alon_ba,bm_athw_ba,bm_alon_oa,bm_athw_oa) 
  
  # create a data frame for echosounder settings
  echo.settings.lbc <- cal.res.lbc %>% 
    select(txdr_freq,gpt_pd,gpt_si,gpt_rcr_bw,gpt_power,txdr_z,env_alpha) 
  
  # create names for cal parameters and beam model results
  names(cal.params.lbc) <- c("Frequency","Model","Serial Number","Transmit Power ($p_\\mathrm{et}$)",
                             "Pulse Duration ($\\tau$)","On-axis Gain ($G_0$)",
                             "$S_\\mathrm{a}$ Correction ($S_\\mathrm{a}\\mathrm{corr}$)",
                             "Bandwidth ($W_\\mathrm{f}$)","Sample Interval",
                             "Eq. Two-way Beam Angle ($\\mathrm{\\Psi}$)",
                             "Absorption Coefficient ($\\alpha_\\mathrm{f}$)",
                             "Angle Sensitivity Along. ($\\mathrm{\\Lambda}_{\\alpha}$)",
                             "Angle Sensitivity Athw. ($\\mathrm{\\Lambda}_{\\beta}$)",
                             "3-dB Beamwidth Along. ($\\alpha_\\mathrm{-3dB}$)",
                             "3-dB Beamwidth Athw. ($\\beta_\\mathrm{-3dB}$)",
                             "Angle Offset Along. ($\\alpha_{0}$)","Angle Offset Athw. ($\\beta_{0}$)",
                             "Theoretical TS ($TS_\\mathrm{theory}$)",
                             "Ambient Noise")
  
  names(bm.res.lbc) <- c("Frequency","On-axis Gain ($G_0$)",
                         "$S_\\mathrm{a}$ Correction ($S_\\mathrm{a}\\mathrm{corr}$)","RMS",
                         "3-dB Beamwidth Along. ($\\alpha_\\mathrm{-3dB}$)",
                         "3-dB Beamwidth Athw. ($\\beta_\\mathrm{-3dB}$)",
                         "Angle Offset Along. ($\\alpha_{0}$)",
                         "Angle Offset Athw. ($\\beta_{0}$)")
  
  names(echo.settings.lbc) <- c("Frequency","Pulse Duration ($\\mu s$)","Sample Interval (m)",
                                "Bandwidth (Hz)","Transmit Power (W)",
                                "Transducer Depth (m)","Absorption Coefficient (dB km$\\^{-1}$)")
  
  # cast output by frequency and transducer model number
  param.output.lbc  <- suppressMessages(dcast(melt(cal.params.lbc, id.vars = "Frequency"), variable~Frequency))
  bm.output.lbc     <- suppressMessages(dcast(melt(bm.res.lbc, id.vars = "Frequency"), variable~Frequency))
  
  # add a column with parameter units
  param.units.lbc <- data.frame(Units = c(" "," ","W","ms","dB re 1","dB re 1","Hz","m","dB re 1 sr","dB km$^{-1}$",
                                          "Elec.$^\\circ$/Geom.$^\\circ$","Elec.$^\\circ$/Geom.$^\\circ$",
                                          "deg","deg","deg","deg","dB re 1 m$^{2}$","dB re 1 W"))
  
  # add a column with beam model units
  bm.units.lbc <- data.frame(Units = c("dB re 1","dB re 1","dB","deg","deg","deg","deg"))
  
  # add units to output and arrange columns
  param.output.lbc <- bind_cols(param.output.lbc, param.units.lbc) %>% 
    select(variable, Units, everything()) %>% 
    rename("Frequency ($f$, kHz)" = variable)
  
  bm.output.lbc <- bind_cols(bm.output.lbc, bm.units.lbc) %>% 
    select(variable, Units, everything()) %>% 
    rename("Frequency ($f$, kHz)" = variable)
  
  # combine results data frames
  all.output.lbc <- rbind(param.output.lbc, bm.output.lbc) %>% 
    rename(" " = "Frequency ($f$, kHz)")
  
  # save output to .Rdata and CSV
  save(all.output.lbc,
       file = here("Output/cal_output_table_LBC.Rdata"))
  
  write_csv(all.output,
            file = here("Output/cal_output_table_LBC.csv"))
}
```

```{r process-cal-lisa-marie}
if ("LM" %in% cal.vessels) {
  
  # Create output directory ------------------------------------------------------
  dir_create(here("Output/Calibration/LM"))
  
  # Calibration directory -----------------------------------------------
  dir.lm <- here("Data/Calibration/LM")
  
  # Get calibration files
  cal.files.lm <- sort(c(list.files(dir.lm,pattern = ".txt", 
                                     full.names = TRUE),
                          list.files(dir.lm,pattern = ".xml", 
                                     full.names = TRUE)))
  
  # set survey info
  survey.vessel.temp   <- "LM"
  survey.name.temp     <- survey.name
  cal.group.temp       <- cal.group
  
  # Create data frames for storing results
  cal.res.lm   <- data.frame()
  cal.info.lm  <- data.frame()
  cal.pings.lm <- data.frame()
  
  # process all calibration results in CALIBRATION directory
  for (i in cal.files.lm) {
    # Extract calibration results, information, and pings
    cal <- extract_cal(i, survey.vessel.temp, survey.name.temp, cal.group.temp)
    
    # combine all results
    cal.res.lm   <- bind_rows(cal.res.lm,   cal$cal.res)
    cal.info.lm  <- bind_rows(cal.info.lm,  cal$cal.info)
    cal.pings.lm <- bind_rows(cal.pings.lm, cal$cal.pings)
  }
  
  # Format results
  cal.res.lm   <- arrange(cal.res.lm, mdy(cal_date), txdr_freq)
  cal.info.lm  <- arrange(cal.info.lm, mdy_hms(date_time))
  cal.pings.lm <- arrange(cal.pings.lm, date_time)
  
  # write all results to file
  save(cal.res.lm, cal.info.lm, cal.pings.lm, cal.files,
       file = here("Data/Calibration/LBC/cal_results_lm.Rdata"))
  
  # Get results from current survey ---------------------------------------------
  
  # create data frame for calibration parameters
  cal.params.lm <- cal.res.lm %>% 
    select(txdr_freq,txdr_type,txdr_sn,gpt_power,gpt_pd,
           txdr_gain,txdr_sa_corr,gpt_rcr_bw,gpt_si,
           txdr_2way_ba,env_alpha,txdr_alon_ang_sens,
           txdr_athw_ang_sens,txdr_alon_ba,txdr_athw_ba,
           txdr_alon_oa,txdr_athw_oa,target_ts) 
  
  # add noise estimates
  # if noise isn't measured, enter -999 in the User Inputs, else use noise estimates
  if (is.na(cal.noise["LM"])) { 
    cal.params.lm$noise <- rep("N/A", length(cal.files.lm))
  } else {
    cal.params.lm$noise <- unlist(cal.noise[cal.vessels])
  }
  
  # create data frame for beam model results
  bm.res.lm <- cal.res.lm %>% 
    select(txdr_freq,bm_txdr_gain,bm_sa_corr,dev_bm_rms,
           bm_alon_ba,bm_athw_ba,bm_alon_oa,bm_athw_oa) 
  
  # create a data frame for echosounder settings
  echo.settings.lm <- cal.res.lm %>% 
    select(txdr_freq,gpt_pd,gpt_si,gpt_rcr_bw,gpt_power,txdr_z,env_alpha) 
  
  # create names for cal parameters and beam model results
  names(cal.params.lm) <- c("Frequency","Model","Serial Number","Transmit Power ($p_\\mathrm{et}$)",
                             "Pulse Duration ($\\tau$)","On-axis Gain ($G_0$)",
                             "$S_\\mathrm{a}$ Correction ($S_\\mathrm{a}\\mathrm{corr}$)",
                             "Bandwidth ($W_\\mathrm{f}$)","Sample Interval",
                             "Eq. Two-way Beam Angle ($\\mathrm{\\Psi}$)",
                             "Absorption Coefficient ($\\alpha_\\mathrm{f}$)",
                             "Angle Sensitivity Along. ($\\mathrm{\\Lambda}_{\\alpha}$)",
                             "Angle Sensitivity Athw. ($\\mathrm{\\Lambda}_{\\beta}$)",
                             "3-dB Beamwidth Along. ($\\alpha_\\mathrm{-3dB}$)",
                             "3-dB Beamwidth Athw. ($\\beta_\\mathrm{-3dB}$)",
                             "Angle Offset Along. ($\\alpha_{0}$)","Angle Offset Athw. ($\\beta_{0}$)",
                             "Theoretical TS ($TS_\\mathrm{theory}$)",
                             "Ambient Noise")
  
  names(bm.res.lm) <- c("Frequency","On-axis Gain ($G_0$)",
                         "$S_\\mathrm{a}$ Correction ($S_\\mathrm{a}\\mathrm{corr}$)","RMS",
                         "3-dB Beamwidth Along. ($\\alpha_\\mathrm{-3dB}$)",
                         "3-dB Beamwidth Athw. ($\\beta_\\mathrm{-3dB}$)",
                         "Angle Offset Along. ($\\alpha_{0}$)",
                         "Angle Offset Athw. ($\\beta_{0}$)")
  
  names(echo.settings.lm) <- c("Frequency","Pulse Duration ($\\mu s$)","Sample Interval (m)",
                                "Bandwidth (Hz)","Transmit Power (W)",
                                "Transducer Depth (m)","Absorption Coefficient (dB km$\\^{-1}$)")
  
  # cast output by frequency and transducer model number
  param.output.lm  <- suppressMessages(dcast(melt(cal.params.lm, id.vars = "Frequency"), variable~Frequency))
  bm.output.lm     <- suppressMessages(dcast(melt(bm.res.lm, id.vars = "Frequency"), variable~Frequency))
  
  # add a column with parameter units
  param.units.lm <- data.frame(Units = c(" "," ","W","ms","dB re 1","dB re 1","Hz","m","dB re 1 sr","dB km$^{-1}$",
                                          "Elec.$^\\circ$/Geom.$^\\circ$","Elec.$^\\circ$/Geom.$^\\circ$",
                                          "deg","deg","deg","deg","dB re 1 m$^{2}$","dB re 1 W"))
  
  # add a column with beam model units
  bm.units.lm <- data.frame(Units = c("dB re 1","dB re 1","dB","deg","deg","deg","deg"))
  
  # add units to output and arrange columns
  param.output.lm <- bind_cols(param.output.lm, param.units.lm) %>% 
    select(variable, Units, everything()) %>% 
    rename("Frequency ($f$, kHz)" = variable)
  
  bm.output.lm <- bind_cols(bm.output.lm, bm.units.lm) %>% 
    select(variable, Units, everything()) %>% 
    rename("Frequency ($f$, kHz)" = variable)
  
  # combine results data frames
  all.output.lm <- rbind(param.output.lm, bm.output.lm) %>% 
    rename(" " = "Frequency ($f$, kHz)")
  
  # save output to .Rdata and CSV
  save(all.output.lm,
       file = here("Output/cal_output_table_LM.Rdata"))
  
  write_csv(all.output,
            file = here("Output/cal_output_table_LM.csv"))
}
```

```{r plot-cal-pings}
# Get ping data from current survey and format for plotting
cal.pings <- cal.pings.all %>% 
  mutate(cal_date = date(date_time)) %>% 
  filter(between(cal_date,
                 ymd(cal.plot.date) - days(cal.window),
                 ymd(cal.plot.date) + days(cal.window))) %>% 
  arrange(txdr_freq, ping_num)

# Set axis limits based on range of ping angles
cal.lim.tmp <- round(max(max(cal.pings$along), max(cal.pings$athw))) 

if (cal.lim.tmp %% 2) {
  # If range is odd, add 1 to make axis ticks look nice
  cal.axis.lims <- c(-(cal.lim.tmp + 1), cal.lim.tmp + 1)
} else {
  cal.axis.lims <- c(-cal.lim.tmp, cal.lim.tmp)
}

# subset only outlier points
outliers <- filter(cal.pings, outlier == 1)

cal.pings <- cal.pings %>% 
  left_join(select(cal.res,txdr_freq,txdr_type,target_ts,
                   txdr_gain,bm_txdr_gain,
                   bm_alon_ba,bm_athw_ba,
                   bm_alon_oa,bm_athw_oa)) %>% 
  mutate(
    txdr_type      = fct_reorder(txdr_type, txdr_freq),
    TS_u_new       = TS_u + 2*(txdr_gain - bm_txdr_gain),
    alpha          = along - bm_alon_oa,
    beta           = athw - bm_athw_oa,
    x              = (2*alpha) / bm_alon_ba,
    y              = (2*beta) / bm_athw_ba,
    B              = 6.0206*(x^2 + y^2 - 0.18*x^2*y^2),
    TS_c_new       = TS_u_new + B,
    relTS_c        = TS_c_new - target_ts,
    relTS_c_scaled = case_when(
      relTS_c >= 1 ~ 1,
      relTS_c <= -1 ~-1,
      between(relTS_c,-1,1) ~ relTS_c))

if (cal.scales == "fixed") {
  # Plot beam-uncompensated target strength data #####
  tsu.scatter <- ggplot(filter(cal.pings, outlier == 0), aes(athw, along)) +
    geom_point(aes(colour = TS_u)) + 
    geom_point(data = filter(cal.pings, outlier == 1), aes(athw, along), 
               shape = "+", size = 1, alpha = 0.7) +
    facet_wrap(~txdr_type, scales = cal.scales) +
    scale_colour_viridis(name = expression(paste(italic(TS)[u]," (dB)",sep = "")),
                         option = "magma") +
    scale_x_continuous('\nAthwartship Beam Angle (deg)',limits = cal.axis.lims,
                       breaks = seq(min(cal.axis.lims), max(cal.axis.lims), 2)) +
    scale_y_continuous('Alongship Beam Angle (deg)\n',limits = cal.axis.lims,
                       breaks = seq(min(cal.axis.lims), max(cal.axis.lims), 2)) +
    guides(size =  "none") + theme_bw() + 
    theme(panel.spacing    = unit(1, "lines"),
          strip.background = element_rect(fill = "white"),
          strip.text.x     = element_text(face = "bold")) + 
    coord_equal()
  
  # Plot beam-compensated target strength data #####
  tsc.scatter <- ggplot(filter(cal.pings, outlier == 0), aes(athw, along)) +
    geom_point(aes(fill = relTS_c_scaled), shape = 21) + 
    geom_point(data = filter(cal.pings, outlier == 1), aes(athw, along),
               shape = "+", size = 4) +
    facet_wrap(~txdr_type, scales = cal.scales) + 
    scale_fill_distiller(name = expression(italic(TS)[rel]),
                         type = "div", palette = "RdBu", limits = c(-1,1)) +
    scale_x_continuous('\nAthwartship Beam Angle (deg)', limits = cal.axis.lims,
                       breaks = seq(min(cal.axis.lims), max(cal.axis.lims), 2)) +
    scale_y_continuous('Alongship Beam Angle (deg)\n',limits = cal.axis.lims,
                       breaks = seq(min(cal.axis.lims), max(cal.axis.lims), 2)) +
    theme_bw() + 
    theme(panel.spacing = unit(1, "lines"),
          strip.background = element_rect(fill = "white"),
          strip.text.x = element_text(face = "bold")) +
    coord_equal()
  
} else {
  # Plot beam-uncompensated target strength data #####
  tsu.scatter <- ggplot(filter(cal.pings, outlier == 0), aes(athw, along)) +
    geom_point(aes(colour = TS_u)) + 
    geom_point(data = filter(cal.pings, outlier == 1), aes(athw, along), 
               shape = "+", size = 1, alpha = 0.7) +
    facet_wrap(~txdr_type, scales = cal.scales) +
    scale_colour_viridis(name = expression(paste(italic(TS)[u]," (dB)",sep = "")),
                         option = "magma") +
    scale_x_continuous('\nAthwartship Beam Angle (deg)') +
    scale_y_continuous('Alongship Beam Angle (deg)\n') +
    guides(size =  "none") + theme_bw() + 
    theme(panel.spacing    = unit(1, "lines"),
          strip.background = element_rect(fill = "white"),
          strip.text.x     = element_text(face = "bold")) 
  
  # Plot beam-compensated target strength data #####
  tsc.scatter <- ggplot(filter(cal.pings, outlier == 0), aes(athw, along)) +
    geom_point(aes(fill = relTS_c_scaled), shape = 21, colour = "gray70") + 
    geom_point(data = filter(cal.pings, outlier == 1), aes(athw, along),
               shape = "+", size = 4) +
    facet_wrap(~txdr_type, scales = cal.scales) + 
    scale_fill_distiller(name = expression(italic(TS)[rel]),
                         type = "div", palette = "RdBu", limits = c(-1,1)) +
    scale_x_continuous('\nAthwartship Beam Angle (deg)') +
    scale_y_continuous('Alongship Beam Angle (deg)\n') +
    theme_bw() + 
    theme(panel.spacing = unit(1, "lines"),
          strip.background = element_rect(fill = "white"),
          strip.text.x = element_text(face = "bold")) 
}

# Save TS_c plot 
ggsave(tsu.scatter, filename = here("Figs/fig_cal_TSu_scatter.png"),  
       width = 9, height = 6)

# Save TS_c plot 
ggsave(here("Figs/fig_cal_TSrel_scatter.png"), tsc.scatter,
       width = 9, height = 6)
```

```{r plot-cal-pings-lbc}
# Get ping data from current survey and format for plotting
cal.pings.lbc <- cal.pings.lbc %>% 
  mutate(cal_date = date(date_time)) %>% 
  # filter(between(cal_date,
  #                ymd(cal.plot.date) - days(cal.window),
  #                ymd(cal.plot.date) + days(cal.window))) %>% 
  arrange(txdr_freq, ping_num)

# Set axis limits based on range of ping angles
cal.lim.tmp <- round(max(max(cal.pings.lbc$along), max(cal.pings.lbc$athw))) 

if (cal.lim.tmp %% 2) {
  # If range is odd, add 1 to make axis ticks look nice
  cal.axis.lims <- c(-(cal.lim.tmp + 1), cal.lim.tmp + 1)
} else {
  cal.axis.lims <- c(-cal.lim.tmp, cal.lim.tmp)
}

# subset only outlier points
outliers <- filter(cal.pings.lbc, outlier == 1)

cal.pings.lbc <- cal.pings.lbc %>% 
  left_join(select(cal.res.lbc,txdr_freq,txdr_type,target_ts,
                   txdr_gain,bm_txdr_gain,
                   bm_alon_ba,bm_athw_ba,
                   bm_alon_oa,bm_athw_oa)) %>% 
  mutate(
    txdr_type      = fct_reorder(txdr_type, txdr_freq),
    TS_u_new       = TS_u + 2*(txdr_gain - bm_txdr_gain),
    alpha          = along - bm_alon_oa,
    beta           = athw - bm_athw_oa,
    x              = (2*alpha) / bm_alon_ba,
    y              = (2*beta) / bm_athw_ba,
    B              = 6.0206*(x^2 + y^2 - 0.18*x^2*y^2),
    TS_c_new       = TS_u_new + B,
    relTS_c        = TS_c_new - target_ts,
    relTS_c_scaled = case_when(
      relTS_c >= 1 ~ 1,
      relTS_c <= -1 ~-1,
      between(relTS_c,-1,1) ~ relTS_c))

if (cal.scales == "fixed") {
  # Plot beam-uncompensated target strength data #####
  tsu.scatter.lbc <- ggplot(filter(cal.pings.lbc, outlier == 0), aes(athw, along)) +
    geom_point(aes(colour = TS_u)) + 
    geom_point(data = filter(cal.pings.lbc, outlier == 1), aes(athw, along), 
               shape = "+", size = 1, alpha = 0.7) +
    facet_wrap(~txdr_type, scales = cal.scales) +
    scale_colour_viridis(name = expression(paste(italic(TS)[u]," (dB)",sep = "")),
                         option = "magma") +
    scale_x_continuous('\nAthwartship Beam Angle (deg)',limits = cal.axis.lims,
                       breaks = seq(min(cal.axis.lims), max(cal.axis.lims), 2)) +
    scale_y_continuous('Alongship Beam Angle (deg)\n',limits = cal.axis.lims,
                       breaks = seq(min(cal.axis.lims), max(cal.axis.lims), 2)) +
    guides(size =  "none") + theme_bw() + 
    theme(panel.spacing    = unit(1, "lines"),
          strip.background = element_rect(fill = "white"),
          strip.text.x     = element_text(face = "bold")) + 
    coord_equal()
  
  # Plot beam-compensated target strength data #####
  tsc.scatter.lbc <- ggplot(filter(cal.pings.lbc, outlier == 0), aes(athw, along)) +
    geom_point(aes(fill = relTS_c_scaled), shape = 21) + 
    geom_point(data = filter(cal.pings.lbc, outlier == 1), aes(athw, along),
               shape = "+", size = 4) +
    facet_wrap(~txdr_type, scales = cal.scales) + 
    scale_fill_distiller(name = expression(italic(TS)[rel]),
                         type = "div", palette = "RdBu", limits = c(-1,1)) +
    scale_x_continuous('\nAthwartship Beam Angle (deg)', limits = cal.axis.lims,
                       breaks = seq(min(cal.axis.lims), max(cal.axis.lims), 2)) +
    scale_y_continuous('Alongship Beam Angle (deg)\n',limits = cal.axis.lims,
                       breaks = seq(min(cal.axis.lims), max(cal.axis.lims), 2)) +
    theme_bw() + 
    theme(panel.spacing = unit(1, "lines"),
          strip.background = element_rect(fill = "white"),
          strip.text.x = element_text(face = "bold")) +
    coord_equal()
  
} else {
  # Plot beam-uncompensated target strength data #####
  tsu.scatter.lbc <- ggplot(filter(cal.pings.lbc, outlier == 0), aes(athw, along)) +
    geom_point(aes(colour = TS_u)) + 
    geom_point(data = filter(cal.pings.lbc, outlier == 1), aes(athw, along), 
               shape = "+", size = 1, alpha = 0.7) +
    facet_wrap(~txdr_type, scales = cal.scales) +
    scale_colour_viridis(name = expression(paste(italic(TS)[u]," (dB)",sep = "")),
                         option = "magma") +
    scale_x_continuous('\nAthwartship Beam Angle (deg)') +
    scale_y_continuous('Alongship Beam Angle (deg)\n') +
    guides(size =  "none") + theme_bw() + 
    theme(panel.spacing    = unit(1, "lines"),
          strip.background = element_rect(fill = "white"),
          strip.text.x     = element_text(face = "bold")) 
  
  # Plot beam-compensated target strength data #####
  tsc.scatter.lbc <- ggplot(filter(cal.pings.lbc, outlier == 0), aes(athw, along)) +
    geom_point(aes(fill = relTS_c_scaled), shape = 21, colour = "gray70") + 
    geom_point(data = filter(cal.pings.lbc, outlier == 1), aes(athw, along),
               shape = "+", size = 4) +
    facet_wrap(~txdr_type, scales = cal.scales) + 
    scale_fill_distiller(name = expression(italic(TS)[rel]),
                         type = "div", palette = "RdBu", limits = c(-1,1)) +
    scale_x_continuous('\nAthwartship Beam Angle (deg)') +
    scale_y_continuous('Alongship Beam Angle (deg)\n') +
    theme_bw() + 
    theme(panel.spacing = unit(1, "lines"),
          strip.background = element_rect(fill = "white"),
          strip.text.x = element_text(face = "bold")) 
}

# Save TS_u plot 
ggsave(tsu.scatter.lbc, filename = here("Figs/fig_cal_TSu_scatter_LBC.png"),  
       width = 9, height = 8)

# Save TS_c plot 
ggsave(tsc.scatter.lbc, filename = here("Figs/fig_cal_TSrel_scatter_LBC.png"), 
       width = 9, height = 8)
```

```{r plot-cal-pings-lm}
# Get ping data from current survey and format for plotting
cal.pings.lm <- cal.pings.lm %>% 
  mutate(cal_date = date(date_time)) %>% 
  # filter(between(cal_date,
  #                ymd(cal.plot.date) - days(cal.window),
  #                ymd(cal.plot.date) + days(cal.window))) %>% 
  arrange(txdr_freq, ping_num)

# Set axis limits based on range of ping angles
cal.lim.tmp <- round(max(max(cal.pings.lm$along), max(cal.pings.lm$athw))) 

if (cal.lim.tmp %% 2) {
  # If range is odd, add 1 to make axis ticks look nice
  cal.axis.lims <- c(-(cal.lim.tmp + 1), cal.lim.tmp + 1)
} else {
  cal.axis.lims <- c(-cal.lim.tmp, cal.lim.tmp)
}

# subset only outlier points
outliers <- filter(cal.pings.lm, outlier == 1)

cal.pings.lm <- cal.pings.lm %>% 
  left_join(select(cal.res.lm,txdr_freq,txdr_type,target_ts,
                   txdr_gain,bm_txdr_gain,
                   bm_alon_ba,bm_athw_ba,
                   bm_alon_oa,bm_athw_oa)) %>% 
  mutate(
    txdr_type      = fct_reorder(txdr_type, txdr_freq),
    TS_u_new       = TS_u + 2*(txdr_gain - bm_txdr_gain),
    alpha          = along - bm_alon_oa,
    beta           = athw - bm_athw_oa,
    x              = (2*alpha) / bm_alon_ba,
    y              = (2*beta) / bm_athw_ba,
    B              = 6.0206*(x^2 + y^2 - 0.18*x^2*y^2),
    TS_c_new       = TS_u_new + B,
    relTS_c        = TS_c_new - target_ts,
    relTS_c_scaled = case_when(
      relTS_c >= 1 ~ 1,
      relTS_c <= -1 ~-1,
      between(relTS_c,-1,1) ~ relTS_c))

if (cal.scales == "fixed") {
  # Plot beam-uncompensated target strength data #####
  tsu.scatter.lm <- ggplot(filter(cal.pings.lm, outlier == 0), aes(athw, along)) +
    geom_point(aes(colour = TS_u)) + 
    geom_point(data = filter(cal.pings.lm, outlier == 1), aes(athw, along), 
               shape = "+", size = 1, alpha = 0.7) +
    facet_wrap(~txdr_type, scales = cal.scales) +
    scale_colour_viridis(name = expression(paste(italic(TS)[u]," (dB)",sep = "")),
                         option = "magma") +
    scale_x_continuous('\nAthwartship Beam Angle (deg)',limits = cal.axis.lims,
                       breaks = seq(min(cal.axis.lims), max(cal.axis.lims), 2)) +
    scale_y_continuous('Alongship Beam Angle (deg)\n',limits = cal.axis.lims,
                       breaks = seq(min(cal.axis.lims), max(cal.axis.lims), 2)) +
    guides(size =  "none") + theme_bw() + 
    theme(panel.spacing    = unit(1, "lines"),
          strip.background = element_rect(fill = "white"),
          strip.text.x     = element_text(face = "bold")) + 
    coord_equal()
  
  # Plot beam-compensated target strength data #####
  tsc.scatter.lm <- ggplot(filter(cal.pings.lm, outlier == 0), aes(athw, along)) +
    geom_point(aes(fill = relTS_c_scaled), shape = 21) + 
    geom_point(data = filter(cal.pings.lm, outlier == 1), aes(athw, along),
               shape = "+", size = 4) +
    facet_wrap(~txdr_type, scales = cal.scales) + 
    scale_fill_distiller(name = expression(italic(TS)[rel]),
                         type = "div", palette = "RdBu", limits = c(-1,1)) +
    scale_x_continuous('\nAthwartship Beam Angle (deg)', limits = cal.axis.lims,
                       breaks = seq(min(cal.axis.lims), max(cal.axis.lims), 2)) +
    scale_y_continuous('Alongship Beam Angle (deg)\n',limits = cal.axis.lims,
                       breaks = seq(min(cal.axis.lims), max(cal.axis.lims), 2)) +
    theme_bw() + 
    theme(panel.spacing = unit(1, "lines"),
          strip.background = element_rect(fill = "white"),
          strip.text.x = element_text(face = "bold")) +
    coord_equal()
  
} else {
  # Plot beam-uncompensated target strength data #####
  tsu.scatter.lm <- ggplot(filter(cal.pings.lm, outlier == 0), aes(athw, along)) +
    geom_point(aes(colour = TS_u)) + 
    geom_point(data = filter(cal.pings.lm, outlier == 1), aes(athw, along), 
               shape = "+", size = 1, alpha = 0.7) +
    facet_wrap(~txdr_type, scales = cal.scales) +
    scale_colour_viridis(name = expression(paste(italic(TS)[u]," (dB)",sep = "")),
                         option = "magma") +
    scale_x_continuous('\nAthwartship Beam Angle (deg)') +
    scale_y_continuous('Alongship Beam Angle (deg)\n') +
    guides(size =  "none") + theme_bw() + 
    theme(panel.spacing    = unit(1, "lines"),
          strip.background = element_rect(fill = "white"),
          strip.text.x     = element_text(face = "bold")) 
  
  # Plot beam-compensated target strength data #####
  tsc.scatter.lm <- ggplot(filter(cal.pings.lm, outlier == 0), aes(athw, along)) +
    geom_point(aes(fill = relTS_c_scaled), shape = 21, colour = "gray70") + 
    geom_point(data = filter(cal.pings.lm, outlier == 1), aes(athw, along),
               shape = "+", size = 4) +
    facet_wrap(~txdr_type, scales = cal.scales) + 
    scale_fill_distiller(name = expression(italic(TS)[rel]),
                         type = "div", palette = "RdBu", limits = c(-1,1)) +
    scale_x_continuous('\nAthwartship Beam Angle (deg)') +
    scale_y_continuous('Alongship Beam Angle (deg)\n') +
    theme_bw() + 
    theme(panel.spacing = unit(1, "lines"),
          strip.background = element_rect(fill = "white"),
          strip.text.x = element_text(face = "bold")) 
}

# Save TS_u plot 
ggsave(tsu.scatter.lm, filename = here("Figs/fig_cal_TSu_scatter_LM.png"),  
       width = 9, height = 8)

# Save TS_c plot 
ggsave(tsc.scatter.lm, filename = here("Figs/fig_cal_TSrel_scatter_LM.png"), 
       width = 9, height = 8)
```

```{r process-nav}
# Source code to get nav data from ERDDAP
source(here("Code/get_nav_erddap.R"))

# Read transect waypoints
wpts <- read_csv(here("Data/Nav", wpt.filename))

# Convert planned transects to sf; CRS = crs.geog
wpts.sf <- wpts %>% 
  filter(Type %in% wpt.types) %>% 
  st_as_sf(coords = c("Longitude","Latitude"), crs = crs.geog)

transects.sf <- wpts.sf %>% 
  group_by(Type, Transect, Region) %>% 
  summarise(do_union = F) %>% 
  st_cast("LINESTRING")

# Create gps.csv file from nav to replace missing data in Echoview
nav.gps <- nav %>% 
  mutate(GPS_date = format(time, format = "%F"),
         GPS_time = format(time, format = "%T")) %>% 
  select(GPS_date, GPS_time, latitude = lat, longitude = long)

write_csv(nav.gps, here("Output/nav.gps.csv"))

# Get most recent vessel position for plotting
nav.now <- tail(nav.sf, 1) %>% 
  mutate(label = paste("Last position:", time, "UTC"))
```

```{r create-basemap,include=F}
# Configure base map options -----------------
# Import landmarks
locations <- filter(read.csv(here("Data/Map/locations.csv")), name %in% label.list) %>%
  project_df(to = crs.proj)

# Get state data
states <- ne_states(country = 'United States of America', returnclass = 'sf')
ca     <- filter(states, name == "California")

# Get countries
countries <- ne_countries(scale = "large", returnclass = "sf") %>%
  filter(subregion %in% c("Northern America","Central America"))

eez_mex <- st_read(here("Data/GIS/eez_mex.shp")) %>% 
  st_transform(crs.geog)

# Read bathy contours shapefile 
bathy <- st_read(here("Data/GIS/bathy_contours.shp")) %>% 
  st_transform(crs.geog) %>% 
  rename(Depth = Contour)

# Set padding around data  
if (resize.map) {
  # Use nav data to resize map to survey progress
  map.bounds <- nav.paths.sf %>% 
    st_transform(crs = crs.proj) %>%
    st_bbox()  
} else {
  # Use nav data to resize map to survey progress
  map.bounds <- transects.sf %>%
    filter(Region ==  "Mexico") %>% 
    st_transform(crs = crs.proj) %>%
    st_bbox()  
}

# Determine map aspect ratio and set height and width
map.aspect <- (map.bounds$xmax - map.bounds$xmin)/(map.bounds$ymax - map.bounds$ymin)
map.height <- 5
map.width  <- map.height*map.aspect

# Create base map
base.map <- get_basemap(nav.paths.sf, states, countries, locations, bathy, map.bounds, crs = crs.proj) +
  # Add scalebar
  annotation_scale(style = "ticks", location = "br", height = unit(0.15, "cm"))
```

```{r process-scs}
if (process.scs) {
  if (scs.source == "CSV") {
    # Process bridge event data #####
    bridge.events <- list.files(here("Data/SCS"), pattern = scs.pattern,
                                full.names = T, recursive = T)  
    # Import from CSV
    bridge.snap <- fs::path(here("Data/SCS")) %>% 
      dir_ls(regexp = scs.pattern) %>% 
      map_df(read_csv) %>% 
      select(Date, Time, Button, Notes = notes.hdr, 
             Lat = gps.lat.hdr, Lon = gps.lon.hdr)
    
  } else if (scs.source == "ELG") {
    # Process bridge event data 
    bridge.events <- list.files(here("Data/SCS"), pattern = "MOA Snap.*elg",
                                full.names = T, recursive = T)
    # create temporary df
    bridge.snap <- data.frame()
    # read and rbind all files
    for (i in bridge.events) {
      bridge.snap.tmp <- read_csv(i) %>% 
        select("Date", "Time", "Button", "Notes" = notes.hdr, 
               "Lat" = gps.lat.hdr, "Lon" = gps.lon.hdr) 
      
      bridge.snap <- bind_rows(bridge.snap, bridge.snap.tmp)
    }
  } else if (scs.source == "XLSX") {
    # Process bridge event data #####
    bridge.events <- dir_ls(here("Data/SCS"), regexp = scs.pattern)
    
    bridge.snap <- tibble()
    
    for (b in bridge.events) {
      moa.tmp <- read_xlsx(bridge.events[b]) %>% 
        mutate(Time = as.character(gsub(".* ", "", Time)),
               datetime = Date + hms(Time)) %>% 
        select(Date, Time, Button, Notes = notes.hdr, 
               Lat = all_of(gps.lat.hdr), Lon = all_of(gps.lon.hdr), datetime) 
      
      bridge.snap <- bind_rows(bridge.snap, moa.tmp)
    }
  }
  
  # Format data
  bridge.snap <- bridge.snap %>% 
    filter(Lon != "", Lat != "") %>% 
    mutate(Lat      = as.numeric(substr(Lat, 1, 2)) + 
             as.numeric(substr(Lat, 3, 7))/60,
           Lon      = -(as.numeric(substr(Lon, 1, 3)) +
                          as.numeric(substr(Lon, 4, 8))/60),
           Button = case_when(
             Button == cb.flush.button ~ "Retracted (5 m)",
             Button == cb.int.button ~  "Intermediate (7 m)",
             Button == cb.ext.button ~  "Extended (9 m)",
             TRUE ~ Button)) %>% 
    arrange(datetime) %>% 
    mutate(datetime =  format(datetime, format = "%m/%d/%Y %H:%M")) 
  
  # save processed SCS data
  save(bridge.snap, file = here("Data/SCS/processed_logs.Rdata"))
} else {
  # load processed SCS data
  load(here("Data/SCS/processed_logs.Rdata"))
}

bridge.snap <- bridge.snap %>% 
  st_as_sf(coords = c("Lon", "Lat"), crs = 4326) %>% 
  st_intersection(select(eez_mex, geometry)) %>% 
  project_sf(crs = 4326) %>% 
  rename(Lat = Y, Lon = X)
  
```

```{r import-trawl-data}
if (get.db) {
  if (trawl.source == "SQL") {
    # Configure ODBC connection to TRAWL database
    trawl.con  <- dbConnect(odbc(), 
                            Driver = "SQL Server", 
                            Server = "161.55.235.187", 
                            Database = "TRAWL", 
                            Trusted_Connection = "True")
  } else if (trawl.source == "Access") {
    trawl.con  <- dbConnect(odbc::odbc(), 
                            Driver = "Microsoft Access Driver (*.mdb, *.accdb)", 
                            DBQ = file.path(here("Data/Trawl"), trawl.db.access))
  }
  
  # Import trawl database tables
  catch.all	     <- tbl(trawl.con,"Catch") %>% collect()
  haul.all       <- tbl(trawl.con,"Haul") %>% collect()
  lengths.all    <- tbl(trawl.con,"Specimen") %>% collect()
  lengthFreq.all <- tbl(trawl.con,"LengthFrequency") %>% collect()
  spp.codes      <- tbl(trawl.con,"SpeciesCodes") %>% collect()
  
  # Close database channel
  dbDisconnect(trawl.con)
  
  # Save imported database data to .Rdata file
  save(catch.all, haul.all, lengths.all, spp.codes, lengthFreq.all, 
       file = here("Data/Trawl/trawl_data.Rdata"))
} else {
  load(here("Data/Trawl/trawl_data.Rdata"))
}
```

```{r process-trawl-haul-data}
# Create startLatitudeDecimal and startLongitudeDecimal for Access data
if (trawl.source == "Access") {
  # Reformat haul data to match SQL
  haul.all <- haul.all %>% 
    mutate(
      startLatDecimal  =   startLatitudeDegrees + (startLatitudeMinutes/60),
      startLongDecimal = -(startLongitudeDegrees + (startLongitudeMinutes/60)),
      stopLatDecimal   =   stopLatitudeDegrees + (stopLatitudeMinutes/60),
      stopLongDecimal  = -(stopLongitudeDegrees + (stopLongitudeMinutes/60)), 
      haulBackTime     = haulbackTime,
      equilibriumTime  = EquilibriumTime) %>% 
    mutate(haulbackTime = case_when(
      haulBackTime < equilibriumTime ~ haulBackTime + days(1),
      TRUE ~ haulBackTime)) %>%
    rename(cruise = Cruise, ship = Ship, haul = Haul, 
           collection = Collection, notes = Notes)
  
  # Identify hauls where date of equilibriumTime or haulBackTime is incorrect
  eq.fix <- which(c(0, diff(haul.all$equilibriumTime)) < 0)
  hb.fix <- which(c(0, diff(haul.all$haulBackTime)) < 0)
  
  # Correct equilibriumTime or haulBackTime
  haul.all$equilibriumTime[eq.fix] <- haul.all$equilibriumTime[eq.fix] + days(1)
  haul.all$haulBackTime[eq.fix]    <- haul.all$haulBackTime[eq.fix] + days(1)
  
  # Reformat length frequency data to match SQL
  lengths.all <- lengths.all %>% 
    rename(cruise = Cruise, ship = Ship, haul = Haul, 
           collection = Collection, species = Species)
  
  # Reformat length frequency data to match SQL
  lengthFreq.all <- lengthFreq.all %>% 
    rename(cruise = Cruise, ship = Ship, haul = Haul, collection = Collection, 
           species = Species, length = Length, lengthType = LengthType, 
           sexUnknown = NotDetermined, male = Male, activeFemale = ActiveFemale, 
           inactiveFemale = InactiveFemale, totalFemale = TotalFemale, 
           subSampleNumber = SubSampleNumber)
} else if (trawl.source == "SQL") {
  haul.all <- haul.all %>% 
    mutate(
      equilibriumTime = ymd_hms(equilibriumTime),
      haulBackTime    = ymd_hms(haulBackTime))
}

# Classify hauls by season (spring or summer)
haul.all <- haul.all %>% 
  mutate(season = case_when(
    month(equilibriumTime) < 6 ~ "spring",
    TRUE ~ "summer")) 

# Filter haul data for current survey
haul <- haul.all %>% 
  select(cruise, ship, haul, collection, startLatDecimal, startLongDecimal, 
         stopLatDecimal, stopLongDecimal, equilibriumTime, haulBackTime, 
         trawlPerformance, season, notes) %>% 
  filter(cruise %in% cruise.name & ship %in% cruise.ship) %>%
  # Calculate haul duration
  mutate(duration = difftime(haulBackTime, equilibriumTime, units = "mins")) %>% 
  # Remove bad trawls
  filter(!trawlPerformance %in% trawl.performance) %>% 
  # Assign cluster based on yearday
  mutate(cluster = cumsum(c(0, diff(equilibriumTime)) > 12) + 1) %>% 
  st_as_sf(coords = c("startLongDecimal", "startLatDecimal"), crs = 4326) %>% 
  st_intersection(select(eez_mex, geometry)) %>% 
  project_sf(crs = 4326) %>% 
  rename(startLatDecimal = Y, startLongDecimal = X)
  # filter(startLatDecimal <= 32.53) %>% 
  # droplevels() # Remove unused factor levels

# # Get haul starts
# Find midpoint of each haul as the mean lat/lon
haul.mid <- haul %>% 
  group_by(cluster, haul) %>% 
  summarise(
    lat  = mean(c(startLatDecimal, stopLatDecimal)),
    long = mean(c(startLongDecimal, stopLongDecimal)))

# Convert haul paths and midpoints to sf; CRS = crs.geog
# Create haul paths from starts and ends
haul.paths <- select(haul, haul, lat = startLatDecimal, long = startLongDecimal) %>% 
  bind_rows(select(haul, haul, lat = stopLatDecimal, long = stopLongDecimal)) %>% 
  arrange(haul) %>% 
  st_as_sf(coords = c("long","lat"), crs = crs.geog) %>% 
  group_by(haul) %>% 
  summarise(do_union = F) %>% 
  st_cast("LINESTRING") 

haul.locs.sf <- haul.mid %>% 
  mutate(label = paste("Haul", haul)) %>% 
  st_as_sf(coords = c("long","lat"), crs = crs.geog) 

# Find midpoint of each haul cluster as the average of haul midpoints
cluster.mid <- haul.mid %>% 
  group_by(cluster) %>% 
  summarise(
    lat  = mean(lat),
    long = mean(long))  
``` 

```{r process-catch-data}
# Filter catch data
catch <- catch.all %>% 
  left_join(dplyr::select(spp.codes, species, scientificName, commonName)) %>% 
  filter(cruise %in% cruise.name & ship %in% cruise.ship & 
           scientificName %in% cps.spp & netSampleType == 'codend') %>% 
  right_join(dplyr::select(haul, haul, cluster)) %>% 
  mutate(key = paste(haul, scientificName),
         totalWeight = subSampleWtkg + remainingSubSampleWtkg) %>% 
  filter(haul %in% unique(haul.locs.sf$haul))

if (nrow(catch) > 0) {
  # Summarize trawl catch by species
  haul.summ.wt <- catch %>% 
    select(haul, cluster, scientificName, totalWeight) %>% 
    tidyr::spread(scientificName, totalWeight) 
  
  # Add species with zero total weight
  if (!has_name(haul.summ.wt, "Engraulis mordax"))      {haul.summ.wt$`Engraulis mordax`      <- 0}
  if (!has_name(haul.summ.wt, "Sardinops sagax"))       {haul.summ.wt$`Sardinops sagax`       <- 0}
  if (!has_name(haul.summ.wt, "Scomber japonicus"))     {haul.summ.wt$`Scomber japonicus`     <- 0}
  if (!has_name(haul.summ.wt, "Trachurus symmetricus")) {haul.summ.wt$`Trachurus symmetricus` <- 0}
  if (!has_name(haul.summ.wt, "Clupea pallasii"))       {haul.summ.wt$`Clupea pallasii`       <- 0}
  if (!has_name(haul.summ.wt, "Atherinopsis californiensis")) {haul.summ.wt$`Atherinopsis californiensis` <- 0}
  
  # Calculate total weight of all CPS species
  haul.summ.wt <- haul.summ.wt %>%  
    replace(is.na(.), 0) %>% 
    mutate(AllCPS = rowSums(select(., -haul, -cluster))) %>%
    # mutate(AllCPS = rowSums(.[, 3:ncol(.)])) %>%
    rename("Jacksmelt"  = "Atherinopsis californiensis",
           "PacHerring" = "Clupea pallasii",
           "Anchovy"    = "Engraulis mordax",
           "Sardine"    = "Sardinops sagax",
           "PacMack"    = "Scomber japonicus",
           "JackMack"   = "Trachurus symmetricus") 
  
  # Summarise catch by cluster
  cluster.summ.wt <- haul.summ.wt %>% 
    select(-haul, -AllCPS) %>% 
    group_by(cluster) %>% 
    summarise_all(list(sum)) %>% 
    mutate(AllCPS = rowSums(select(., -cluster))) %>% 
    right_join(cluster.mid) %>% 
    replace(is.na(.), 0)
  
  # Add lat/long to haul summary for plotting
  haul.summ.wt <- haul.summ.wt %>% 
    right_join(haul.mid) %>% 
    replace(is.na(.), 0)
  
} else {
  # Summarize trawl catch by species
  haul.summ.wt <- bind_cols(select(haul, haul, cluster),
                            data.frame(
                              "Jacksmelt"  = rep(0, nrow(haul)),
                              "PacHerring" = rep(0, nrow(haul)),
                              "Anchovy"    = rep(0, nrow(haul)),
                              "Sardine"    = rep(0, nrow(haul)),
                              "PacMack"    = rep(0, nrow(haul)),
                              "JackMack"   = rep(0, nrow(haul)),
                              "AllCPS"     = rep(0, nrow(haul)))) %>% 
    right_join(haul.mid)
  
  # Summarise catch by cluster
  cluster.summ.wt <- haul.summ.wt %>% 
    select(-haul, -AllCPS) %>% 
    group_by(cluster) %>% 
    summarise_all(list(sum)) %>% 
    mutate(AllCPS = rowSums(select(., -cluster))) %>%
    right_join(cluster.mid) %>% 
    replace(is.na(.), 0)
}

# Prepare catch data for plotting ----------------------------------------------
# Select and rename trawl data for pie charts
haul.pie <- haul.summ.wt %>% 
  select(haul, long, lat, Anchovy, JackMack, 
         Jacksmelt, PacHerring, PacMack, Sardine, AllCPS) %>% 
  mutate(bin       = cut(AllCPS, trawl.breaks, include.lowest = TRUE),
         bin.level = as.numeric(bin)) %>% 
  project_df(to = crs.proj) %>% 
  mutate(
    label = paste("Haul", haul),
    popup = paste('<b>Cluster:', haul, '</b><br/>',
                  'Anchovy:', Anchovy, 'kg<br/>',
                  'Sardine:', Sardine, 'kg<br/>',
                  'Jack Mackerel:', JackMack, 'kg<br/>',
                  'P. herring:', PacHerring, 'kg<br/>',
                  'P. mackerel:', PacMack, 'kg<br/>',
                  'All CPS:', AllCPS, 'kg'))

cluster.pie <- cluster.summ.wt %>% 
  select(cluster, long, lat, Anchovy, JackMack, 
         Jacksmelt, PacHerring, PacMack, Sardine, AllCPS) %>% 
  mutate(bin       = cut(AllCPS, trawl.breaks, include.lowest = TRUE),
         bin.level = as.numeric(bin)) %>% 
  project_df(to = crs.proj) %>% 
  mutate(
    label = paste("Cluster", cluster),
    popup = paste('<b>Cluster:', cluster, '</b><br/>',
                  'Anchovy:', Anchovy, 'kg<br/>',
                  'Sardine:', Sardine, 'kg<br/>',
                  'Jack Mackerel:', JackMack, 'kg<br/>',
                  'P. herring:', PacHerring, 'kg<br/>',
                  'P. mackerel:', PacMack, 'kg<br/>',
                  'All CPS:', AllCPS, 'kg'))

# Filter for empty trawls
haul.zero    <- filter(haul.pie, AllCPS == 0)

cluster.zero <- filter(cluster.pie, AllCPS == 0)

# Calculate pie radius based on latitude range
pie.radius <- as.numeric(abs(map.bounds$ymin - map.bounds$ymax)*0.025)

# Calculate pie radius of each pie, based on All CPS landings
# Calculate pie radius of each pie, based on All CPS landings
if (scale.pies) {
  haul.pie$r    <- pie.radius*log(haul.pie$bin.level+1)
  cluster.pie$r <- pie.radius*log(cluster.pie$bin.level+1)
} else {
  haul.pie$r    <- pie.radius
  cluster.pie$r <- pie.radius
}

# Filter for positive hauls and clusters
haul.pos <- filter(haul.pie, AllCPS > 0) %>% 
  arrange(X)

cluster.pos <- filter(cluster.pie, AllCPS > 0) %>% 
  arrange(X)

# Substitute very small value for species with zero catch, just for pie charts
if (nrow(haul.pos) > 0) {
  haul.pos <- haul.pos %>% 
    replace(. == 0, 0.0000001) 
  
  cluster.pos <- cluster.pos %>% 
    replace(. == 0, 0.0000001) 
}

# Convert haul data for plotting
haul.catch <- haul.summ.wt %>% 
  project_df(to = crs.proj)

# sum total weight of sardine, anchovy, and mackerel
haul.Anchovy.kg    <- sum(haul.catch$Anchovy, na.rm = T)
haul.Sardine.kg    <- sum(haul.catch$Sardine, na.rm = T)
haul.PacMack.kg    <- sum(haul.catch$PacMack, na.rm = T)
haul.JackMack.kg   <- sum(haul.catch$JackMack, na.rm = T)
haul.PacHerring.kg <- sum(haul.catch$PacHerring, na.rm = T)
haul.CPS.kg        <- sum(haul.Anchovy.kg, haul.Sardine.kg, haul.PacMack.kg,
                          haul.JackMack.kg, haul.PacHerring.kg, na.rm = T)

# summarize trawl haul data
trawl.summ <- haul.catch %>% 
  left_join(select(haul, haul, Date = equilibriumTime, Latitude = startLatDecimal,
                   Longitude = startLongDecimal)) %>% 
  select(Haul = haul, Date, Latitude, Longitude, "N. Anchovy" = Anchovy, "P. Sardine" = Sardine, 
         "P. Mackerel" = PacMack, "J. Mackerel" = JackMack, "P. Herring" = PacHerring, 
         Jacksmelt, "All CPS" = AllCPS) %>% 
  mutate(Date = format(Date, "%m/%d/%Y %H:%M"))
```

```{r process-cufes}
# Read CUFES data
cufes.filename <- list.files(here("Data/CUFES"), pattern = "*.sqlite")
cufes.con      <- dbConnect(SQLite(), dbname = here("Data/CUFES", cufes.db.sqlite))
cufes.raw <- tbl(cufes.con, "cufessqlite") %>%
  collect() %>% 
  mutate(
    Start = case_when(
      cufes.date.format == "mdy" ~ mdy_hms(Start), #"06/01/2019-15:43:00"
      cufes.date.format == "ymd" ~ ymd_hms(Start)),#"1996-03-15 15:43:00 -08:00"
    Stop = case_when(
      cufes.date.format == "mdy" ~ mdy_hms(Stop), #"06/01/2019-15:43:00"
      cufes.date.format == "ymd" ~ ymd_hms(Stop)),#"1996-03-15 15:43:00 -08:00"
    Duration = as.numeric(difftime(Stop, Start, units = "mins")),
    Year = year(Start),
    AllEggs = SardineEggs + AnchovyEggs + JackMackerelEggs) %>% 
  rename(lat = StartLatitude, long = StartLongitude) %>% 
  filter(!is.na(lat), !is.na(long)) %>% 
  # filter(lat <= 32.53) %>% 
  project_df(to = crs.proj)

# Close connection
dbDisconnect(cufes.con)

# Process CUFES data
cufes <- cufes.raw %>% 
  # Convert cufes to long format for plotting
  select(
    SampleNumber, Year, Ship, Cruise, lat, long, Duration, 
    SardineEggs, AnchovyEggs, JackMackerelEggs, SquidEggs, HakeEggs, OtherFishEggs,
    Comments) %>%
  gather(Species, Counts, -SampleNumber, -Year, -Ship, -Cruise, 
         -lat, -long, -Duration, -Comments) %>% 
  mutate(Density = Counts/Duration/0.64,
         # Create bins for defining point size in NASC plots
         bin = cut(Density, cufes.breaks, include.lowest = T),
         bin.level = as.numeric(bin)) %>% 
  left_join(select(cufes.raw, SampleNumber, Start, Stop)) %>% 
  st_as_sf(coords = c("long", "lat"), crs = 4326) %>% 
  st_intersection(select(eez_mex, geometry)) %>% 
  project_df(to = 3310) 

# Prepare CUFES data for plotting ----------------------------------------------
# Select CUFES sample with zero density for plotting
cufes.neg <- filter(cufes.raw, AllEggs == 0) %>% 
  mutate(bin.level = 1) %>% 
  st_as_sf(coords = c("X", "Y"), crs = 3310) %>% 
  st_transform(crs = 4326) %>% 
  st_intersection(select(eez_mex, geometry)) %>% 
  project_sf(crs = 3310) %>% 
  # mutate(lat = Y, long = X) %>% 
  select(X, Y, SampleNumber) 

# Identify bad CUFES samples
cufes.bad <- filter(cufes.raw, Duration <= 0)

# save(cufes.bad, file = here("Output/cufes_bad.Rdata"))

# Remove bad samples from CUFES
cufes <- cufes %>% 
  filter(!SampleNumber %in% cufes.bad$SampleNumber)

# Write CUFES data from current survey to CSV
# write.csv(cufes, file = here("Output/cufes_data.csv"), quote = F)

# Create bins for defining point size in NASC plots
cufes <- cufes %>% 
  mutate(bin = cut(Density, cufes.breaks, include.lowest = T),
         bin.level = as.numeric(bin))

# Project CUFES data from CPS
cufes.plot <- cufes %>% 
  filter(Density > 0, Species %in% cufes.plot.spp) %>%
  arrange(desc(Density))

# Project CUFES data from squid
cufes.plot.squid <- cufes %>% 
  filter(Density > 0, Species == "SquidEggs") %>% 
  arrange(desc(Density))

# Project CUFES data from other fish eggs (mostly P. mackerel)
cufes.plot.ofe <- cufes %>% 
  filter(Density > 0, Species == "OtherFishEggs") %>%
  filter(str_detect(Comments, "japonicus")) %>% 
  arrange(desc(Density))
```  

```{r process-ctd-stations}
# Extract CTD and UCTD cast locations
events_sf <- bridge.snap %>% 
  filter(Lat <= 32.53) %>% 
  st_as_sf(coords = c("Lon","Lat"), crs = crs.geog)

ctd.sta         <- filter(events_sf, Button == ctd.button)
uctd.sta        <- filter(events_sf, Button == uctd.button)

# Extract bongo locations
bongo.sta       <- filter(events_sf, Button == bongo.button)
  

# Extract pairovet locations
pairovet.sta    <- filter(events_sf, Button == pairovet.button)

# Extract all CTD/UCTD stations
all.ctds        <- bind_rows(filter(events_sf, Button == ctd.button),
                             filter(events_sf, Button == uctd.button)) %>% 
  project_sf(crs = 4326) %>% 
  arrange(datetime) %>% 
  select(Date = datetime, Button, Latitude = Y, Longitude = X) %>% 
  mutate(Button = case_when(
    Button == ctd.button ~ "CTD Cast",
    Button == uctd.button ~ "UCTD Cast", 
    TRUE ~ Button))
```  

```{r process-csv-cps}
if (process.csv) {
  if (file.exists(here("Output/processed_cps_mexico.Rdata"))) {
    # Load already processed CSV files
    load(here("Output/processed_cps_mexico.Rdata"))
  }
  
  # List local CSV files
  csv.files.cps <- dir_ls(here("Data/Backscatter", survey.vessel.primary), 
                          regexp = nasc.pattern.cps[survey.vessel.primary],
                          ignore.case = TRUE)
  
  if (process.csv.all) {
    # Create final data frame
    nasc.cps <- data.frame()
  } else {
    # Load already processed files
    load(here("Output/nasc_cps.Rdata")) 
    # List only new CSV files
    csv.files.cps <- csv.files.cps[!csv.files.cps %in% processed.cps]
  }
  
  if (length(csv.files.cps) > 0) {
    # Configure progress bar
    pb <- winProgressBar(title = "CSV File Processing Progress - CPS", 
                         label = "0% done", min = 0, max = 100, initial = 0)
    
    # Process all .CSV files
    for (i in 1:length(csv.files.cps)) {
      # Process i-th file
      nasc.cps <- bind_rows(nasc.cps, extract_csv(csv.files.cps[i]))
      
      # Update the progress bar
      info <- sprintf("%d%% done", round((i / length(csv.files.cps)) * 100))
      setWinProgressBar(pb, round((i / length(csv.files.cps)) * 100), label = info)
    }
    close(pb)
    
    # Calculate summary interval
    nasc.cps <- nasc.cps %>%
      mutate(int = cut(Interval, seq(1, max(Interval) + nasc.summ.interval, nasc.summ.interval),
                       labels = F, include.lowest = T))
    
    # Save results
    save(nasc.cps, file = here("Output/nasc_cps.Rdata"))
    write.csv(nasc.cps, file = here("Output/nasc_cps.csv"), row.names = F, quote = F)
  }
} else {
  load(here("Output/nasc_cps.Rdata"))
}

if (process.csv) {
  # Filter NASC
  nasc.cps <- nasc.cps %>% 
    mutate(transect = as.numeric(transect)) %>% 
    st_as_sf(coords = c("long", "lat"), crs = 4326) %>% 
    st_intersection(select(eez_mex, geometry)) %>% 
    project_sf(crs = 4326) %>% 
    rename(lat = Y, long = X) %>% 
    filter(!is.na(transect),
           !transect %in% as.numeric(tx.rm$RL))
  
  # Save subset data; processing takes some time
  save(nasc.cps, file = here("Output/nasc_cps_mx.Rdata"))
  
} else {
  load(here("Output/nasc_cps_mx.Rdata"))
}

# Get intervals with bad lat/long values
bad.nasc.cps <- filter(nasc.cps, lat == 999, long == 999)
# write_csv(bad.nasc.cps, here("Output/nasc_bad_cps.csv"))

# Summarize nasc for reporting effort
nasc.summ <- nasc.cps %>% 
  group_by(transect) %>% 
  summarise(
    distance = length(Interval)*100/1852,
    lat = lat[which.min(long)],
    lon = long[which.min(long)])

# average NASC.70 data over new intervals or number of intervals in a 2 km radius
nasc.summ.cps <- nasc.cps %>%
  mutate(transect = as.numeric(transect)) %>% 
  filter(lat != 999, long != 999) %>% 
  group_by(transect, int) %>%
  summarise(
    bins    = length(int),
    bin.mid = as.integer(round(bins / 2)),
    lat     = lat[1],
    long    = long[1],
    NASC    = mean(NASC.70)
  )

# Average cps.nasc over defined interval
# Summarize by filename, not transect, so that renamed (i.e., strip.tx.chars == T) transects get included.
nasc.cps.sf <- nasc.cps %>%
  select(filename, transect, int, dist_m, datetime, lat, long, cps.nasc = NASC.70) %>% 
  group_by(filename, transect, int) %>% 
  summarise(
    lat   = lat[1],
    long  = long[1],
    NASC  = mean(cps.nasc),
    label = paste0('Transect: ', transect[1], "; ",
                   'Distance: ', round(min(dist_m)), "-", round(max(dist_m)), ' m'),
    popup = paste0('<b>Transect: </b>', transect[1], '<br/>',
                   '<b>Time: </b>', min(datetime), " - ", max(datetime), ' UTC<br/>',
                   '<b>Distance: </b>', round(min(dist_m)), "-", round(max(dist_m)), ' m<br/>',
                   '<b>NASC: </b>', round(mean(NASC)), ' m<sup>2</sup> nmi<sup>-2</sup>')) %>%
  # Create bins for defining point size in NASC plots
  mutate(bin       = cut(NASC, nasc.breaks, include.lowest = T),
         bin.level =  as.numeric(bin)) %>% 
  filter(!is.na(bin)) %>% 
  st_as_sf(coords = c("long","lat"), crs = crs.geog) 

nasc.plot.cps <- project_sf(nasc.cps.sf, crs.proj)

# Convert acoustic transects to sf
nasc.tx.sf <- st_as_sf(nasc.cps.sf, coords = c("long","lat"), crs = crs.geog) %>% 
  select(transect) %>% 
  group_by(transect) %>% 
  summarise(do_union = F) %>% 
  st_cast("LINESTRING") %>% 
  filter(!transect %in% tx.rm)

# create acoustic transect labels
nasc.tx.labels.cps <- nasc.cps %>%
  group_by(transect) %>%
  summarise(
    lat = lat[which.max(long)],
    long = max(long)
  )

# List already processed CSV files and save
processed.cps <- unique(nasc.cps$filename)
save(processed.cps, file =  here("Output/processed_cps_mexico.Rdata"))
```

```{r process-csv-krill}
if (process.csv) {
  if (file.exists(here("Output/processed_krill.Rdata"))) {
    # Load already processed CSV files
    load(here("Output/processed_krill.Rdata"))
  }
  
  # List local CSV files
  csv.files.krill <- dir_ls(here("Data/Backscatter", survey.vessel.primary), 
                            regexp = nasc.pattern.krill[survey.vessel.primary],
                            ignore.case = TRUE)
  
  if (process.csv.all) {
    # Create final data frame
    nasc.krill <- data.frame()
  } else {
    # Load already processed files
    load(here("Output/nasc_krill.Rdata")) 
    # List only new CSV files
    csv.files.krill <- csv.files.krill[!csv.files.krill %in% processed.krill]
  }
  
  if (length(csv.files.krill) > 0) {
    # Configure progress bar
    pb <- winProgressBar(title = "CSV File Processing Progress - Krill", 
                         label = "0% done", min = 0, max = 100, initial = 0)
    
    # Process all .CSV files
    for (i in 1:length(csv.files.krill)) {
      # Process i-th file
      nasc.krill <- bind_rows(nasc.krill, extract_csv(csv.files.krill[i]))
      
      # Update the progress bar
      info <- sprintf("%d%% done", round((i / length(csv.files.krill)) * 100))
      setWinProgressBar(pb, round((i / length(csv.files.krill)) * 100), label = info)
    }
    close(pb)
    
    # Calculate summary interval
    nasc.krill <- nasc.krill %>%
      mutate(int = cut(Interval, seq(1, max(Interval) + nasc.summ.interval, nasc.summ.interval),
                       labels = F, include.lowest = T)) 
    
    # Save results
    save(nasc.krill, file = here("Output/nasc_krill.Rdata"))
    write.csv(nasc.krill, file = here("Output/nasc_krill.csv"), row.names = F, quote = F)
  }
} else {
  load(here("Output/nasc_krill.Rdata"))
}

# Get intervals with bad lat/long values
bad.nasc.krill <- filter(nasc.krill, lat == 999, long == 999)
write_csv(bad.nasc.krill, here("Output/nasc_bad_krill.csv"))

# Average NASC.350 data over new intervals or number of intervals in a 2 km radius
nasc.summ.krill <- nasc.krill %>%
  filter(lat != 999, long != 999) %>%
  group_by(transect, int) %>%
  summarise(
    bins    = length(int),
    bin.mid = as.integer(round(bins / 2)),
    lat     = lat[1],
    long    = long[1],
    NASC    = mean(NASC.350)
  )

# Average krill.nasc over defined interval
# Summarize by filename, not transect, so that renamed (i.e., strip.tx.chars == T) transects get included.
nasc.krill.sf <- nasc.krill %>%
  filter(lat != 999, long != 999) %>%
  select(filename, transect, int, dist_m, datetime, lat, long, krill.nasc = NASC.350) %>% 
  group_by(filename, transect, int) %>% 
  summarise(
    lat   = lat[1],
    long  = long[1],
    NASC  = mean(krill.nasc),
    label = paste0('Transect: ', transect[1], "; ",
                   'Distance: ', round(min(dist_m)), "-", round(max(dist_m)), ' m'),
    popup = paste0('<b>Transect: </b>', transect[1], '<br/>',
                   '<b>Time: </b>', min(datetime), " - ", max(datetime), ' UTC<br/>',
                   '<b>Distance: </b>', round(min(dist_m)), "-", round(max(dist_m)), ' m<br/>',
                   '<b>NASC: </b>', round(mean(NASC)), ' m<sup>2</sup> nmi<sup>-2</sup>')) %>%
  # Create bins for defining point size in NASC plots%>% 
  mutate(bin       = cut(NASC, nasc.breaks, include.lowest = T),
         bin.level =  as.numeric(bin))  %>% 
  filter(!is.na(bin)) %>% 
  st_as_sf(coords = c("long","lat"), crs = crs.geog) 

nasc.plot.krill <- project_sf(nasc.krill.sf, crs.proj)

# create acoustic transect labels
nasc.tx.labels.krill <- nasc.krill %>%
  group_by(transect) %>%
  summarise(
    lat = lat[which.max(long)],
    long = max(long)
  )

# List already processed CSV files and save
processed.krill <- unique(nasc.krill$filename)
save(processed.krill, file =  here("Output/processed_krill.Rdata"))

# Select plot levels for backscatter data
nasc.plot.all   <- rbind(nasc.cps.sf, nasc.krill.sf) %>% 
  filter(NASC >= 200)
```

```{r plot-maps, include=F}
if (save.figs) {
  # Map planned transects
  survey.plan <- base.map +    
    geom_sf(data = transects.sf, aes(colour = Type, linetype = Type), 
            show.legend = "line") +
    scale_colour_manual("Type", values = wpt.colors) +
    scale_linetype_manual(name = "Type", values = wpt.linetypes) +
    coord_sf(crs = crs.proj, # CA Albers Equal Area Projection
             xlim = c(map.bounds["xmin"], map.bounds["xmax"]), 
             ylim = c(map.bounds["ymin"], map.bounds["ymax"]))
  
  # save survey plan map
  ggsave(here("Figs/fig_survey_plan_mx.png"), survey.plan,
         height = map.height, width = map.width)
  
  # Plot Survey Track, Acoustic Transects, and Trawl Locations #####
  survey.track.map <- base.map +
    # Plot transects data
    geom_sf(data = transects.sf, aes(linetype = Type), colour = "gray70",
            show.legend = "line") +
    # Plot ship track data
    geom_sf(data = nav.paths.sf, colour = "gray50", size = 0.5, alpha = 0.5) +
    # Plot acoustic transects
    geom_sf(data = nasc.tx.sf, size = 1) +
    # Plot trawl transects
    geom_point(data = haul.catch, aes(X, Y),
               shape = 21, colour = "black", fill = "white", size = 1.5) +
    scale_linetype_manual("Type", 
                          values = c(Adaptive = "dashed", Compulsory = "solid",
                                     Offshore = "dashed", Nearshore = "dotted",
                                     transit = "dashed")) +
    coord_sf(crs = crs.proj, # CA Albers Equal Area Projection
             xlim = c(map.bounds["xmin"], map.bounds["xmax"]), 
             ylim = c(map.bounds["ymin"], map.bounds["ymax"]))
  
  # save figure as PNG and PDF images
  ggsave(here("Figs/fig_vessel_track_mexico.png"), survey.track.map,
         height = map.height, width = map.width)
  
  # Plot Side Station Sampling Locations #####
  survey.station.map <- base.map +
    # Plot transects data
    geom_sf(data = transects.sf, aes(linetype = Type), colour = "gray70",
            show.legend = FALSE) +
    # Plot ship track data
    geom_sf(data = nav.paths.sf, colour = "gray70", size = 0.5, alpha = 0.5) +
    # Plot acoustic transects
    geom_sf(data = nasc.tx.sf, size = 1) +
    # scale_linetype_manual(name = "Type", values = wpt.linetypes) +
    # Plot trawl transects
    geom_point(data = haul.catch, aes(X, Y),
               shape = 21, colour = "black", fill = "white", size = 2) +
    geom_sf(data = bongo.sta, 
            shape = 24, size = 2, fill = "orange", colour = "black") +
    geom_sf(data = ctd.sta, 
            shape = 21, size = 2, fill = "red", colour = "black") +
    geom_sf(data = uctd.sta, 
            shape = 21, size = 2, fill = "red", colour = "black") +
    coord_sf(crs = crs.proj,
             xlim = c(map.bounds["xmin"], map.bounds["xmax"]), 
             ylim = c(map.bounds["ymin"], map.bounds["ymax"]))
  
  ggsave(here("Figs/fig_station_samples_mx.png"), survey.station.map,
         height = map.height, width = map.width)
  
  # Create final backscatter summary map
  source(here("Code/plot_sA_CPS.R"))
  # Create final CUFES egg density map
  source(here("Code/plot_CUFES.R"))
  # Create trawl haul and cluster proportion figures
  source(here("Code/plot_haul_proportion_wt.R"))
  
  # Combine backscatter, CUFES, and trawl maps
  nasc.cufes.trawl.plot      <- plot_grid(nasc.map.cps, cufes.density.all, trawl.pie.haul.wt,
                                          nrow = 1, labels = c("a)", "b)", "c)"))
  
  # Save maps
  ggsave(nasc.cufes.trawl.plot, 
         filename = here("Figs/fig_nasc_cufes_haul_wt_mx.png"),
         width = map.width*3, height = map.height)
  
  # Create calibration site maps #####  
  # Put calibration location lat/lon in a df, for point plot
  cal.map.df <- data.frame(cal.lat.dd, cal.lon.dd)
  
  # get Google map from calibration location
  cal.map <- ggmap(get_googlemap(c(cal.lon.dd, cal.lat.dd), 
                                 maptype = "terrain", zoom = 12))
  
  # get extent of calibration map for inset
  cal.map.extent <- cal.map$data
  
  # create inset map
  map.inset <- ggplot(ca) +
    geom_sf() +
    geom_path(data = cal.map.extent[c(1, 3, 4, 2, 1),], aes(lon, lat),
              size = 0.5, colour = 'red') + # plot extent of inset map
    theme(axis.text.x     = element_blank(), 
          axis.text.y     = element_blank(), 
          axis.ticks      = element_blank(),
          axis.title.x    = element_blank(),
          axis.title.y    = element_blank(),
          plot.background = element_blank())
  
  # create main map using ggmap
  map.main <- cal.map + 
    geom_point(data = cal.map.df, aes(x = cal.lon.dd,y = cal.lat.dd),
               shape = 23, size = 5, fill = "yellow", colour = "black") + 
    xlab("\nLongitude (W)") + ylab("Latitude (N)\n") + 
    theme_bw() + 
    theme(plot.background = element_blank(),
          axis.text.y = element_text(angle = 90, hjust = 0.5))
  
  # Create final color map
  cal.map.color <- ggdraw() +
    draw_plot(map.main,0,0,1,1) +
    draw_plot(map.inset,0.65,0.65,0.325,0.325)
  
  # Save map image
  ggsave(cal.map.color, filename = here("Figs/fig_cal_map.png"),
         height = 10, width = 10, units = "in")
} 
```  

\pagenumbering{gobble}

**Preliminary report on the `r survey.name.long` (`r survey.name`), `r survey.start` to `r survey.end` `r survey.year`, conducted aboard NOAA Ship _`r survey.vessel.long`_ in Mexican Waters**  

Kevin L. Stierhoff^1^, Juan P. Zwolinski^1,2^, Thomas S. Sessions^1^, and David A. Demer^1^ [**Revise?**]

^1^Fisheries Resources Division  
Southwest Fisheries Science Center (SWFSC)  
NOAA-National Marine Fisheries Service  
8901 La Jolla Shores Dr.  
La Jolla, CA 92037, USA  

^2^University of California, Santa Cruz  
The Cooperative Institute for Marine Ecosystems and Climate (CIMEC)  
1156 High St
Santa Cruz, CA 95064, USA  

# Executive Summary

\newpage
\pagenumbering{arabic}

# Introduction {#introduction}

The `r survey.name.long` was conducted by NOAA's Southwest Fisheries Science Center (SWFSC) aboard NOAA Ship *`r survey.vessel.long`* (hereafter, *`r survey.vessel`*), `r survey.start` to `r survey.end` `r survey.year`. The Acoustic-Trawl Method (ATM) is routinely used to assess coastal pelagic fish species (CPS) and krill within the California Current Ecosystem (CCE), between Vancouver Island, British Columbia and San Diego, CA. In 2021, for the first time, the survey extended southward to central Baja California, Mexico. Data were collected using multi-frequency echosounders, surface trawls, obliquely integrating net tows, a continuous underway fish-egg sampler (CUFES), and conductivity-temperature-depth probes (CTDs).

The objectives for the survey were to: 1) acoustically map the distributions and estimate the abundances of CPS, i.e., Pacific Sardine *Sardinops sagax*, Northern Anchovy *Engraulis mordax*, Pacific Herring *Clupea pallasii*, Pacific Mackerel *Scomber japonicus*, and Jack Mackerel *Trachurus symmetricus*; and krill (euphausiid spp.) in the CCE; 2) characterize and investigate linkages to their biotic and abiotic environments; and 3) gather information regarding their life histories.

The survey area was defined primarily by the modeled distribution of potential habitat for the northern subpopulation (stock) of Pacific Sardine [@Zwolinski2011]. This area was chosen to encompass the anticipated distributions of the northern stock of Pacific Sardine and the central stock of Northern Anchovy off the west coasts of the U.S. and Mexico, from approximately `r survey.landmark.s`, to `r survey.landmark.n`, but also spanned portions of the stocks of Pacific Mackerel, Jack Mackerel, and Pacific Herring.

This report provides a preliminary overview of the survey objectives and a summary of the survey equipment, sampling, and preliminary results from data collected in Mexican waters. This report does not include estimates of the animal distributions and biomasses, which are documented separately. The final report will be provided no later than `r ymd("2021-10-04")+years(1)`.  

**This survey was conducted with the approval of the Secretaria de Relaciones Exteriores (SRE, Diplomatic note CTC/1312/2021), the INEGI (Permit: EG0082021), and CONAPESCA/SAGARPA (Permiso de Pesca de Fomento: PPFE/DGOPA-073/21).**

_1. Georeferenced data of the measurements of meteorological parameters, temperature and salinity profiles of seawater up to 350 meters deep or shallower depths using CTD, oxygen, chlorophyll-a, of currents made with the ADCP profiler_

_2. Results of the sound recordings of marine mammals and those that are obtained from samples of fish, plankton, fish larvae, cephalopods and Euphausiids._ 

_3. Results of the genetic analysis of sardine and anchovy, as well as those of otolith reading of 50 sardines and 25 fish of other species._  

# Scientific Personnel {#introduction-personnel}

The collection and analysis of the survey data were conducted by members of the Fisheries Resources Division at the SWFSC. Asterisks denote Chief Scientists.  

**Project Lead:**

* D. Demer

**Acoustic Data Collection and Processing:**

* Leg I: G. Johnson (NOAA Corps) and J. Renfree*
* Leg II:  S. Dolan (NCSU), S. Mau, and E. V. Perez Flores (INAPESCA)
* Leg III:  L. Altimirano Lopez (INAPESCA), D. Murfin, J. Zwolinski* (UCSC)
* Leg IV: T. Sessions, K. Stierhoff*, and R. Vallarta (INAPESCA)

**Trawl Sampling:**

* Leg I:  M. Craig, A. Freire, L. Heberer, S. Hensman, E. Gardner, and P. Kuriyama
* Leg II:  J. Barnes, B. Bellerud, D. Hernandez Cruz (INAPESCA), M. Human, L. Martin, B. Schwartzkopf, W. Watson*
* Leg III: E. Adams, S. del Carmen Morales Gutierrez (INAPESCA), C. Fahy, E. Gardner, Z. Skelton, O. Snodgrass, J. Walker
* Leg IV: K. James, K. Koch, K. Lane, D. Lowry, V. H. Martinez Magana (INAPESCA), J. E. Osuna Soto (INAPESCA), L. Vasquez del Mercado

**Echosounder Calibrations:**  

* _Reuben Lasker_
  + J. Renfree and T. Sessions

# Methods {#methods}
## Survey region and design {#methods-survey-design}

During spring, the northern stock of Pacific Sardine typically aggregates offshore of central and southern California to spawn [@Demer2012, and reference therein]. During summer, if the stock is large enough, adults will migrate north, compress along the coast, and feed in the upwelled regions (**Fig. \@ref(fig:sardine-distribution)**).  

During `r tolower(survey.season)` `r survey.year`, the west coast of the United States was surveyed using _`r survey.vessel`_. Compulsory transects were nearly perpendicular to the coast with separations of 15 (south of Pt. Conception) to 20 nmi (north of Pt. Conception). The survey began off `r survey.landmark.s`, and progressed northwards toward `r survey.landmark.n`. Irrespective of the size of the stock and the extent of its migration, the northern stock of Pacific Sardine tends to reside within its potential habitat [@Zwolinski2011].  

The planned transects (**Fig. \@ref(fig:survey-plan)**) spanned the latitudinal extent of the potential habitat of the northern stock of Pacific Sardine^[https://coastwatch.pfeg.noaa.gov/erddap/griddap/sardine_habitat_modis.html] at the time of the survey. The offshore extent of planned transects was adjusted during the survey according to the observed distribution of putative CPS backscatter in the echosounders, CPS eggs in CUFES samples, or CPS caught in trawls. To estimate CPS biomass near shore, where it is too shallow to navigate NOAA ships safely, sampling from _`r survey.vessel`_ was augmented with echosounder and purse-seine sampling from two fishing vessels (**Fig. \@ref(fig:survey-plan)**).  

(ref:sardine-distribution) Conceptual spring (shaded region) and summer (hashed region) distributions of potential habitat for the northern stock of Pacific Sardine along the west coasts of Mexico, the United States, and Canada. The dashed and dotted lines represent, respectively, the approximate summer and spring positions of the 0.2 mg m^–3^ chlorophyll-a concentration isoline. This isoline appears to oscillate in synchrony with the transition zone chlorophyll front [TZCF, @Polovina2001] and the offshore limit of the northern stock Pacific Sardine potential habitat [@Zwolinski2011]. Mackerels are found within and on the edge of the same oceanographic  habitat [e.g., @Demer2012; @Zwolinski2012]. The TZCF may delineate the offshore and southern limit of both Pacific Sardine and Pacific Mackerel distributions, and juveniles may have nursery areas in the Southern California Bight (SCB), downstream of upwelling regions.

```{r sardine-distribution, fig.cap='(ref:sardine-distribution)',out.height='8in',fig.pos='H'}
include_graphics(here("Images/img_survey_region.png"))  
```  

\newpage  

(ref:survey-plan) Planned compulsory (solid black lines) and adaptive (dashed red lines) transect lines sampled by _`r survey.vessel`_; adaptive transect lines sampled by unmanned surface vehicles (solid yellow lines); and nearshore transect lines sampled by fishing vessels (solid magenta lines). Isobaths (light gray lines) are placed at 50, 200, 500, and 2,000 m (or approximately 25, 100, 250, and 1,000 fathoms).

```{r survey-plan, fig.cap='(ref:survey-plan)', out.height='8in',fig.align='center',fig.pos='H'}
include_graphics(here("Figs/fig_survey_plan_mx.png"))
```  

\newpage  

## Acoustic sampling {#methods-acoustic-sampling}
### Echosounders {#methods-echosounders}  

On _`r survey.vessel`_, multi-frequency EK80 Wideband Transceivers (18-, 38-, 70-, 120-, 200-, and 333-kHz WBTs; Simrad) were configured with split-beam transducers (`r echo.models`, respectively; Simrad). The transducers were mounted on the bottom of a retractable keel or "centerboard" (**Fig. \@ref(fig:cb-config)**). The keel was retracted (transducers ~`r cb.retracted`-m depth) during calibration, and extended to the intermediate position (transducers ~`r cb.intermediate`-m depth) during the survey. Exceptions were made during shallow water operations, when the keel was retracted; or during times of heavy weather, when the keel was extended (transducers ~`r cb.extended`-m depth) to provide extra stability and reduce the effect of weather-generated noise. Transducer position and motion were measured at 5 Hz using an inertial motion unit (POS-MV, Trimble/Applanix).  

(ref:cb-config) Transducer locations on the bottom of the centerboard aboard _`r survey.vessel`_.

```{r cb-config,fig.cap='(ref:cb-config)',out.width = '5.5in',fig.align='center',fig.pos='H'}
if (survey.vessel.primary == "RL") {
  include_graphics(here("Images/img_centerboard_config_RL.png"))
} else {
  include_graphics(here("Images/img_centerboard_config_SH.png"))
}
```  

### Calibrations {#methods-echosounder-calibration}  

The echosounder systems were calibrated using the standard sphere technique [@Foote1987;@Demer2015]. On _`r survey.vessel`_, each WBT was calibrated in both CW (i.e., continuous wave or chirp mode) and FM modes (i.e., frequency modulation or broadband mode). The reference target was a `r cal.sphere`; calibrations of WBTs in FM mode used both the WC38.1 and a smaller 25-mm WC sphere. On each vessel, GPTs or WBTs were configured using the calibration results via the control software (EK80 `r ek80.version`, Simrad; see **Section \@ref(results-echosounder-calibration)**).  

### Data collection {#methods-acoustic-data-collection}  

Computer clocks were synchronized with the GPS clock (UTC) using synchronization software (NetTime^[http://timesynctool.com]). On _`r survey.vessel`_, the 18-kHz GPT, operated by a separate PC from the other echosounders, was programmed to track the seabed and output the detected depth to the ship’s Scientific Computing System (SCS). The 38-, 70-, 120-, 200-, and 333-kHz echosounders were controlled by the ER60 Adaptive Logger [EAL^[https://swfsc.noaa.gov/eal/] **[Update URL]**, @Renfree2016]. The EAL optimizes the pulse interval based on the seabed depth, while avoiding aliased seabed echoes, and was programmed such that once an hour the echosounders would operate in passive mode and record three pings, for obtaining estimates of the background noise level. Acoustic sampling for CPS-density estimation along the pre-determined transects was limited to daylight hours (approximately between sunrise and sunset).  

Measurements of volume backscattering strength ($S_v$; dB re 1 m^2^ m^-3^) and target strength ($TS$, dB re 1 m^2^), indexed by time and geographic positions provided by GPS receivers, were logged to 60 m beyond the detected seabed range or to a maximum of `r raw.log.range` m and stored in Simrad .raw format with a `r raw.size`-MB maximum file size. During daytime and nighttime, the echosounders were set to operate in CW and FM modes, respectively. For each acoustic instrument, the prefix for the file name is a concatenation of the survey name (e.g.,  `r survey.name`), the operational mode (CW or FM), and the logging commencement date and time from the EK80 software. For example, a file generated by the Simrad EK80 software (`r ek80.version`) for a WBT operated in CW mode is named `2107RL-CW-D20210723-T125901.raw`.   

To minimize acoustic interference, transmit pulses from the EK60, EK80, ME70, MS70, SX90, and the acoustic Doppler current profiler (Ocean Surveyor Model OS75 ADCP, Teledyne RD Instruments) on _`r survey.vessel`_ were triggered using a synchronization system (K-Sync, Simrad). The K-Sync trigger rate, and thus echosounder ping interval, was modulated by the EAL using the 18-kHz seabed depth provided by the SCS. During daytime, the ME70, SX90, and ADCP were operated continuously, while the MS70 was only operated at times when CPS were present. At nighttime, only the EK60, EK80, and ADCP were operated. All other instruments that produce sound within the echosounder bandwidths were secured during daytime-survey operations. Exceptions were made during stations (e.g., plankton sampling and fish trawling) or in shallow water when the vessel's command occasionally operated the bridge's 50- and 200-kHz echosounders (Furuno), the Doppler velocity log (Model SRD-500A, Sperry Marine), or both.  

### Data processing {#methods-acoustic-data-processing}  

Echoes from schooling CPS and plankton were identified using a semi-automated data processing algorithm implemented using Echoview software (`r ev.version`; Echoview Software Pty Ltd). The filters and thresholds were based on a subsample of echoes from randomly selected CPS schools. The aim of the filter criteria is to retain at least 95% of the noise-free backscatter from CPS while rejecting at least 95% of the non-CPS backscatter. Data were processed using the methods outlined in Stierhoff et al. [-@Stierhoff2020a].  

## Trawl sampling {#methods-trawl-sampling}  

To identify the species composition and length distributions of acoustic targets, up to three nighttime surface trawls were conducted at night in areas where echoes from CPS schools were observed in echograms or eggs were observed in the CUFES earlier that day. The net, a Nordic 264 rope trawl (NET Systems; Bainbridge Island, WA; **Figs. \@ref(fig:trawl-diagrams)a, b**), has a rectangular opening in the fishing portion of the net with an area of approximately 300 m^2^ (~15-m tall x 20-m wide), variable-sized mesh in the throat, an 8-mm square-mesh cod-end liner (to retain a large range of animal sizes), and a "marine mammal excluder device" to prevent the capture of large animals, such as dolphins, turtles, or sharks [@Dotson2010]. The trawl doors are foam-filled and the trawl headrope is lined with floats so the trawl tows at the surface.  

Trawls were towed at ~4 kn for 45 min. The total catch from each trawl was weighed and sorted by species or groups. From the catches with CPS, specimens were selected randomly for each of the target species. Specimens were weighed and measured. Sex and maturity were recorded; ovaries were preserved and otoliths were extracted for a portion of the specimens. The combined catches in up to three trawls per night (i.e., trawl cluster) were used to estimate the proportions of species contributing to the nearest samples of acoustic backscatter.  

(ref:trawl-diagrams) Schematic drawings of the Nordic 264 rope trawl a) net and b) cod-end.

```{r trawl-diagrams, fig.cap='(ref:trawl-diagrams)',out.height='8.5in',fig.pos='H'}
include_graphics(here("Images/img_Nordic_264.png")) 
```  

\newpage  

## Ichthyoplankton and oceanographic sampling {#methods-other-sampling}
### Egg and larva sampling {#methods-egg-sampling}  

During the day, fish eggs were collected using a CUFES [@Checkley1997], which collects water and plankton from an intake on the hull of the ship at ~3-m depth and the particles in the sampled water were sieved by a 505-$\mu$m mesh. Pacific Sardine, Northern Anchovy, Jack Mackerel, and Pacific Hake (_Merluccius productus_) eggs were identified to species, counted, and logged. Eggs from other species (e.g., Pacific Mackerel and flatfishes) were also counted and logged as "other fish eggs". Typically, the duration of each CUFES sample was 30 min, corresponding to a distance of 5 nmi at a speed of 10 kn. Because the durations of the initial egg stages is short for most CPS, the egg distributions inferred from CUFES samples indicate the nearby presence of actively spawning fish.  

CalCOFI bongo oblique net [a bridleless pair of 71-cm diameter nets with 505-$\mu$m mesh, @Smith1977] was used to sample ichthyoplankton and krill after sunset, to contribute to the CalCOFI ichthyoplankton time series. Bongo samples were stored in 5% buffered formalin.  

### Conductivity and temperature versus depth (CTD) sampling {#methods-ctd-sampling}  

Conductivity and temperature were measured versus depth to `r ctd.depth` m using calibrated sensors on a CTD rosette or underway probe (UnderwayCTD, or UCTD; Teledyne Oceanscience) cast from the vessel. These data were used to estimate the time-averaged sound speed [@Demer2004c], for estimating ranges to the sound scatterers, and frequency-specific sound absorption coefficients, for compensating signal attenuation of the sound pulse between the transducer and scatterers [@Simmonds2005]. These data indicate the depth of the surface mixed layer, above which most epipelagic CPS reside during the day, which is later used to determine the integration-stop depth during acoustic data processing.  

# Results {#results}
## Echosounder calibrations {#results-echosounder-calibration}  

For _`r survey.vessel`_, the EK80s were calibrated on `r cal.datetime` while the vessel was alongside the pier near `r cal.loc` (`r cal.lat.dd` $^\circ$N, `r cal.lon.dd` $^\circ$W). Measurements of sea-surface temperature ($t_w$ = `r cal.temp` $^\circ$C) and salinity ($s_w$ = `r cal.sal` psu) were measured to a depth of 10 m using a handheld probe (Pro2030, YSI) and input to the WBT-control software (EK80 `r ek80.version`, Simrad), which derived estimates of sound speed ($c_w$ = `r cal.c` m s^-1^) and absorption coefficients (see **Table \@ref(tab:cal-results)**). Varying with tide, the seabed was approximately `r cal.min.z` to `r cal.max.z` m beneath the transducers. The calibration spheres were positioned in the far-field of each transducer, at 3.5- to 7-m range. WBT information, settings, and calibration results are presented in **Table \@ref(tab:cal-results)**.  

\newpage

(ref:cal-results) Simrad EK80 wideband transceiver (WBT; 18, 38, 70, 120, 200, and 333 kHz) and transducer information; pre-calibration settings (above horizontal line); and beam model results following calibration (below horizontal line). Prior to the survey, on-axis gain ($G_0$), beam angles ($\alpha_{-3dB}$ and $\beta_{-3dB}$) and angle offsets ($\alpha_0$ and $\beta_0$), and $S_a$ Correction ($S_\mathrm{a}\mathrm{corr}$) values from calibration results were entered into the control software (EK80 `r ek80.version`, Simrad).

```{r cal-results,results='asis'}
if (doc.type == "docx") {
    # create kable object (for Word)
  pander(all.output)
} else {
  # print LaTeX table for HTML or PDF
  kable(all.output, format = knitr.format, align = c("l","l",rep("c", ncol(all.output) - 2)),
        booktabs = T, escape = F, linesep = "",
        caption = '(ref:cal-results)') %>% 
    kable_styling(position = "center", 
                  latex_options = c("scale_down","hold_position")) %>% 
    row_spec(18, hline_after = T) %>% 
    add_header_above(c(" " = 2, "Frequency (kHz)" = ncol(all.output) - 2))
}
```  

\newpage

## Data collection {#results-data-collection}
### Acoustic and net sampling {#results-acoustic-trawl-sampling}  

**Summarize transect effort in Mexico.**

_`r survey.vessel`_ entered Mexican waters on 7 October and sampling occurred from north to south. The survey spanned the area from approximately the U.S.-Mexico border in the north to Bahia Santa Maria near San Quintin, Baja California in the south. _`r survey.vessel`_ sampled `r nrow(nasc.summ)` east-west transects totaling `r prettyNum(sum(nasc.summ$distance), digits = 1, big.mark = ",")` nmi, and conducted `r nrow(trawl.summ)` Nordic trawls. On 15 October, _`r survey.vessel`_ arrived at the 10th Avenue Marine Terminal in San Diego to complete the survey.

### Ichthyoplankton and oceanographic sampling {#results-other-sampling}  

A total of `r nrow(uctd.sta)` UCTD casts were conducted, and `r prettyNum(nrow(cufes.raw), digits = 1, big.mark = ",")` CUFES samples were collected underway. The locations of CTD and UCTD stations are presented in **Fig. \@ref(fig:station-sampling)** and **Appendix \@ref(appendix-ctd-sampling)**.  

## Distribution of CPS {#results-cps-distribution}

Acoustic backscatter sampled by  _`r survey.vessel`_ and ascribed to CPS (**Fig. \@ref(fig:nasc-cufes-trawl)a**)  was observed throughout the survey area, but was most prevalent within ~10 nmi of shore. 

Fish eggs observed in the CUFES were mostly identified as Pacific Sardine eggs; however, these identifications must be verified (**Fig. \@ref(fig:nasc-cufes-trawl)b**). A few Jack Mackerel and Northern Anchovy eggs were observed (**Fig. \@ref(fig:nasc-cufes-trawl)b**).  

Northern Anchovy catches were predominant, by weight, in trawl samples collected throughout the survey area in Mexico (**Fig. \@ref(fig:nasc-cufes-trawl)c**). Jack Mackerel were most abundant in several trawls offshore (**Fig. \@ref(fig:nasc-cufes-trawl)c**) and Pacific Sardine were most abundant in several trawls close to shore (**Fig. \@ref(fig:nasc-cufes-trawl)c**). A few Pacific Mackerel were collected but are not visible at this scale (**Fig. \@ref(fig:nasc-cufes-trawl)c**). Overall, the `r nrow(trawl.summ)` trawls captured a combined `r prettyNum(haul.CPS.kg, digits = 1, big.mark = ",")` kg of CPS (`r prettyNum(haul.Sardine.kg, digits = 1, big.mark = ",")` kg Pacific Sardine, `r prettyNum(haul.Anchovy.kg, digits = 1, big.mark = ",")` kg Northern Anchovy, `r prettyNum(haul.JackMack.kg, digits = 1, big.mark = ",")` kg Jack Mackerel, and `r prettyNum(haul.PacMack.kg, digits = 1, big.mark = ",")` kg Pacific Mackerel; **Appendix \@ref(appendix-trawl-sampling)**).  

\newpage  

(ref:station-sampling) The locations of surface trawls (white points) and CTD and UCTD casts (red circles) relative to the east-west acoustic transects (black lines) and cruise track of _`r survey.vessel`_ (heavy gray line). **No bongo net samples were collected.**

```{r station-sampling,fig.cap='(ref:station-sampling)',out.height='8in',fig.align='center',fig.pos='H'}
include_graphics(here("Figs/fig_station_samples_mx.png"))
```  

\newpage  
\blandscape  

(ref:nasc-cufes-trawl) Survey transects overlaid with (a) the distribution of 38-kHz integrated backscattering coefficients ($s_A$, m^2^ nmi^-2^;  averaged over 2000-m distance intervals and from `r int.start`- to `r cps.depth`-m deep) ascribed to CPS; (b) egg densities (eggs m^-3^) for Northern Anchovy, Jack Mackerel, and Pacific Sardine from the CUFES; and (c) proportions, by weight, of CPS species in each trawl sample (black points indicate trawls with no CPS). Species with low catch weights may not visible at this scale.

```{r nasc-cufes-trawl,fig.cap='(ref:nasc-cufes-trawl)',out.width='8in',fig.pos='H'}
include_graphics(here("Figs/fig_nasc_cufes_haul_wt_mx.png"))
``` 

\elandscape
\newpage  

# Disposition of Data {#data-disposition}

```{r calc-raw-size, eval=FALSE}
if (calc.raw.size) {
  # calculate sizes of ER60, EK80, ME70, MS70, and SX90 .RAW files
  ek60.file.size <- sum(dir_info(file.path(survey.dir,'DATA/EK60/RAW'), 
                                 regexp = '.raw$', recursive = T)$size)
  ek80.file.size <- sum(dir_info(file.path(survey.dir,'DATA/EK80/RAW'), 
                                 regexp = '.raw$', recursive = T)$size)
  me70.file.size <- sum(dir_info(file.path(survey.dir,'DATA/ME70/RAW'), 
                                 regexp = '.raw$', recursive = T)$size)
  ms70.file.size <- sum(dir_info(file.path(survey.dir,'DATA/MS70/RAW'), 
                                 regexp = '.raw$', recursive = T)$size)
  sx90.file.size <- sum(dir_info(file.path(survey.dir,'DATA/SX90/RAW'), 
                                 regexp = '.raw$', recursive = T)$size)
  
  save(ek60.file.size, ek80.file.size, me70.file.size, ms70.file.size, sx90.file.size,
       file = here("Output/raw_file_info.Rdata"))
} else {
  load(here("Output/raw_file_info.Rdata"))
}
```  

All raw EK80, ME70, MS70, and SX90 data are archived on the SWFSC data server. For more information, contact: David Demer (Southwest Fisheries Science Center, 8901 La Jolla Shores Drive, La Jolla, California, 92037, U.S.A.; phone: 858-546-5603; email: <david.demer@noaa.gov>).

# Acknowledgements {#acknowledgements}  

We thank the crew members of NOAA Ship _`r survey.vessel`_, as well as the scientists that participated in the sampling operations at sea. We especially thank our collaborators from Mexico **[Isaac, others?]**. We also thank Wendy Bradfield-Smith, Graciela Alvarez, Ed Gorecki, **[other major contributors? Cisco Werner, Adm. Chad Cary, Dr. Arenas?]** and many others who assisted in the preparation and approval of the Mexican research permit.  

# References {-}  

<div id = 'refs'></div>  

\newpage  

# (APPENDIX) Appendix {-}
# Appendix {-} 
# CTD and UCTD sampling locations {#appendix-ctd-sampling}

Times and locations of conductivity and temperature versus depth casts while on station (CTD) and underway (UCTD).  

```{r uctd-sample-table,results='asis'}
# Rename
all.ctds.table <- all.ctds %>% 
  mutate(Button = str_replace(Button, " Cast","")) %>% 
  rename(`Date Time` = Date, `Cast Type` = Button,
         `Latitude` = Latitude, `Longitude` = Longitude)

if (doc.type == "docx") {
  # create flextable object (for Word)
  pander(all.ctds.table)
} else {
  # print LaTeX table for HTML or PDF
  kable(all.ctds.table,
        align = c("l","l","c","c"), 
        digits = c(0,0,4,4),
        escape = F, longtable = T, 
        booktabs = T) %>% 
    kable_styling(position = "center", 
                  latex_options = c("repeat_header","hold_position"))
}
```  

\newpage
\blandscape

# Trawl sample summary {#appendix-trawl-sampling}

Date, time, location at the start of trawling (i.e., at net equilibrium, when the net is fully deployed and begins fishing), and biomasses (kg) of CPS collected for each trawl haul conducted in Mexican waters.

```{r trawl-catch-summary,results='asis'}
# Summarize catch by species, used to subset columns
pos.cps <- trawl.summ %>% 
  select(-Haul, -Date, -Latitude, -Longitude, -"All CPS") %>% 
  gather(key = "scientificName", value = "weight") %>% 
  group_by(scientificName) %>% 
  summarise(weight = sum(weight, na.rm = T)) %>% 
  filter(weight != 0) %>% 
  pull(scientificName)

# Select only species with positive catch weight
trawl.summ <- trawl.summ %>% 
  filter(Latitude <= 32.53) %>% 
  select(Haul, `Date Time` = Date, `Latitude` = Latitude, 
         `Longitude` = Longitude,
         all_of(pos.cps), "All CPS") %>% 
  arrange(Haul)

# Replace zeros with NA
trawl.summ[trawl.summ == 0] <- NA

if (doc.type == "docx") {
  # create kable object (for Word)
  pander(trawl.summ)
} else {
  # print LaTeX table for HTML or PDF
  kable(trawl.summ,escape = F,longtable = T,booktabs = T,
        align = c("r","c",rep("r",ncol(trawl.summ) - 2)),
        digits = c(0,0,4,4,rep(2, ncol(trawl.summ) - 4))) %>% 
    kable_styling(position = "center", 
                  latex_options = c("repeat_header","hold_position"))
}
```  

\elandscape
