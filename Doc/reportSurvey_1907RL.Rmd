---
output: 
  bookdown::pdf_document2:
    number_sections: yes
    toc: no
    includes: 
      in_header: yaml/header.tex
  bookdown::word_document2:
    reference_docx: template/report_template_Rmarkdown.docx
csl: csl/ices-journal-of-marine-science.csl
bibliography: bib/ast_bib.bib
---
```{r load-libraries, echo=F, error=F, message=F, warning=F}
# Install and load pacman (library management package)
if (!require("pacman")) install.packages("pacman")

# Install and load required packages from CRAN ---------------------------------
pacman::p_load(tidyverse,swfscMisc,readr,pander,kableExtra,bookdown,mapview,
          knitr,ggmap,maps,readxl,RSQLite,shadowtext,xml2,sf,odbc,ggspatial,
          maptools,png,grid,gridExtra,cowplot,flextable,fs,magick,
          stringr,xtable,devtools,gdata,reshape2,lubridate,rworldmap,
          scatterpie,forcats,here,viridis,rnaturalearth,rworldxtra)

# Install and load required packages from Github -------------------------------
# surveyR
pacman::p_load_gh("kstierhoff/surveyR")
# atm
pacman::p_load_gh("kstierhoff/atm")
# rnaturalearth data
pacman::p_load_gh("ropenscilabs/rnaturalearthdata")
pacman::p_load_gh("ropenscilabs/rnaturalearthhires")

# determines method of table generation (whether kable or xtable) for best formatting
doc.type <- knitr::opts_knit$get('rmarkdown.pandoc.to')
if (is.null(doc.type)) {doc.type <- "html"}

# global knitr chunk options
knitr::opts_chunk$set(echo = F, warning = F, message = F,progress = T,
                      fig.align = 'center',dev = "png",
                      dev.args = list(type = "cairo"),dpi = 150)

# determine global knitr table format
if (doc.type == "latex") {
  knitr.format <- "latex"
} else {
  knitr.format <- "html" 
}

# set global knitr table format
options(knitr.table.format = knitr.format,knitr.kable.NA = '')

# global pander options
panderOptions('table.style','rmarkdown'); panderOptions('table.split.table', Inf); panderOptions('digits', 6);
panderOptions('round', 6); panderOptions('keep.trailing.zeros', T); panderOptions('missing', "")

# Register Google Maps API
register_google(key = google_map_api)
```

```{r user-input}
# Get project name from directory
prj.name <- last(unlist(str_split(here(), "/")))

# Get all settings files
settings.files <- dir(here("Doc/settings"))

# Source survey settings file
prj.settings <- settings.files[str_detect(settings.files, paste(prj.name, ".R", sep = ""))]
source(here("Doc/settings", prj.settings))

# Define ggplot theme
theme_set(theme_bw())
```

```{r controls, echo=F, error=F, message=F, warning=F}
# Processing instructions (T/F)
copy.files      <- F # copy files from source
download.maps   <- F # download habitat map
save.figs       <- F # draw plots and maps, or use existing
resize.map      <- F # Resize map during survey; if T, uses anticipated bounds of survey area
process.scs     <- F # process SCS logs, or load processed data
get.nav         <- F # download nave data from ERDDAP
process.csv     <- F # process CSV files from Echoview
process.csv.all <- F # Process all CSV files (F = only new files)
process.cal     <- F # process calibration results from previous surveys for time series plots
process.zmux    <- F # process impedance data
calc.raw.size   <- F # computer RAW file size, or use existing
```

```{r copy-bib,include=F}
if (copy.files) {
  # Update bibliography and CSL
  file.copy("//swc-storage1/ast1/LITERATURE/Rmarkdown/csl/ices-journal-of-marine-science.csl",
            "csl", overwrite = T)
  file.copy("//swc-storage1/AST1/LITERATURE/Rmarkdown/bib/ast_bib.bib",
            "bib", overwrite = T)
}
```

```{r download-habitat-maps}
if (download.maps) {
  # If survey is ongoing, use today's date to get habitat maps
  if (date(erddap.survey.end) > date(now())) {
    erddap.survey.end <- date(now()) - days(2)
  } 
  # Calculate the number of days in the survey
  hab.days <- floor((ymd(erddap.survey.end) - ymd(erddap.survey.start))/3)
  
  # Format survey start date for downloading sardine potential habitat maps
  hab.date.start <- format(ymd(erddap.survey.start), "%Y%m%d")
  hab.date.mid1  <- format(ymd(erddap.survey.start) + hab.days, "%Y%m%d")
  hab.date.mid2  <- format(ymd(erddap.survey.start) + hab.days*2, "%Y%m%d")
  hab.date.end   <- format(ymd(erddap.survey.end), "%Y%m%d")
  
  # Create URLs for downloading downloading sardine potential habitat maps
  hab.url.start <- paste("http://swfscdata.nmfs.noaa.gov/AST/sardineHabitat/images/",
                         hab.date.start,"_Habitat.png", sep = "")
  hab.url.mid1  <- paste("http://swfscdata.nmfs.noaa.gov/AST/sardineHabitat/images/",
                         hab.date.mid1,"_Habitat.png", sep = "")
  hab.url.mid2  <- paste("http://swfscdata.nmfs.noaa.gov/AST/sardineHabitat/images/",
                         hab.date.mid2,"_Habitat.png", sep = "")
  hab.url.end   <- paste("http://swfscdata.nmfs.noaa.gov/AST/sardineHabitat/images/",
                         hab.date.end,"_Habitat.png", sep = "")
  
  # Download sardine potential habitat map files
  download.file(hab.url.start, here("Figs/fig_habitat_start.png"), mode = "wb") 
  download.file(hab.url.mid1,  here("Figs/fig_habitat_mid1.png"), mode = "wb") 
  download.file(hab.url.mid2,  here("Figs/fig_habitat_mid2.png"), mode = "wb") 
  download.file(hab.url.end,   here("Figs/fig_habitat_end.png"), mode = "wb") 
  
  # Create cowplot objects for map grid
  hab.start <- ggdraw() + draw_image(here("Figs/fig_habitat_start.png")) 
  hab.mid1  <- ggdraw() + draw_image(here("Figs/fig_habitat_mid1.png"))
  hab.mid2  <- ggdraw() + draw_image(here("Figs/fig_habitat_mid2.png"))
  hab.end   <- ggdraw() + draw_image(here("Figs/fig_habitat_end.png")) 
  
  # Create final figure
  hab.map <- plot_grid(hab.start, hab.mid1, hab.mid2, hab.end, 
                       nrow = 2, labels = c("a)","b)","c)","d)"))  
  
  # Save final figure
  ggsave(hab.map, filename = here("Figs/fig_habitat_map.png"),
         width = 8, height = 8)
}
```

```{r process-cal-all}
if (process.cal) {
  source(here("Code/extractCalAll.R"))
} else {
  load(here("Data/Calibration/cal_results_all.Rdata"))
}

if (save.figs) {
  source(here("Code/plotCalTimeSeries.R"))
}
```  

```{r process-cal}
# Get results from current survey ---------------------------------------------
cal.res <- cal.res.all %>% 
  mutate(cal_date = mdy(cal_date)) %>% 
  filter(between(cal_date,
                 ymd(cal.plot.date) - days(cal.window),
                 ymd(cal.plot.date) + days(cal.window))) %>% 
  arrange(txdr_freq)


# create data frame for calibration parameters
cal.params <- cal.res %>% 
  select(txdr_freq,txdr_type,txdr_sn,gpt_power,gpt_pd,
         txdr_gain,txdr_sa_corr,gpt_rcr_bw,gpt_si,
         txdr_2way_ba,env_alpha,txdr_alon_ang_sens,
         txdr_athw_ang_sens,txdr_alon_ba,txdr_athw_ba,
         txdr_alon_oa,txdr_athw_oa,target_ts) 

# add noise estimates
# if noise isn't measured, enter -999 in the User Inputs, else use noise estimates
if (is.na(cal.noise[cal.vessels])) { 
  cal.params$noise <- rep("N/A", length(cal.files))
}else{
  cal.params$noise <- unlist(cal.noise[cal.vessels])
}

# create data frame for beam model results
bm.res <- cal.res %>% 
  select(txdr_freq,bm_txdr_gain,bm_sa_corr,dev_bm_rms,
         bm_alon_ba,bm_athw_ba,bm_alon_oa,bm_athw_oa) 

# create a data frame for echosounder settings
echo.settings <- cal.res %>% 
  select(txdr_freq,gpt_pd,gpt_si,gpt_rcr_bw,gpt_power,txdr_z,env_alpha) 

# create names for cal parameters and beam model results
names(cal.params) <- c("Frequency","Model","Serial Number","Transmit Power ($p_\\mathrm{et}$)",
                       "Pulse Duration ($\\tau$)","On-axis Gain ($G_0$)",
                       "$S_\\mathrm{a}$ Correction ($S_\\mathrm{a}\\mathrm{corr}$)",
                       "Bandwidth ($W_\\mathrm{f}$)","Sample Interval",
                       "Eq. Two-way Beam Angle ($\\mathrm{\\Psi}$)",
                       "Absorption Coefficient ($\\alpha_\\mathrm{f}$)",
                       "Angle Sensitivity Along. ($\\mathrm{\\Lambda}_{\\alpha}$)",
                       "Angle Sensitivity Athw. ($\\mathrm{\\Lambda}_{\\beta}$)",
                       "3-dB Beamwidth Along. ($\\alpha_\\mathrm{-3dB}$)",
                       "3-dB Beamwidth Athw. ($\\beta_\\mathrm{-3dB}$)",
                       "Angle Offset Along. ($\\alpha_{0}$)","Angle Offset Athw. ($\\beta_{0}$)",
                       "Theoretical TS ($TS_\\mathrm{theory}$)",
                       "Ambient Noise")

names(bm.res) <- c("Frequency","On-axis Gain ($G_0$)",
                   "$S_\\mathrm{a}$ Correction ($S_\\mathrm{a}\\mathrm{corr}$)","RMS",
                   "3-dB Beamwidth Along. ($\\alpha_\\mathrm{-3dB}$)",
                   "3-dB Beamwidth Athw. ($\\beta_\\mathrm{-3dB}$)",
                   "Angle Offset Along. ($\\alpha_{0}$)",
                   "Angle Offset Athw. ($\\beta_{0}$)")

names(echo.settings) <- c("Frequency","Pulse Duration ($\\mu s$)","Sample Interval (m)",
                          "Bandwidth (Hz)","Transmit Power (W)",
                          "Transducer Depth (m)","Absorption Coefficient (dB km$\\^{-1}$)")

# cast output by frequency and transducer model number
param.output  <- suppressMessages(dcast(melt(cal.params, id.vars = "Frequency"), variable~Frequency))
bm.output     <- suppressMessages(dcast(melt(bm.res, id.vars = "Frequency"), variable~Frequency))

# add a column with parameter units
param.units <- as.data.frame(c(" "," ","W","ms","dB re 1","dB re 1","Hz","m","dB re 1 sr","dB km$^{-1}$",
                               "Elec.$^\\circ$/Geom.$^\\circ$","Elec.$^\\circ$/Geom.$^\\circ$",
                               "deg","deg","deg","deg","dB re 1 m$^{2}$","dB re 1 W"))

# add a column with beam model units
bm.units <- as.data.frame(c("dB re 1","dB re 1","dB","deg","deg","deg","deg"))

# add label for first column
names(param.units) <- "Units"
names(bm.units)    <- "Units"

# add units to output and arrange columns
param.output <- bind_cols(param.output, param.units) %>% 
  select(variable, Units, everything()) %>% 
  rename("Frequency ($f$, kHz)" = variable)

bm.output <- bind_cols(bm.output, bm.units) %>% 
  select(variable, Units, everything()) %>% 
  rename("Frequency ($f$, kHz)" = variable)

# combine results data frames
all.output <- rbind(param.output, bm.output) %>% 
  rename(" " = "Frequency ($f$, kHz)")

# save output to .Rdata and CSV
save(all.output,
     file = here("Output/cal_output_table.Rdata"))

write.csv(all.output,
          file = here("Output/cal_output_table.csv"), 
          quote = F, row.names = F)
```

```{r plot-cal-pings}
# Get ping data from current survey and format for ploting
cal.pings <- cal.pings.all %>% 
  mutate(cal_date = date(date_time)) %>% 
   filter(between(cal_date,
                 ymd(cal.plot.date) - days(cal.window),
                 ymd(cal.plot.date) + days(cal.window))) %>% 
  arrange(txdr_freq, ping_num)

# Set axis limits based on range of ping angles
cal.lim.tmp <- round(max(max(cal.pings$along), max(cal.pings$athw))) 

if (cal.lim.tmp %% 2) {
  # If range is odd, add 1 to make axis ticks look nice
  cal.axis.lims <- c(-(cal.lim.tmp + 1), cal.lim.tmp + 1)
} else {
  cal.axis.lims <- c(-cal.lim.tmp, cal.lim.tmp)
}

# subset only outlier points
outliers <- filter(cal.pings, outlier == 1)

cal.pings <- cal.pings %>% 
  left_join(select(cal.res,txdr_freq,txdr_type,target_ts,
                   txdr_gain,bm_txdr_gain,
                   bm_alon_ba,bm_athw_ba,
                   bm_alon_oa,bm_athw_oa)) %>% 
  mutate(
    txdr_type      = fct_reorder(txdr_type, txdr_freq),
    TS_u_new       = TS_u + 2*(txdr_gain - bm_txdr_gain),
    alpha          = along - bm_alon_oa,
    beta           = athw - bm_athw_oa,
    x              = (2*alpha) / bm_alon_ba,
    y              = (2*beta) / bm_athw_ba,
    B              = 6.0206*(x^2 + y^2 - 0.18*x^2*y^2),
    TS_c_new       = TS_u_new + B,
    relTS_c        = TS_c_new - target_ts,
    relTS_c_scaled = case_when(
      relTS_c >= 1 ~ 1,
      relTS_c <= -1 ~-1,
      between(relTS_c,-1,1) ~ relTS_c))

# Plot beam-uncompensated target strength data #####
tsu.scatter <- ggplot(filter(cal.pings, outlier == 0), aes(athw, along)) +
  geom_point(aes(colour = TS_u)) + 
  geom_point(data = filter(cal.pings, outlier == 1), aes(athw, along), 
             shape = 3, size = 1, alpha = 0.7) +
  facet_wrap(~txdr_type, scales = cal.scales) +
  scale_colour_viridis(name = expression(paste(italic(TS)[u]," (dB)",sep = "")),
                       option = "magma") +
  scale_x_continuous('\nAthwartship Beam Angle (deg)',limits = cal.axis.lims,
                     breaks = seq(min(cal.axis.lims), max(cal.axis.lims), 2)) +
  scale_y_continuous('Alongship Beam Angle (deg)\n',limits = cal.axis.lims,
                     breaks = seq(min(cal.axis.lims), max(cal.axis.lims), 2)) +
  guides(size =  F) + theme_bw() + 
  theme(panel.spacing    = unit(1, "lines"),
        strip.background = element_rect(fill = "white"),
        strip.text.x     = element_text(face = "bold")) +
  coord_equal()

# Save TS_c plot 
ggsave(here("Figs/fig_cal_TSu_scatter.png"), tsu.scatter, 
       width = 9, height = 6)

# Plot beam-compensated target strength data #####
tsc.scatter <- ggplot(filter(cal.pings, outlier == 0), aes(athw, along)) +
  geom_point(aes(fill = relTS_c_scaled),shape = 21) + 
  geom_point(data = filter(cal.pings, outlier == 1), aes(athw, along),
             shape = "+", size = 4) +
  facet_wrap(~txdr_type, scales = cal.scales) + 
  scale_fill_distiller(name = expression(italic(TS)[rel]),
                       type = "div", palette = "RdBu", limits = c(-1,1)) +
  scale_x_continuous('\nAthwartship Beam Angle (deg)', limits = cal.axis.lims,
                     breaks = seq(min(cal.axis.lims), max(cal.axis.lims), 2)) +
  scale_y_continuous('Alongship Beam Angle (deg)\n',limits = cal.axis.lims,
                     breaks = seq(min(cal.axis.lims), max(cal.axis.lims), 2)) +
  theme_bw() + 
  theme(panel.spacing = unit(1, "lines"),
        strip.background = element_rect(fill = "white"),
        strip.text.x = element_text(face = "bold")) +
  coord_equal()

# Save TS_c plot 
ggsave(here("Figs/fig_cal_TSrel_scatter.png"), tsc.scatter,
       width = 9, height = 6)
```

```{r process-nav}
if (get.nav) {
  # Load existing nav data
  if (file.exists(here("Data/Nav/nav_data.Rdata"))) {
    load(here("Data/Nav/nav_data.Rdata"))
    
    # Calculate difference between max nav time and now
    nav.lag <- difftime(now(tzone = "UTC"), max(ymd_hms(nav$time)), units = "hours")
    
    # Get new erddap start date from max date
    erddap.survey.start <- max(date(nav$time))
  }
  
  if (nav.lag > 24) {
    # Generate ERDDAP URL
    dataURL <- URLencode(paste(
      "http://coastwatch.pfeg.noaa.gov/erddap/tabledap/fsuNoaaShip",
      erddap.vessel, ".csv0?", erddap.vars,
      "&time>=", erddap.survey.start, "&time<=", erddap.survey.end,
      sep = ""))
    
    # Download and parse ERDDAP nav data
    nav.temp <- data.frame(read.csv(dataURL, header = F, colClasses = erddap.classes, 
                                    row.names = NULL, skip = 0))
    names(nav.temp) <- erddap.headers
    
    # Filter to remove bad SST values
    nav.temp <- nav.temp %>% 
      mutate(long     = long - 360,
             SOG      = SOG * 1.94384,
             datetime = ymd_hms(time),
             SST      = na_if(SST, NaN),
             leg      = paste("Leg", cut(as.numeric(date(datetime)), 
                                         leg.breaks, labels = F))) %>%
      filter(is.nan(SOG) == F, SOG > 0, SOG < 15,
             between(lat, min(survey.lat), max(survey.lat)), 
             between(long, min(survey.long), max(survey.long)))
    
    # Append new nav data
    if (exists("nav")) {
      nav <- bind_rows(nav, nav.temp) %>% 
        distinct()
    } else {
      nav <- nav.temp
    }
  }
  
  # Convert nav to spatial
  nav.sf <- st_as_sf(nav, coords = c("long","lat"), crs = crs.geog) 
  
  # Cast nav to transects
  nav.paths.sf <- nav.sf %>% 
    group_by(leg) %>% 
    summarise(do_union = F) %>% 
    st_cast("LINESTRING")
  
  # Save results
  save(nav, nav.sf, nav.paths.sf, file = here("Data/Nav/nav_data.Rdata"))
} else {
  # Load nav data
  load(here("Data/Nav/nav_data.Rdata"))
}

# Read transect waypoints
wpts <- read_csv(here("Data/Nav", wpt.filename))

# Convert planned transects to sf; CRS = crs.geog
wpts.sf <- wpts %>% 
  filter(Type %in% wpt.types) %>% 
  st_as_sf(coords = c("Longitude","Latitude"), crs = crs.geog)

transects.sf <- wpts.sf %>% 
  group_by(Type, Transect) %>% 
  summarise(do_union = F) %>% 
  st_cast("LINESTRING")

# Create gps.csv file from nav to replace missing data in Echoview
nav.gps <- nav %>% 
      mutate(GPS_date = format(datetime, format = "%F"),
             GPS_time = format(datetime, format = "%T")) %>% 
      select(GPS_date, GPS_time, latitude = lat, longitude = long)

write_csv(nav.gps, here("Output/nav.gps.csv"))

# Get most recent vessel position for plotting
nav.now <- tail(nav.sf, 1) %>% 
  mutate(label = paste("Last position:", datetime, "GMT"))
```

```{r create-basemap,include=F}
# Configure base map options -----------------
# Import landmarks
locations <- filter(read.csv(here("Data/Map/locations.csv")), name %in% label.list) %>%
  project_df(to = crs.proj)

# Get state data
states <- ne_states(country = 'United States of America', returnclass = 'sf')
ca     <- filter(states, name == "California")

# Get countries
countries <- ne_countries(scale = "large", returnclass = "sf") %>%
  filter(subregion %in% c("Northern America","Central America"))

# Read bathy contours shapefile 
bathy <- st_read(here("Data/GIS/bathy_contours.shp")) %>% 
  st_transform(crs.geog) %>% 
  rename(Depth = Contour)

# Set padding around data  
if (resize.map) {
  # Use nav data to resize map to survey progress
  map.bounds <- nav.paths.sf %>% 
    st_transform(crs = crs.proj) %>%
    st_bbox()  
} else {
  # Use nav data to resize map to survey progress
  map.bounds <- transects.sf %>%
    st_transform(crs = crs.proj) %>%
    st_bbox()  
}

# Determine map aspect ratio and set height and width
map.aspect <- (map.bounds$xmax - map.bounds$xmin)/(map.bounds$ymax - map.bounds$ymin)
map.height <- 10
map.width  <- map.height*map.aspect

# Create base map
base.map <- get_basemap(nav.paths.sf, states, countries, locations, bathy, map.bounds, crs = crs.proj) +
  # Add scalebar
  annotation_scale(style = "ticks", location = "br", height = unit(0.15, "cm"))

# Save the basemap
ggsave(base.map,file = here("Figs/fig_basemap.png"), 
       height = map.height, width = map.width)

save(base.map, file = here("Data/Map/basemap.Rdata"))
```

```{r process-scs}
if (process.scs) {
  if (scs.source == "CSV") {
    # Process bridge event data #####
    bridge.events <- list.files(here("Data/SCS"), pattern = "MOA Snap.*csv",
                                full.names = T, recursive = T)  
    # Import from CSV
    bridge.snap <- fs::path(here("Data/SCS")) %>% 
      dir_ls(regexp = "MOA.*csv") %>% 
      map_df(read_csv) %>% 
      select(Date, Time, Button, Notes = notes.hdr, 
             Lat = gps.lat.hdr, Lon = gps.lon.hdr)
      
  } else {
    # Process bridge event data 
    bridge.events <- list.files(here("Data/SCS"), pattern = "MOA Snap.*elg",
                                full.names = T, recursive = T)
    # create temporary df
    bridge.snap <- data.frame()
    # read and rbind all files
    for (i in bridge.events) {
      bridge.snap.tmp <- read_csv(i) %>% 
        select("Date", "Time", "Button", "Notes" = notes.hdr, 
               "Lat" = gps.lat.hdr, "Lon" = gps.lon.hdr) 
      
      bridge.snap <- bind_rows(bridge.snap, bridge.snap.tmp)
    }
  }
  
  # Format data
  bridge.snap <- bridge.snap %>% 
    filter(Lon != "", Lat != "") %>% 
    mutate(datetime = mdy_hms(paste(Date, Time)),
           Lat      = as.numeric(substr(Lat, 1, 2)) + 
                      as.numeric(substr(Lat, 3, 7))/60,
           Lon      = -(as.numeric(substr(Lon, 1, 3)) +
                        as.numeric(substr(Lon, 4, 8))/60),
           Button = case_when(
             Button == cb.flush.button ~ "Retracted (5 m)",
             Button == cb.int.button ~  "Intermediate (7 m)",
             Button == cb.ext.button ~  "Extended (9 m)",
             TRUE ~ Button)) %>% 
    arrange(datetime) %>% 
    mutate(datetime =  format(datetime, format = "%m/%d/%Y %H:%M")) 
  
  # save processed SCS data
  save(bridge.snap, file = here("Data/SCS/processed_logs.Rdata"))
} else {
  # load processed SCS data
  load(here("Data/SCS/processed_logs.Rdata"))
}
```

```{r import-trawl-data}
if (trawl.source == "SQL") {
  # Configure ODBC connection to TRAWL database
  trawl.channel  <- dbConnect(odbc(), 
                 Driver = "SQL Server", 
                 Server = "161.55.235.187", 
                 Database = "TRAWL", 
                 Trusted_Connection = "True")
} else if (trawl.source == "Access") {
  trawl.channel  <- dbConnect(odbc::odbc(), 
                    Driver = "Microsoft Access Driver (*.mdb, *.accdb)", 
                    DBQ = file.path(here("Data/Trawl"), trawl.db.access))
}

# Import trawl database tables
catch.all	     <- tbl(trawl.channel,"Catch") %>% collect()
haul.all       <- tbl(trawl.channel,"Haul") %>% collect()
lengths.all    <- tbl(trawl.channel,"Specimen") %>% collect()
lengthFreq.all <- tbl(trawl.channel,"LengthFrequency") %>% collect()
spp.codes      <- tbl(trawl.channel,"SpeciesCodes") %>% collect()

# Close database channel
dbDisconnect(trawl.channel)

# Save imported database data to .Rdata file
save(catch.all, haul.all, lengths.all, spp.codes, lengthFreq.all, 
     file = here("Data/Trawl/trawl_data.Rdata"))
```

```{r process-trawl-haul-data}
# Create startLatitudeDecimal and startLongitudeDecimal for Access data
if (trawl.source == "Access") {
  # Reformat haul data to match SQL
  haul.all <- haul.all %>% 
    mutate(
      startLatDecimal  =   startLatitudeDegrees + (startLatitudeMinutes/60),
      startLongDecimal = -(startLongitudeDegrees + (startLongitudeMinutes/60)),
      stopLatDecimal   =   stopLatitudeDegrees + (stopLatitudeMinutes/60),
      stopLongDecimal  = -(stopLongitudeDegrees + (stopLongitudeMinutes/60)),
      equilibriumTime  =   ymd_hms(paste(as.character(trawlDate),
                                         format(haul.all$EquilibriumTime, 
                                                format = "%H:%M:%S"))),
      haulBackTime     =   ymd_hms(paste(as.character(trawlDate),
                                         format(haul.all$haulbackTime,
                                                format = "%H:%M:%S")))) %>% 
    mutate(haulBackTime = case_when(
      haulBackTime < equilibriumTime ~ haulBackTime + days(1),
      TRUE ~ haulBackTime)) %>%
    rename(cruise = Cruise, ship = Ship, haul = Haul, 
           collection = Collection, notes = Notes)
  
  # Identify hauls where date of equilibriumTime or haulBackTime is incorrect
  eq.fix <- which(c(0, diff(haul.all$equilibriumTime)) < 0)
  hb.fix <- which(c(0, diff(haul.all$haulBackTime)) < 0)
  
  # Correct equilibriumTime or haulBackTime
  haul.all$equilibriumTime[eq.fix] <- haul.all$equilibriumTime[eq.fix] + days(1)
  haul.all$haulBackTime[eq.fix]    <- haul.all$haulBackTime[eq.fix] + days(1)
  
  # Reformat length frequency data to match SQL
  lengths.all <- lengths.all %>% 
    rename(cruise = Cruise, ship = Ship, haul = Haul, 
           collection = Collection, species = Species)
  
  # Reformat length frequency data to match SQL
  lengthFreq.all <- lengthFreq.all %>% 
    rename(cruise = Cruise, ship = Ship, haul = Haul, collection = Collection, 
           species = Species, length = Length, lengthType = LengthType, 
           sexUnknown = NotDetermined, male = Male, activeFemale = ActiveFemale, 
           inactiveFemale = InactiveFemale, totalFemale = TotalFemale, 
           subSampleNumber = SubSampleNumber)
} else if (trawl.source == "SQL") {
  haul.all <- haul.all %>% 
    mutate(
      equilibriumTime = ymd_hms(equilibriumTime),
      haulBackTime    = ymd_hms(haulBackTime))
}

# Classify hauls by season (spring or summer)
haul.all <- haul.all %>% 
  mutate(season = case_when(
    month(equilibriumTime) < 6 ~ "spring",
    TRUE ~ "summer"))

# Filter haul data for current survey
haul <- haul.all %>% 
  select(cruise, ship, haul, collection, startLatDecimal, startLongDecimal, 
         stopLatDecimal, stopLongDecimal, equilibriumTime, haulBackTime, 
         trawlPerformance, season, notes) %>% 
  filter(cruise %in% cruise.name & ship %in% cruise.ship) %>%
  # Calculate haul duration
  mutate(duration = difftime(haulBackTime, equilibriumTime, units = "mins")) %>% 
  # Remove bad trawls
  filter(!trawlPerformance %in% trawl.performance) %>% 
  # Assign cluster based on yearday
  mutate(cluster = cumsum(c(0, diff(equilibriumTime)) > 12) + 1) %>% 
  droplevels() # Remove unused factor levels

# # Get haul starts
# Find midpoint of each haul as the mean lat/lon
haul.mid <- haul %>% 
  group_by(cluster, haul) %>% 
  summarise(
    lat  = mean(c(startLatDecimal, stopLatDecimal)),
    long = mean(c(startLongDecimal, stopLongDecimal)))

# Convert haul paths and midpoints to sf; CRS = crs.geog
# Create haul paths from starts and ends
haul.paths <- select(haul, haul, lat = startLatDecimal, long = startLongDecimal) %>% 
  bind_rows(select(haul, haul, lat = stopLatDecimal, long = stopLongDecimal)) %>% 
  arrange(haul) %>% 
  st_as_sf(coords = c("long","lat"), crs = crs.geog) %>% 
  group_by(haul) %>% 
  summarise(do_union = F) %>% 
  st_cast("LINESTRING") 

haul.locs.sf <- haul.mid %>% 
  mutate(label = paste("Haul", haul)) %>% 
  st_as_sf(coords = c("long","lat"), crs = crs.geog) 

# Find midpoint of each haul cluster as the average of haul midpoints
cluster.mid <- haul.mid %>% 
  group_by(cluster) %>% 
  summarise(
    lat  = mean(lat),
    long = mean(long))  

# Save haul data
save(haul, file = here("Output/haul_info.Rdata"))
``` 

```{r process-catch-data}
# Filter catch data
catch <- catch.all %>% 
  left_join(dplyr::select(spp.codes, species, scientificName, commonName)) %>% 
  filter(cruise %in% cruise.name & ship %in% cruise.ship & 
           scientificName %in% cps.spp & netSampleType == 'codend') %>% 
  left_join(dplyr::select(haul, haul, cluster)) %>% 
  mutate(key = paste(haul, scientificName),
         totalWeight = subSampleWtkg + remainingSubSampleWtkg)

if (nrow(catch) > 0) {
  # Summarize trawl catch by species
  haul.summ.wt <- catch %>% 
    select(haul, cluster, scientificName, totalWeight) %>% 
    tidyr::spread(scientificName, totalWeight) 
  
  # Add species with zero total weight
  if (!has_name(haul.summ.wt, "Engraulis mordax"))      {haul.summ.wt$`Engraulis mordax`      <- 0}
  if (!has_name(haul.summ.wt, "Sardinops sagax"))       {haul.summ.wt$`Sardinops sagax`       <- 0}
  if (!has_name(haul.summ.wt, "Scomber japonicus"))     {haul.summ.wt$`Scomber japonicus`     <- 0}
  if (!has_name(haul.summ.wt, "Trachurus symmetricus")) {haul.summ.wt$`Trachurus symmetricus` <- 0}
  if (!has_name(haul.summ.wt, "Clupea pallasii"))       {haul.summ.wt$`Clupea pallasii`       <- 0}
  if (!has_name(haul.summ.wt, "Atherinopsis californiensis")) {haul.summ.wt$`Atherinopsis californiensis` <- 0}
  
  # Calculate total weight of all CPS species
  haul.summ.wt <- haul.summ.wt %>%  
    replace(is.na(.), 0) %>% 
    mutate(AllCPS = rowSums(select(., -haul, -cluster))) %>%
    # mutate(AllCPS = rowSums(.[, 3:ncol(.)])) %>%
    rename("Jacksmelt"  = "Atherinopsis californiensis",
           "PacHerring" = "Clupea pallasii",
           "Anchovy"    = "Engraulis mordax",
           "Sardine"    = "Sardinops sagax",
           "PacMack"    = "Scomber japonicus",
           "JackMack"   = "Trachurus symmetricus") 
  
  # Summarise catch by cluster
  cluster.summ.wt <- haul.summ.wt %>% 
    select(-haul, -AllCPS) %>% 
    group_by(cluster) %>% 
    summarise_all(list(sum)) %>% 
    mutate(AllCPS = rowSums(select(., -cluster))) %>% 
    right_join(cluster.mid) %>% 
    replace(is.na(.), 0)
  
  # Add lat/long to haul summary for plotting
  haul.summ.wt <- haul.summ.wt %>% 
    right_join(haul.mid) %>% 
    replace(is.na(.), 0)
  
} else {
  # Summarize trawl catch by species
  haul.summ.wt <- bind_cols(select(haul, haul, cluster),
                             data.frame(
                               "Jacksmelt"  = rep(0, nrow(haul)),
                               "PacHerring" = rep(0, nrow(haul)),
                               "Anchovy"    = rep(0, nrow(haul)),
                               "Sardine"    = rep(0, nrow(haul)),
                               "PacMack"    = rep(0, nrow(haul)),
                               "JackMack"   = rep(0, nrow(haul)),
                               "AllCPS"     = rep(0, nrow(haul)))) %>% 
    right_join(haul.mid)
                             
  # Summarise catch by cluster
  cluster.summ.wt <- haul.summ.wt %>% 
    select(-haul, -AllCPS) %>% 
    group_by(cluster) %>% 
    summarise_all(list(sum)) %>% 
    mutate(AllCPS = rowSums(select(., -cluster))) %>%
    right_join(cluster.mid) %>% 
    replace(is.na(.), 0)
}

# Prepare catch data for plotting ----------------------------------------------
# Select and rename trawl data for pie charts
haul.pie <- haul.summ.wt %>% 
  select(haul, long, lat, Anchovy, JackMack, 
         Jacksmelt, PacHerring, PacMack, Sardine, AllCPS) %>% 
  project_df(to = crs.proj)

cluster.pie <- cluster.summ.wt %>% 
  select(cluster, long, lat, Anchovy, JackMack, 
         Jacksmelt, PacHerring, PacMack, Sardine, AllCPS) %>% 
  project_df(to = crs.proj)

# Filter for positive hauls and clusters
haul.pos <- filter(haul.pie, AllCPS > 0) %>% 
  arrange(desc(X))

cluster.pos <- filter(cluster.pie, AllCPS > 0) %>% 
  arrange(desc(X))

# Substitute very small value for species with zero catch, just for pie charts
if (nrow(haul.pos) > 0) {
  haul.pos <- haul.pos %>% 
    replace(. == 0, 0.0000001) 
  
  cluster.pos <- cluster.pos %>% 
    replace(. == 0, 0.0000001) 
}

# Filter for empty trawls
haul.zero    <- filter(haul.pie, AllCPS == 0)

cluster.zero <- filter(cluster.pie, AllCPS == 0)

# Calculate pie radius based on latitude range
pie.radius <- as.numeric(abs(map.bounds$ymin - map.bounds$ymax)*pie.scale)

# Calculate pie radius of each pie, based on All CPS landings
if (scale.pies) {
  haul.pie$radius    <- pie.radius*haul.pie$bin
  cluster.pie$radius <- pie.radius*cluster.pie$bin
} else {
  haul.pie$radius    <- pie.radius
  cluster.pie$radius <- pie.radius
}

# Convert haul data for plotting
haul.catch <- haul.summ.wt %>% 
  project_df(to = crs.proj)

# sum total weight of sardine, anchovy, and mackerel
haul.Anchovy.kg    <- sum(haul.catch$Anchovy, na.rm = T)
haul.Sardine.kg    <- sum(haul.catch$Sardine, na.rm = T)
haul.PacMack.kg    <- sum(haul.catch$PacMack, na.rm = T)
haul.JackMack.kg   <- sum(haul.catch$JackMack, na.rm = T)
haul.PacHerring.kg <- sum(haul.catch$PacHerring, na.rm = T)
haul.CPS.kg        <- sum(haul.Anchovy.kg, haul.Sardine.kg, haul.PacMack.kg,
                          haul.JackMack.kg, haul.PacHerring.kg, na.rm = T)

# summarize trawl haul data
trawl.summ <- haul.catch %>% 
  left_join(select(haul, haul, Date = equilibriumTime, Latitude = startLatDecimal,
                   Longitude = startLongDecimal)) %>% 
  select(Haul = haul, Date, Latitude, Longitude, "N. Anchovy" = Anchovy, "P. Sardine" = Sardine, 
         "P. Mackerel" = PacMack, "J. Mackerel" = JackMack, "P. Herring" = PacHerring, 
         Jacksmelt, "All CPS" = AllCPS) %>% 
  mutate(Date = format(Date, "%m/%d/%Y %H:%M"))
```

```{r process-cufes}
# Read CUFES data
cufes.filename <- list.files(here("Data/CUFES"), pattern = "*.sqlite")
cufes.con      <- dbConnect(SQLite(), dbname = here("Data/CUFES", cufes.db.sqlite))
cufes.raw <- tbl(cufes.con, "cufessqlite") %>%
  collect() %>% 
  mutate(
    Start = case_when(
      cufes.date.format == "mdy" ~ mdy_hms(Start), #"06/01/2019-15:43:00"
      cufes.date.format == "ymd" ~ ymd_hms(Start)),#"1996-03-15 15:43:00 -08:00"
    Stop = case_when(
      cufes.date.format == "mdy" ~ mdy_hms(Stop), #"06/01/2019-15:43:00"
      cufes.date.format == "ymd" ~ ymd_hms(Stop)),#"1996-03-15 15:43:00 -08:00"
    Duration = as.numeric(difftime(Stop, Start, units = "mins")),
    Year = year(Start),
    AllEggs = SardineEggs + AnchovyEggs + JackMackerelEggs) %>% 
  rename(lat = StartLatitude, long = StartLongitude) %>% 
  project_df(to = crs.proj)

# Close connection
dbDisconnect(cufes.con)

# save raw cufes table to CSV
write.csv(cufes.raw, file = here("Output/cufes_raw.csv"), 
          quote = F, row.names = F)

# Process CUFES data
cufes <- cufes.raw %>% 
  # Convert cufes to long format for plotting
  select(
    SampleNumber, Year, Ship, Cruise, lat, long, X, Y, Duration, 
    SardineEggs, AnchovyEggs, JackMackerelEggs, SquidEggs, HakeEggs, OtherFishEggs,
    Comments) %>%
  gather(Species, Counts, -SampleNumber, -Year, -Ship, -Cruise, 
         -lat, -long, -X, -Y, -Duration, -Comments) %>% 
  mutate(Density = Counts/Duration/0.64,
         # Create bins for defining point size in NASC plots
         bin = cut(Density, cufes.breaks, include.lowest = T),
         bin.level = as.numeric(bin)) %>% 
  left_join(select(cufes.raw, SampleNumber, Start, Stop)) 

# Save processed cufes to CSV
write.csv(cufes, file = here("Output/cufes_proc.csv"), 
          quote = F, row.names = F)

# Prepare CUFES data for plotting ----------------------------------------------
# Select CUFES sample with zero density for plotting
cufes.neg <- filter(cufes.raw, AllEggs == 0) %>% 
  mutate(bin.level = 1) %>% 
  select(X, Y, SampleNumber)

# Identify bad CUFES samples
cufes.bad <- filter(cufes.raw, Duration <= 0)

save(cufes.bad, file = here("Output/cufes_bad.Rdata"))

# Remove bad samples from CUFES
cufes <- cufes %>% 
  filter(!SampleNumber %in% cufes.bad$SampleNumber)

# Write CUFES data from current survey to CSV
write.csv(cufes, file = here("Output/cufes_data.csv"), quote = F)

# Create bins for defining point size in NASC plots
cufes <- cufes %>% 
  mutate(bin = cut(Density, cufes.breaks, include.lowest = T),
         bin.level = as.numeric(bin))

# Project CUFES data from CPS
cufes.plot <- cufes %>% 
  filter(Density > 0, Species %in% cufes.plot.spp) %>%
  arrange(desc(Density))

# Project CUFES data from squid
cufes.plot.squid <- cufes %>% 
  filter(Density > 0, Species == "SquidEggs") %>% 
  arrange(desc(Density))

# Project CUFES data from other fish eggs (mostly P. mackerel)
cufes.plot.ofe <- cufes %>% 
  filter(Density > 0, Species == "OtherFishEggs") %>%
  filter(str_detect(Comments, c("Scomber japonicus","scomber japonicus",
                                "S. japonicus"))) %>% 
  arrange(desc(Density))
```  

```{r process-ctd-stations}
# Extract CTD and UCTD cast locations
events_sf <- st_as_sf(bridge.snap, coords = c("Lon","Lat"), crs = crs.geog)
  
ctd.sta         <- filter(events_sf, Button == ctd.button)
uctd.sta        <- filter(events_sf, Button == uctd.button)

# Extract bongo locations
bongo.sta       <- filter(events_sf, Button == bongo.button)

# Extract pairovet locations
pairovet.sta    <- filter(events_sf, Button == pairovet.button)

# Extract all CTD/UCTD stations
all.ctds        <- bind_rows(filter(bridge.snap, Button == ctd.button),
                             filter(bridge.snap, Button == uctd.button)) %>% 
  arrange(datetime) %>% 
  select(Date = datetime, Button, Latitude = Lat, Longitude = Lon) %>% 
  mutate(Button = case_when(
    Button == ctd.button ~ "CTD Cast",
    Button == uctd.button ~ "UCTD Cast", 
    TRUE ~ Button))
```  

```{r process-csv-cps}
if (process.csv) {
  if (file.exists(here("Output/processed_cps.Rdata"))) {
    # Load already processed CSV files
    load(here("Output/processed_cps.Rdata"))
  }
  
  # List local CSV files
  csv.files.cps <- dir_ls(here("Data/Backscatter", survey.vessel.primary), 
                          regex = nasc.pattern.cps[survey.vessel.primary])
  
  if (process.csv.all) {
    # Create final data frame
    nasc.cps <- data.frame()
  } else {
    # Load already processed files
    load(here("Output/nasc_cps.Rdata")) 
    # List only new CSV files
    csv.files.cps <- csv.files.cps[!csv.files.cps %in% processed.cps]
  }
  
  if (length(csv.files.cps) > 0) {
    # Configure progress bar
    pb <- winProgressBar(title = "CSV File Processing Progress - CPS", 
                         label = "0% done", min = 0, max = 100, initial = 0)
    
    # Process all .CSV files
    for (i in 1:length(csv.files.cps)) {
      # Process i-th file
      nasc.cps <- bind_rows(nasc.cps, extract_csv(csv.files.cps[i]))
      
      # Update the progress bar
      info <- sprintf("%d%% done", round((i / length(csv.files.cps)) * 100))
      setWinProgressBar(pb, round((i / length(csv.files.cps)) * 100), label = info)
    }
    close(pb)
    
    # Calculate summary interval
    nasc.cps <- nasc.cps %>%
      mutate(int = cut(Interval, seq(1, max(Interval) + nasc.summ.interval, nasc.summ.interval),
                       labels = F, include.lowest = T))
    
    # Save results
    save(nasc.cps, file = here("Output/nasc_cps.Rdata"))
    write.csv(nasc.cps, file = here("Output/nasc_cps.csv"), row.names = F, quote = F)
  }
} else {
  load(here("Output/nasc_cps.Rdata"))
}

# Get intervals with bad lat/long values
bad.nasc.cps <- filter(nasc.cps, lat == 999, long == 999)
write_csv(bad.nasc.cps, here("Output/nasc_bad_cps.csv"))

# Summarize nasc for reporting effort
nasc.summ <- nasc.cps %>% 
  group_by(transect) %>% 
  summarise(
    distance = length(Interval)*100/1852,
    lat = lat[which.min(long)],
    lon = long[which.min(long)])

# average NASC.70 data over new intervals or number of intervals in a 2 km radius
nasc.summ.cps <- nasc.cps %>%
  filter(lat != 999, long != 999) %>% 
  group_by(transect, int) %>%
  summarise(
    bins    = length(int),
    bin.mid = as.integer(round(bins / 2)),
    lat     = lat[1],
    long    = long[1],
    NASC    = mean(NASC.70)
  )

# Average cps.nasc over defined interval
# Summarize by filename, not transect, so that renamed (i.e., strip.tx.chars == T) transects get included.
nasc.cps.sf <- nasc.cps %>%
  filter(lat != 999, long != 999) %>%
  select(filename, transect, int, dist_m, datetime, lat, long, cps.nasc = NASC.70) %>% 
  group_by(filename, transect, int) %>% 
  summarise(
    lat   = lat[1],
    long  = long[1],
    NASC  = mean(cps.nasc),
    label = paste0('Transect: ', transect[1], "; ",
                   'Distance: ', round(min(dist_m)), "-", round(max(dist_m)), ' m'),
    popup = paste0('<b>Transect: </b>', transect[1], '<br/>',
                   '<b>Time: </b>', min(datetime), " - ", max(datetime), ' UTC<br/>',
                   '<b>Distance: </b>', round(min(dist_m)), "-", round(max(dist_m)), ' m<br/>',
                   '<b>NASC: </b>', round(mean(NASC)), ' m<sup>2</sup> nmi<sup>-2</sup>')) %>%
  # Create bins for defining point size in NASC plots
  mutate(bin       = cut(NASC, nasc.breaks, include.lowest = T),
         bin.level =  as.numeric(bin)) %>% 
  filter(!is.na(bin)) %>% 
  st_as_sf(coords = c("long","lat"), crs = crs.geog) 

nasc.plot.cps <- project_sf(nasc.cps.sf, crs.proj)

# Convert acoustic transects to sf
nasc.tx.sf <- st_as_sf(nasc.cps.sf, coords = c("long","lat"), crs = crs.geog) %>% 
  select(transect) %>% 
  group_by(transect) %>% 
  summarise(do_union = F) %>% 
  st_cast("LINESTRING") %>% 
  filter(!transect %in% tx.rm)

# create acoustic transect labels
nasc.tx.labels.cps <- nasc.cps %>%
  group_by(transect) %>%
  summarise(
    lat = lat[which.max(long)],
    long = max(long)
  )

# List already processed CSV files and save
processed.cps <- unique(nasc.cps$filename)
save(processed.cps, file =  here("Output/processed_cps.Rdata"))
```

```{r process-csv-krill}
if (process.csv) {
  if (file.exists(here("Output/processed_krill.Rdata"))) {
    # Load already processed CSV files
    load(here("Output/processed_krill.Rdata"))
  }
  
  # List local CSV files
  csv.files.krill <- dir_ls(here("Data/Backscatter", survey.vessel.primary), 
                          regex = nasc.pattern.krill[survey.vessel.primary])
  
  if (process.csv.all) {
    # Create final data frame
    nasc.krill <- data.frame()
  } else {
    # Load already processed files
    load(here("Output/nasc_krill.Rdata")) 
    # List only new CSV files
    csv.files.krill <- csv.files.krill[!csv.files.krill %in% processed.krill]
  }
  
  if (length(csv.files.krill) > 0) {
    # Configure progress bar
    pb <- winProgressBar(title = "CSV File Processing Progress - Krill", 
                         label = "0% done", min = 0, max = 100, initial = 0)
    
    # Process all .CSV files
    for (i in 1:length(csv.files.krill)) {
      # Process i-th file
      nasc.krill <- bind_rows(nasc.krill, extract_csv(csv.files.krill[i]))
      
      # Update the progress bar
      info <- sprintf("%d%% done", round((i / length(csv.files.krill)) * 100))
      setWinProgressBar(pb, round((i / length(csv.files.krill)) * 100), label = info)
    }
    close(pb)
    
    # Calculate summary interval
    nasc.krill <- nasc.krill %>%
      mutate(int = cut(Interval, seq(1, max(Interval) + nasc.summ.interval, nasc.summ.interval),
                       labels = F, include.lowest = T)) 
    
    # Save results
    save(nasc.krill, file = here("Output/nasc_krill.Rdata"))
    write.csv(nasc.krill, file = here("Output/nasc_krill.csv"), row.names = F, quote = F)
  }
} else {
  load(here("Output/nasc_krill.Rdata"))
}

# Get intervals with bad lat/long values
bad.nasc.krill <- filter(nasc.krill, lat == 999, long == 999)
write_csv(bad.nasc.krill, here("Output/nasc_bad_krill.csv"))

# Average NASC.350 data over new intervals or number of intervals in a 2 km radius
nasc.summ.krill <- nasc.krill %>%
  filter(lat != 999, long != 999) %>%
  group_by(transect, int) %>%
  summarise(
    bins    = length(int),
    bin.mid = as.integer(round(bins / 2)),
    lat     = lat[1],
    long    = long[1],
    NASC    = mean(NASC.350)
  )

# Average krill.nasc over defined interval
# Summarize by filename, not transect, so that renamed (i.e., strip.tx.chars == T) transects get included.
nasc.krill.sf <- nasc.krill %>%
  filter(lat != 999, long != 999) %>%
  select(filename, transect, int, dist_m, datetime, lat, long, krill.nasc = NASC.350) %>% 
  group_by(filename, transect, int) %>% 
  summarise(
    lat   = lat[1],
    long  = long[1],
    NASC  = mean(krill.nasc),
    label = paste0('Transect: ', transect[1], "; ",
                   'Distance: ', round(min(dist_m)), "-", round(max(dist_m)), ' m'),
    popup = paste0('<b>Transect: </b>', transect[1], '<br/>',
                   '<b>Time: </b>', min(datetime), " - ", max(datetime), ' UTC<br/>',
                   '<b>Distance: </b>', round(min(dist_m)), "-", round(max(dist_m)), ' m<br/>',
                   '<b>NASC: </b>', round(mean(NASC)), ' m<sup>2</sup> nmi<sup>-2</sup>')) %>%
  # Create bins for defining point size in NASC plots%>% 
  mutate(bin       = cut(NASC, nasc.breaks, include.lowest = T),
         bin.level =  as.numeric(bin))  %>% 
  filter(!is.na(bin)) %>% 
  st_as_sf(coords = c("long","lat"), crs = crs.geog) 

nasc.plot.krill <- project_sf(nasc.krill.sf, crs.proj)

# create acoustic transect labels
nasc.tx.labels.krill <- nasc.krill %>%
  group_by(transect) %>%
  summarise(
    lat = lat[which.max(long)],
    long = max(long)
  )

# List already processed CSV files and save
processed.krill <- unique(nasc.krill$filename)
save(processed.krill, file =  here("Output/processed_krill.Rdata"))

# Select plot levels for backscatter data
nasc.plot.all   <- rbind(nasc.cps.sf, nasc.krill.sf) %>% 
  filter(NASC >= 200)
```

```{r plot-maps,include=F}
if (save.figs) {
  # Map planned transects
  survey.plan <- base.map +    
    geom_sf(data = transects.sf, aes(colour = Type, linetype = Type), 
            show.legend = "line") +
    scale_colour_manual("Type", 
                        values = c(Adaptive = "red", Compulsory = "black",
                                   Offshore = "green", Nearshore = "magenta",
                                   transit = "orange")) +
    scale_linetype_manual("Type", 
                        values = c(Adaptive = "dashed", Compulsory = "solid",
                                   Offshore = "dashed", Nearshore = "solid",
                                   transit = "dashed")) +
    coord_sf(crs = crs.proj, # CA Albers Equal Area Projection
           xlim = c(map.bounds["xmin"], map.bounds["xmax"]), 
           ylim = c(map.bounds["ymin"], map.bounds["ymax"]))
 
  # save survey plan map
  ggsave(here("Figs/fig_survey_plan.png"), survey.plan,
         height = map.height, width = map.width)
  
  # Plot Survey Track, Acoustic Transects, and Trawl Locations #####
  survey.track.map <- base.map +
    # Plot transects data
    geom_sf(data = transects.sf, aes(linetype = Type), colour = "gray70",
            show.legend = "line") +
    # Plot ship track data
    geom_sf(data = nav.paths.sf, colour = "gray50", size = 0.5, alpha = 0.5) +
    # Plot acoustic transects
    geom_sf(data = nasc.tx.sf, size = 1) +
    # Plot trawl transects
    geom_point(data = haul.catch, aes(X, Y),
               shape = 21, colour = "black", fill = "white", size = 1.5) +
    scale_linetype_manual("Type", 
                          values = c(Adaptive = "dashed", Compulsory = "solid",
                                     Offshore = "dashed", Nearshore = "dotted",
                                     transit = "dashed")) +
    coord_sf(crs = crs.proj, # CA Albers Equal Area Projection
             xlim = c(map.bounds["xmin"], map.bounds["xmax"]), 
             ylim = c(map.bounds["ymin"], map.bounds["ymax"]))
  
  # save figure as PNG and PDF images
  ggsave(here("Figs/fig_vessel_track.png"), survey.track.map,
         height = map.height, width = map.width)
  
  # Plot Side Station Sampling Locations #####
  survey.station.map <- base.map +
    # Plot transects data
    geom_sf(data = transects.sf, aes(linetype = Type), colour = "gray70",
            show.legend = "line") +
    # plot ship track data
    geom_sf(data = nav.paths.sf, colour = "gray70", size = 0.5, alpha = 0.5) +
    # plot acoustic transects
    geom_sf(data = nasc.tx.sf, size = 1) +
    scale_linetype_manual("Type", 
                          values = c(Adaptive = "dashed", Compulsory = "solid",
                                     Offshore = "dashed", Nearshore = "dotted",
                                     transit = "dashed")) +
    # plot station ops
    # geom_sf(data = pairovet.sta, #aes(Lon, Lat),
    #            shape = 25, size = 2, fill = "green", colour = "black") +
    geom_sf(data = bongo.sta, #aes(Lon, Lat), 
               shape = 24, size = 1.5, fill = "orange", colour = "black") +
    geom_sf(data = ctd.sta, 
            shape = 21, size = 1, fill = "red", colour = "black") +
    geom_sf(data = uctd.sta, 
            shape = 21, size = 1, fill = "red", colour = "black") +
    coord_sf(crs = crs.proj, # CA Albers Equal Area Projection
             xlim = c(map.bounds["xmin"], map.bounds["xmax"]), 
             ylim = c(map.bounds["ymin"], map.bounds["ymax"]))
  
  ggsave(here("Figs/fig_station_samples.png"), survey.station.map,
         height = map.height, width = map.width)
  
  # Plot Sa, Trawl, and CUFES map #####
  source(here("Code/plot_sA_trawl_cufes.R"))
  
  # Create calibration site maps #####  
  # Put calibration location lat/lon in a df, for point plot
  cal.map.df <- data.frame(cal.lat.dd, cal.lon.dd)
  
  # get Google map from calibration location
  cal.map <- ggmap(get_googlemap(c(cal.lon.dd, cal.lat.dd), 
                           maptype = "terrain", zoom = 12))
  
  # get extent of calibration map for inset
  cal.map.extent <- cal.map$data
  
  # create inset map
  map.inset <- ggplot(ca) +
    geom_sf() +
    geom_path(data = cal.map.extent[c(1, 3, 4, 2, 1),], aes(lon, lat),
              size = 0.5, colour = 'red') + # plot extent of inset map
    theme(axis.text.x     = element_blank(), 
          axis.text.y     = element_blank(), 
          axis.ticks      = element_blank(),
          axis.title.x    = element_blank(),
          axis.title.y    = element_blank(),
          plot.background = element_blank())
  
  # create main map using ggmap
  map.main <- cal.map + 
    geom_point(data = cal.map.df, aes(x = cal.lon.dd,y = cal.lat.dd),
               shape = 23, size = 5, fill = "yellow", colour = "black") + 
    xlab("\nLongitude (W)") + ylab("Latitude (N)\n") + 
    theme_bw() + 
    theme(plot.background = element_blank(),
          axis.text.y = element_text(angle = 90, hjust = 0.5))
  
  # Create final color map
  cal.map.color <- ggdraw() +
    draw_plot(map.main,0,0,1,1) +
    draw_plot(map.inset,0.65,0.65,0.325,0.325)
  
  # Save map image
  ggsave(cal.map.color, filename = here("Figs/fig_cal_map.png"),
         height = 10, width = 10, units = "in")
} 
```  

\pagenumbering{gobble}

**Report on the `r survey.name.long` (`r survey.name`), `r survey.start` to `r survey.end` `r survey.year`, conducted aboard NOAA Ship _`r survey.vessel.long`_**  

Kevin L. Stierhoff, Juan P. Zwolinski, Josiah S. Renfree, Gabriel Johnson, Scott A. Mau, David W. Murfin, Thomas S. Sessions, and David A. Demer

Fisheries Resources Division  
Southwest Fisheries Science Center  
NOAA-National Marine Fisheries Service  
8901 La Jolla Shores Dr.  
La Jolla, CA 92037, USA  

\newpage
\pagenumbering{arabic}

# Introduction {#introduction}
The `r survey.name.long` (`r survey.name`) was conducted by the Fisheries Resources Division (FRD) of the Southwest Fisheries Science Center (SWFSC) aboard NOAA Ship _`r survey.vessel.long`_ (hereafter, _`r survey.vessel`_; **Fig. \@ref(fig:lasker-pic)**), `r survey.start` to `r survey.end` `r survey.year`. The Acoustic-Trawl Method (ATM) was used to assess coastal pelagic fish species (CPS) and krill within the CCE. Data were collected using multi-frequency echosounders, surface trawls, obliquely integrating net tows, a continuous underway fish-egg sampler (CUFES), and conductivity-temperature-depth probes (CTDs).  

The objectives for the survey were to: 1) acoustically map the distributions and estimate the abundances of CPS, i.e., Pacific Sardine _Sardinops sagax_, Northern Anchovy _Engraulis mordax_, Pacific Herring _Clupea pallasii_, Pacific Mackerel _Scomber japonicus_, and Jack Mackerel _Trachurus symmetricus_; and krill (euphausiid spp.); 2) characterize their biotic and abiotic environments, and investigate linkages; 3) gather information regarding their life histories; and 4) evaluate the use of unmanned surface vehicles (USVs) and industry vessels to augment the sampling from NOAA ships.  

The survey domain encompassed the anticipated distributions of the northern sub-population (stock) of Pacific Sardine and the central and northern stocks of Northern Anchovy off the west coasts of the U.S. and Canada from approximately `r survey.landmark.s`, to `r survey.landmark.n`, but also encompassed a large portion of the anticipated distributions of the southern stock of Pacific Sardine, Pacific Mackerel, Jack Mackerel, and Pacific Herring. The survey domain was defined by the modeled distribution of Pacific Sardine potential habitat [@Zwolinski2011], and information recently gathered from other research projects (e.g., California Cooperative Oceanic Fisheries Investigations [CalCOFI] samples) or the fishing industry (e.g., bycatch of Pacific Sardine).  

This report provides an overview of the survey objectives and a summary of the survey equipment, acoustic-system calibration, sampling and analysis methods, and preliminary results. **This report does not include estimates of the distributions and biomasses of CPS or krill. Advantages and disadvantages to the combination of NOAA ships, USVs, and industry vessels are discussed from the SWFSC perspective.**  

(ref:lasker-pic) NOAA Ship _`r survey.vessel`_.

```{r lasker-pic, fig.cap='(ref:lasker-pic)', out.height='3in', fig.pos='H'}
include_graphics(here("Images/img_reuben_lasker.jpg"))
```

\newpage  

## Scientific Personnel {#introduction-personnel}
As elaborated below, the collection and analysis of the survey data was conducted by the Fisheries Resources Division at the SWFSC. Superscripts denote the roles of the other cruise participants: 1-Chief Scientist and 2-Volunteer.  

**Project Leads:**

* D. Demer^1^

**Acoustic Data Collection and Processing:**

* Leg I:   D. Demer^1^ and J. Renfree
* Leg II:  G. Johnson and K. Stierhoff^1^
* Leg III: D. Murfin and T. Sessions
* Leg IV:  S. Mau and J. Zwolinski^1^

**Trawl Sampling:**

* Leg I:   A. Friere, D. Griffith, M. Human, D. Jones^2^, B. Overcash, D. Pinkard-Meier^2^, R. Reed^2^, L. Vasquez de Mercado
* Leg II:  S. Bal Raj^2^, E. Gardner, H. Hicks^2^, B. Overcash, L. Vasquez de Mercado, W. Watson, A. Whalen^2^
* Leg III: E. Gardner, N. Hunter^2^, D. Hwang^2^, H. Manjebrayakath^2^, A. Thompson, L. Vasquez de Mercado, E. Weber^1,2^
* Leg IV:  M. Craig, A. Hays, A. Friere, D. Griffith, T. Howard^2^, K. Runge^2^, **D. Sweetnam**

**Echosounder Calibration:**

* D. Murfin, J. Renfree, and T. Sessions

# Methods {#methods}
## Survey region and design {#methods-survey-design}
During spring, Pacific Sardine typically aggregate offshore of central and southern California to spawn [@Demer2012, and reference therein]. During summer, if the stock is large enough, adults will migrate north, compress along the coast, and feed in the upwelled regions (**Fig. \@ref(fig:sardine-distribution)**).  

During `r tolower(survey.season)` `r survey.year`, the west coast of the United States was surveyed using _`r survey.vessel`_. Compulsory transects were nearly perpendicular to the coast with separations of 10 to 20 nmi. The survey began off `r survey.landmark.n`, and progressed southwards toward `r survey.landmark.s`.  

The planned transects (**Fig. \@ref(fig:survey-plan)**) spanned the latitudinal extent of the potential habitat of the northern stock of Pacific Sardine^[http://swfscdata.nmfs.noaa.gov/AST/sardineHabitat/habitat.asp] at the time of the survey (**Fig. \@ref(fig:sardine-habitat)**). Transect positions, lengths, and spaces were adaptively adjusted during the survey according to the observed distribution of putative CPS backscatter in the echosounders, CPS eggs in CUFES, or CPS landed in trawls.

(ref:sardine-distribution) Conceptual spring (shaded region) and summer (hashed region) distributions of potential habitat for the northern stock of Pacific Sardine along the west coasts of Mexico, the United States, and Canada. The dashed and dotted lines represent, respectively, the approximate summer and the spring position of the 0.2 mg m^–3^ isoline of chlorophyll-a concentration. This isoline appears to oscillate in synchrony with the transition zone chlorophyll front [TZCF, @Polovina2001] and the offshore limit of the Pacific Sardine potential habitat [@Zwolinski2014]. Mackerels are found within and on the edge of the same oceanographic  habitat [e.g., @Demer2012; @Zwolinski2012]. The TZCF may delineate the offshore and southern limit of both Pacific Sardine and Pacific Mackerel distributions, and juveniles may have nursery areas in the Southern California Bight, downstream of upwelling regions.

```{r sardine-distribution, fig.cap='(ref:sardine-distribution)',out.width='4in',fig.pos='H'}
include_graphics(here("Images/img_survey_region.png"))  
```  

\newpage  

(ref:survey-plan) Planned pre-survey transit (orange line); compulsory (black lines) and adaptive (red lines) acoustic transect lines; offshore extensions to acoustic transects for marine mammal and CPS sampling (green lines); and Saildrone transects for nearshore CPS (magenta lines). Isobaths (light gray lines) are placed at 50, 200, 500, and 2,000 m (or approximately ~1,000 fathoms). **Remove unsampled nearshore/offshore transects**

```{r survey-plan, fig.cap='(ref:survey-plan)', out.height='8in',fig.align='center',fig.pos='H'}
include_graphics(here("Figs/fig_survey_plan.png"))
```  

\newpage  

(ref:sardine-habitat) Distribution of potential habitat for the northern stock of Pacific Sardine (a) before, (b,c), during, and (d) at the end of the `r tolower(survey.season)` `r survey.year` survey. Areas in white correspond to no available data, e.g., cloud coverage preventing satellite-sensed observations.

```{r sardine-habitat,fig.cap='(ref:sardine-habitat)',out.width = '7in',fig.align='center',fig.pos='H'}
include_graphics(here("Figs/fig_habitat_map.png"))
```  

\newpage

## Acoustic sampling {#methods-acoustic-sampling}
### Echosounders {#methods-echosounders}
On _`r survey.vessel`_, multi-frequency EK60 General Purpose Transceivers (18 and 38 kHz GPTs; Simrad) and EK80 Wideband Transceivers (70, 120, 200, and 333 kHz WBTs; Simrad) were configured with split-beam transducers (`r echo.models`, respectively; Simrad). The transducers were mounted on the bottom of a retractable keel or "centerboard" (**Fig. \@ref(fig:cb-config)**). The keel was retracted (transducers ~`r cb.retracted`-m depth) during calibration, and extended to the intermediate position (transducers ~`r cb.intermediate`-m depth) during the survey. Exceptions were made during shallow water operations, when the keel was retracted; or during times of heavy weather, when the keel was extended (transducers ~`r cb.extended`-m depth) to provide extra stability and reduce the effect of weather-generated noise (**Appendix \@ref(appendix-cb-pos)**).  

(ref:cb-config) Transducer locations on the bottom of the centerboard aboard _`r survey.vessel`_.

```{r cb-config,fig.cap='(ref:cb-config)',out.width = '6.5in',fig.align='center',fig.pos='H'}
if (survey.vessel.primary == "RL") {
  include_graphics(here("Images/img_centerboard_config_RL.png"))
} else {
  include_graphics(here("Images/img_centerboard_config_SH.png"))
}
``` 

### Calibration {#methods-echosounder-calibration}
Prior to calibration, the integrity of each transducer was verified through impedance measurements of each transducer in water and air using an LCR meter (Agilent E4980A) and custom Matlab software. For each transducer, impedance magnitude ($|Z|$, $\Omega$), phase ($\theta$, $^\circ$), conductance ($G$, $S$), susceptance ($B$, $S$), resistance ($R$, $\Omega$), and reactance ($X$, $\Omega$) were measured at the operational frequencies with the transducer quadrants connected in parallel (**Appendix \@ref(appendix-impedance-plots)**). The echosounders were calibrated using the standard sphere technique [@Foote1987;@Demer2015]. The reference target was a `r cal.sphere` (`r cal.sphere.name`). The GPTs were configured, via the ER60 software, using the calibration results (see **Section \@ref(results-echosounder-calibration)**).  

### Data collection {#methods-acoustic-data-collection}
Computer clocks were synchronized with the GPS clock (GMT) using synchronization software (NetTime^[http://timesynctool.com]). Echosounder pulses were transmitted simultaneously at all frequencies, at variable intervals, as controlled by the ER60 Adaptive Logger [EAL, @Renfree2016]. The EAL optimizes the pulse interval, based on the seabed depth, while minimizing aliased seabed echoes. Acoustic sampling for CPS-density estimation along the pre-determined transects was limited to daylight hours (approximately between sunrise and sunset).

Measurements of volume backscattering strength ($S_v$; dB re 1 m^2^ m^-3^) and target strength ($TS$, dB re 1 m^2^), indexed by time and geographic positions provided by GPS receivers, were logged to 60 m beyond the detected seabed range or to a maximum of `r raw.log.range` m and stored in Simrad .raw format with a `r raw.size`-MB maximum file size. For each acoustic instrument, the prefix for the file names is a concatenation of the survey name (e.g.,  `r survey.name`), the operational mode (CW for continuous wave or chirp mode; FM for frequency modulation or broadband mode), and the logging commencement date and time from the GPT-control software. For example, file generated by the Simrad EK80 software (`r ek80.version`) is named `1907RL-CW-D20190723-T125901.raw`.   

To minimize acoustic interference, transmit pulses from the ME70, the MS70, the SX90, and the acoustic Doppler current profiler (Ocean Surveyor Model OS75, Teledyne RD Instruments) were triggered using the K-Sync synchronization system (Simrad). All other instruments that produce sound within the echosounder bandwidths were secured during daytime survey operations. Exceptions were made during stations (e.g., plankton sampling and fish trawling) or in shallow water when the vessel's command occasionally operated the bridge's 50- and 200-kHz echosounders (Furuno), the Doppler velocity log (Sperry Marine Model SRD-500A), or both.  

### Data processing {#methods-acoustic-data-processing}
Echoes from schooling CPS were identified using a semi-automated data processing algorithm implemented using Echoview software (`r ev.version`). The filters and thresholds were based on a subsample of echoes from randomly selected CPS schools. The aim of the filter criteria is to retain at least 95% of the noise-free backscatter from CPS while rejecting at least 95% of the non-CPS backscatter (**Fig. \@ref(fig:ev-filtering-example)**). The filter includes the following steps: **Review these processing steps. In particular, variable logging depth, speed filter, noise monitoring, passive ping removal, etc.**

+ Estimate and subtract background noise using the built-in Echoview background noise removal function [@DeRobertis2007,  **Fig. \@ref(fig:ev-filtering-example)b,e**];
+ Average the noise-free $S_v$ echograms using non-overlapping 11-sample by 3-ping bins;
+ Expand the averaged, noise-reduced _S~v~_ echograms with a 7 pixel x 7 pixel dilation;
+ For each pixel, compute: $S_{v,\mathrm{200kHz}}-S_{v,\mathrm{38kHz}}$, $S_{v,\mathrm{120kHz}}-S_{v,\mathrm{38kHz}}$, and $S_{v,\mathrm{70kHz}}-S_{v,\mathrm{38kHz}}$;
+ Create a Boolean echogram for $S_v$ differences in the CPS range: $-13.85 < S_{v,\mathrm{70kHz}}-S_{v,\mathrm{38kHz}} < 9.89 \wedge -135.5 < S_{v,\mathrm{120kHz}}-S_{v,\mathrm{38kHz}} < 9.37 \wedge -13.51 < S_{v,\mathrm{200kHz}}-S_{v,\mathrm{38kHz}} < 12.53$;
+ Compute the standard deviation (SD) of $S_{v,\mathrm{120kHz}}$ and $S_{v,\mathrm{200kHz}}$ using non-overlapping 11-sample by 3-ping bins;
+ Expand the SD($S_{v,\mathrm{120kHz}}$) and SD($S_{v,\mathrm{200kHz}}$) echograms with a 7 pixel x 7 pixel dilation;
+ Create a Boolean echogram based on the SDs in the CPS range: SD($S_{v,\mathrm{200kHz}}$) > -65 dB $\wedge$ SD($S_{v,\mathrm{120kHz}}$) > -65 dB. Diffuse backscattering layers [@Zwolinski2010] have low standard deviations whereas fish schools have high standard deviations [@Demer2009];
+ Intersect the two Boolean echograms. The resulting echogram has samples with "TRUE" for candidate CPS schools and "FALSE" elsewhere;
+ Mask the noise-reduced echograms using the CPS Boolean echogram (**Fig. \@ref(fig:ev-filtering-example)c,f**);
+ Create an integration-start line at a range of `r int.start` m from the transducer (~10 m depth);
+ Create an integration-stop line `r adz.range` m above the estimated seabed [@Demer2009], or to the maximum logging range (e.g., `r int.stop` m), whichever is shallowest;
+ Set the minimum $S_v$ threshold to -60 dB (corresponding to a density of approximately three fish per 100 m^3^ in the case of 20-cm-long Pacific Sardine);
+ Integrate the volume backscattering coefficients ($s_V$, m^2^ m^-3^) attributed to CPS over 5-m depths and averaged over 100-m distances;
+ Output the resulting nautical area scattering coefficients ($s_A$; m^2^ nmi^-2^) and associated information from each transect and frequency to comma-delimited text (.csv) files.  

When necessary, the start and stop integration lines were manually edited to exclude reverberation due to bubbles, to include the entirety of shallow CPS aggregations, or to exclude seabed echoes.
    
(ref:ev-filtering-example) Echogram depicting CPS schools (red) and plankton aggregations (blue and green) at 38 kHz (top row) and 120 kHz (bottom row). Example data processing steps include the original echogram (left column), after noise subtraction and bin-averaging (middle column), and filtering to retain only putative CPS echoes (right column).

```{r ev-filtering-example,fig.cap='(ref:ev-filtering-example)',out.width = '5.5in',fig.pos='H'}
include_graphics(here("Images/img_echoview_filtering_example-labeled.png"))
```  

(ref:sv-diff) $S_v$-differences (minimum, maximum; dB) for putative CPS.  

```{r sv-diff,fig.cap='(ref:sv-diff)', results='asis',eval=F}
# create table for Sv difference values
sv.diff <- read.delim(here("Data/sv_diff_table.txt"))
names(sv.diff) <- c("$S_\\mathrm{v70kHz}-S_\\mathrm{v38kHz}$", 
                    "$S_\\mathrm{v120kHz}-S_\\mathrm{v38kHz}$", 
                    "$S_\\mathrm{v200kHz}-S_\\mathrm{v38kHz}$")

if (doc.type == "docx") {
  # create kable object (for Word)
  pander(sv.diff)
} else {
  # print LaTeX table for HTML or PDF
  kable(sv.diff, align = rep("c", ncol(sv.diff)), escape = F,
        caption = '(ref:sv-diff)') %>% 
    kable_styling(position = "center", latex_options = c("hold_position"))
}
```  

## Trawl sampling {#methods-trawl-sampling}
During the day, CPS form schools in the upper mixed layer [to 70-m depth in the spring; @Kim2005], and shallower in summer. After sunset, CPS schools tend to ascend and disperse; at that time, with reduced visibility and no schooling behavior, they are less able to avoid a net [@Mais1974]. Therefore, trawl sampling for identifying the species composition and length distributions of acoustic targets was performed at night.  

The net, a Nordic 264 rope trawl (NET Systems; Bainbridge Island, WA), has a rectangular opening in the fishing portion of the net with an area of approximately 300 m^2^ (~15-m tall x 20-m wide), variable-sized mesh in the throat, an 8-mm square-mesh cod end liner (to retain a large range of animal sizes), and a "marine mammal excluder device" to prevent the capture of large animals, such as dolphins, turtles, or sharks [@Dotson2010]. The trawl doors are foam-filled and the trawl headrope is lined with floats so the trawl tows at the surface.  

Up to three nighttime (i.e., 60 min after sunset to 30 min before sunrise) surface trawls, typically spaced 5-10 nmi-apart, were conducted in areas where putative echoes from CPS schools were observed earlier that day. Each evening, trawl locations were selected by an acoustician who monitored CPS echoes and a member of the trawl group who measured the densities of CPS eggs in CUFES. The locations were provided to the watch Officers who charted the proposed trawl sites. Trawl locations were selected using the following criteria, in descending priority: CPS schools in echograms that day, CPS eggs in CUFES that day, and the trawl locations and catches during the previous night. If no CPS echoes or CPS eggs were observed along the transect(s) that day, the trawls were alternatively placed nearshore one night and offshore the next night, with consideration given to the seabed depth and the modeled distribution of CPS habitat.  

Trawls were towed at ~4 kn for 45 min. The total catch from each trawl was weighed and sorted by species or groups. From the catches with CPS, up to 50 fish were selected randomly for each of the target species. Those were weighed (g) and measured to either their standard length ($L_S$; mm) for Pacific Sardine and Northern Anchovy, or fork length ($L_F$; mm) for Jack Mackerel, Pacific Mackerel, and Pacific Herring. In addition, sex and maturity were recorded for all species; ovaries were preserved for Pacific Mackerel if active, hydrated, or both. Fin clips were removed from Pacific Sardine and Northern Anchovy and preserved in ethanol for genetic analysis. Otoliths were removed from all 50 Pacific Sardine in the subsample; for other CPS species, 25 otoliths were removed "as equally as possible" from the range of sizes present. Regional species composition was estimated from the nearest trawl cluster, i.e., the combined catches of up to three trawls per night, separated by **less than 12 h**. **Check the collection of genetic samples, number of otolith samples, ovary samples.**  

## Ichthyoplankton and oceanographic sampling {#methods-other-sampling}
### Egg and larva sampling {#methods-egg-sampling}
During the day, fish eggs were collected using CUFES [@Checkley1997], which collects water and plankton at a rate of ~640 l min^-1^ from an intake on the hull of the ship at ~3-m depth. The particles in the sampled water were sieved by a 505 $\mu$m mesh. Pacific Sardine, Northern Anchovy, Jack Mackerel, and Pacific Hake _Merluccius productus_ eggs were identified to species, counted, and logged. Eggs from other species (e.g., flatfishes) were also counted and logged as "other fish eggs". Typically, the duration of each CUFES sample was 30 min, corresponding to a distance of 5 nmi at a speed of 10 kn. Because the duration of the initial stages of the egg phase is short for most fish species, the egg distributions inferred from CUFES indicate the nearby presence of actively spawning fish.  

A CalCOFI bongo oblique net [or bongo; a paired, bridleless, 71-cm diameter net with 505-$\mu$m mesh; @Smith1977] was used to sample ichthyoplankton and krill **opportunistically** soon after sunset. Where there was adequate depth, 300 m of wire was deployed at a rate of 50 m min^-1^ and then retrieved at 20 m min^-1^, at a nominal wire angle of 45$^\circ$. Bongo samples were stored in 5% buffered formalin.  

### Conductivity and temperature versus depth (CTD) sampling {#methods-ctd-sampling}
Day and night, conductivity and temperature versus depth to `r ctd.depth` m were measured with calibrated sensors on a CTD rosette or underway probe (UCTD) cast from the vessel. These data were used to estimate the time-averaged sound speed [@Demer2004c], for estimating ranges to the sound scatterers, and frequency-specific sound absorption coefficients, for compensating signal attenuation of the sound pulse between the transducer and scatterers [@Simmonds2005]. These data also provided indication of the depth of the upper-mixed layer, where most epipelagic CPS reside during the day, which is later used to determine the integration depth during acoustic data processing.  

\newpage  

# Results {#results}
## EK80 echosounder calibration {#results-echosounder-calibration}
The EK80s were calibrated on `r cal.datetime` while the vessel was at anchor near `r cal.loc` (`r cal.lat.dd` $^\circ$N, `r cal.lon.dd` $^\circ$W). Measurements of sea-surface temperature ($t_w$ = `r cal.temp` $^\circ$C) and salinity ($s_w$ = `r cal.sal` psu) to a depth of 10 m were measured using a handheld probe (Pro2030, YSI) and input to the WBT-control software (EK80 `r ek80.version`, Simrad), which derived estimates of sound speed ($c_w$ = `r cal.c` m s^-1^) and absorption coefficients (see **Table \@ref(tab:cal-results)**). Varying with tide, the seabed was approximately `r cal.min.z` to `r cal.max.z` m beneath the transducers. The calibration sphere was positioned nominally 5-8 m below the transducers.  

WBT information, configuration settings, and beam model results following calibration are presented in **Table \@ref(tab:cal-results)**. Measurements of uncompensated sphere target strength ($TS_u$, dB re 1 m^2^) are plotted in **Fig. \@ref(fig:tsu-plot)** and beam-compensated sphere target strength ($TS_{rel}$, dB re 1 m^2^), relative to the theoretical target strength, are plotted in **Fig. \@ref(fig:tsc-plot)**. A time-series of calibration results for _`r survey.vessel`_, including on-axis gain ($G_0$), $S_a$ correction ($S_\mathrm{a}\mathrm{corr}$), beamwidths ($\alpha_{-3dB}$ and $\beta_{-3dB}$), offset angles ($\alpha_0$ and $\beta_0$), and RMS, are plotted in **Fig. \@ref(fig:cal-ts-plot)**.  

**At the time of the calibration, one of the 18-kHz quadrants appeared faulty, but we did attempt to collect on-axis sphere data which could be used to derive gain and Sa_corr. However, that quadrant subsequently appeared to be OK, which would make that gain and Sa_corr inapplicable. So it seems that we could either leave the 18-kHz results blank, or perhaps include the gain and Sa_corr but with an * and note describing why it might be faulty.**

**It could be useful to state that the transducer appeared to have been dysfunctional during calibration, with the center-board retracted, but may have been functional during the survey, with the center-board extended. If the latter case is supported by the impedance measurements made during the survey, then presenting the last known good calibration for that transducer could be useful to someone wanting to use those data (with caveats).**  

(ref:cal-results) Simrad EK80 wideband transceiver (WBT) information, pre-calibration settings (above horizontal line), and beam model results following calibration (below horizontal line). Prior to the survey, on-axis gain ($G_0$), beam angles and angle offsets, and $S_a$ Correction ($S_\mathrm{a}\mathrm{corr}$) values from calibration results were entered into the WBT-control software (Simrad EK80). **Update calibration results!**

```{r cal-results,results='asis'}
if (doc.type == "docx") {
  # create kable object (for Word)
  pander(all.output)
} else {
  # print LaTeX table for HTML or PDF
  kable(all.output, format = knitr.format, align = c("l","l",rep("c",ncol(all.output) - 2)),
        booktabs = T, escape = F,
        caption = '(ref:cal-results)') %>% 
    kable_styling(position = "center", latex_options = c("scale_down","hold_position")) %>% 
    row_spec(18, hline_after = T) %>% 
    add_header_above(c(" " = 2, "Frequency (kHz)" = ncol(all.output) - 2))
}
```  

(ref:cal-map) Map of the calibration location (yellow diamond) near `r cal.loc`. The red box in the inset indicates the location and extent of the main map.

```{r cal-map,fig.cap='(ref:cal-map)',out.width = '5in',fig.align='center',fig.pos='H',eval=F}
include_graphics(here("Figs/fig_cal_map.png"))
``` 

\newpage  

(ref:tsu-plot) Uncompensated sphere target strength ($TS_u$, dB re 1 m^2^) measurements of a `r cal.sphere`, at `r echo.freqs` kHz. Crosses indicate measurements marked as outliers after viewing the beam model results.

```{r tsu-plot,fig.cap='(ref:tsu-plot)',out.width='5in',fig.align='center',fig.pos='H'}
include_graphics(here("Figs/fig_cal_TSu_scatter.png"))
```  

(ref:tsc-plot) Relative beam-compensated sphere target strength ($TS_{rel}$, dB re 1 m^2^) measurements of a `r cal.sphere`, at `r echo.freqs` kHz. $TS_{rel}$ is calculated as the difference between the beam-compensated target strength ($TS_c$) and the theoretical target strength ($TS_{theory}$, see **Table \@ref(tab:cal-results)**). Crosses indicate measurements marked as outliers after viewing the beam model results.

```{r tsc-plot,fig.cap='(ref:tsc-plot)',out.width='5in',fig.align='center',fig.pos='H'}
include_graphics(here("Figs/fig_cal_TSrel_scatter.png"))
```  

\newpage

(ref:cal-ts-plot) Time series of beam model results of a) on-axis gain ($G_0$, dB); b) $S_a$ correction ($S_a corr$, dB re 1); c) alongship ($\alpha_\mathrm{-3dB}$, blue) and athwartship ($\beta_\mathrm{-3dB}$, red) beamwidths (deg); d) alongship ($\alpha_\mathrm{0}$, blue) and athwartship ($\beta_\mathrm{0}$, red) offset angles (deg); and e) RMS (dB) for `r echo.freqs` kHz. Unfilled circles indicate results from the current survey. **Update calibration time series!**

```{r cal-ts-plot,fig.cap='(ref:cal-ts-plot)',out.width='6.5in',fig.align='center',fig.pos='H'}
include_graphics(here("Figs/fig_cal_time_series_combo.png"))
``` 

\newpage  

## Data collection {#results-data-collection}
### Acoustic and trawl sampling {#results-acoustic-trawl-sampling}
The survey spanned an area from approximately `r survey.landmark.n`, to `r survey.landmark.s` (**Fig. \@ref(fig:cruise-track)**), with `r nrow(nasc.summ)` east-west transects totaling `r sprintf("%.0f",sum(nasc.summ$distance))` nmi, and `r nrow(trawl.summ)` Nordic trawls.  

**Leg I**  
On 13 June, _`r survey.vessel`_ departed from the Exploratorium (Pier 15) in San Francisco, CA at ~1800 (all times GMT) and began the transit to northern Vancouver Island. Throughout the transit, sampling was conducted during the day with CUFES, EK60s, EK80s, ME70, MS70 and SX90. On 17 June, _`r survey.vessel`_ arrived at the first offshore station off Cape Scott, British Columbia at ~1230 to begin acoustic sampling along Transect 129. On 01 July, acoustic sampling ceased after the completion of Transect 84 off Newport, OR. On 2 July, _`r survey.vessel`_ arrived at the Marine Operations-Pacific (MOC-P) Pier in Newport, OR at ~2100 to complete Leg I.  

During Leg I, _`r survey.vessel`_ coordinated sampling with F/V _Lisa Marie_, which conducted acoustic and purse seine sampling in shallow, nearshore (within ~ 5 nmi) waters between Cape Flattery, WA and **XXXX**, OR. On 21 June, Greg Shaughnessy from **[affiliation]** embarked _`r survey.vessel`_ to observe survey operations. At the same time, Josiah Renfree boarded _Lisa Marie_ to remedy minor issues with her echosounder before rejoining _`r survey.vessel`_. On **XX** June, Mr. Shaughnessy disembarked _`r survey.vessel`_ for _Lisa Marie_ and returned to port in Westport, WA.

**Leg II**  
On 8 July, _`r survey.vessel`_ departed from MOC-P Pier in Newport, OR at ~0000. Acoustic sampling resumed at ~0145 on 8 July along Transect 083 south of Newport, OR. On 24 July, acoustic sampling ceased after the completion of Transect **XX** off **[LANDMARK]**. On 25 July, _`r survey.vessel`_ arrived at the Pier 30/32 in San Francisco, CA at **~[HHMM]** to complete Leg II.  

During Leg II, _`r survey.vessel`_ coordinated sampling 


**Leg III**  

**UPDATE (Murfin/Sessions).**

**Leg IV**

**UPDATE (Mau/Zwolinski).** 

**Unmanned Surface Vehicle Sampling**


### Ichthyoplankton and oceanographic sampling {#results-other-sampling}
A total of `r nrow(ctd.sta)` CTD casts and `r nrow(bongo.sta)` bongo tows were conducted throughout the survey. In addition, `r nrow(uctd.sta)` UCTD casts were conducted and `r nrow(cufes.raw)` CUFES samples were collected underway. The locations of CTD and UCTD stations are shown in **Fig. \@ref(fig:station-sampling)** and **Appendix \@ref(appendix-ctd-sampling)**.  

\newpage

(ref:cruise-track) Cruise track of _`r survey.vessel`_ (gray line), east-west acoustic transects (black lines), and locations of surface trawls (white points).

```{r cruise-track,fig.cap='(ref:cruise-track)',out.height='8.5in',fig.align='center',fig.pos='H'}
include_graphics(here("Figs/fig_vessel_track.png"))
```  

\newpage  

(ref:station-sampling) Locations of CTD and UCTD casts (red circles) and bongo net samples (orange triangles) relative to the acoustic transects (black lines) and vessel track (light gray line).

```{r station-sampling,fig.cap='(ref:station-sampling)',out.height='8.5in',fig.align='center',fig.pos='H'}
include_graphics(here("Figs/fig_station_samples.png"))
```  

\newpage  

## Distribution of CPS {#results-cps-distribution}

**[Update]**

Acoustic backscatter ascribed to CPS (**Fig. \@ref(fig:nasc-cufes-trawl)a**) was observed throughout the survey area, but was most prevalent off Cape Flattery, between the Columbia River and Cape Blanco, inshore between Bodega Bay and Morro Bay, CA, and throughout the Southern California Bight.  

Jack Mackerel eggs were the most abundant of any CPS species (**Fig. \@ref(fig:nasc-cufes-trawl)b**) and were present in the CUFES throughout most of the survey area, found predominately in the offshore portion of transects between Cape Scott and approximately Newport. Northern Anchovy eggs were present in the CUFES samples nearshore off the Columbia River; nearshore between Bodega Bay and Morro Bay; and to a lesser extent in the Santa Barbara Basin south of Pt. Conception. Pacific Sardine eggs, overall small in density, were observed in the CUFES offshore of the Columbia River (obscured by dense Jack Mackerel eggs in the same area); between Cape Blanco and San Francisco; south of Pt. Conception; and near San Diego.  

Jack Mackerel comprised the greatest proportion of catch in trawl samples (**Fig. \@ref(fig:nasc-cufes-trawl)c**) between Cape Flattery and San Francisco, and offshore in the Southern CA Bight. Pacific Herring comprised the greatest proportion of catch in trawl samples in Canadian waters, and to a lesser extent in nearshore waters off Newport. Anchovy were predominantly collected in trawls conducted between Westport, WA, and Newport; and between San Francisco and San Diego. Sardine were collected in trawls conducted between the Columbia River and Newport; between Pt. Conception and Long Beach, CA; and to a lesser extent offshore between Crescent City, CA, and Cape Mendocino. Overall, the `r nrow(trawl.summ)` trawls captured a combined `r sprintf("%.0f",haul.CPS.kg)` kg of CPS (`r sprintf("%.0f",haul.Sardine.kg)` kg Pacific Sardine, `r sprintf("%.0f",haul.Anchovy.kg)` kg Northern Anchovy, `r sprintf("%.0f",haul.JackMack.kg)` kg Jack Mackerel, `r sprintf("%.0f",haul.PacMack.kg)` kg Pacific Mackerel, and `r sprintf("%.0f",haul.PacHerring.kg)` kg Pacific Herring; **Appendix \@ref(appendix-trawl-sampling)**).  

# Discussion  

**[Update]**  

\newpage  
\blandscape  

(ref:nasc-cufes-trawl) Survey transects overlaid with (a) the distribution of 38-kHz integrated backscattering coefficients ($s_A$, m^2^ nmi^-2^;  averaged over 2000-m distance intervals and from `r int.start`- to `r cps.depth`-m deep) ascribed to CPS; (b) Northern Anchovy-, Jack Mackerel-, and Pacific Sardine-egg densities (eggs m^-3^) from the CUFES; and (c) proportions, by weight, of CPS species in trawl clusters (black points indicate trawls with no CPS).

```{r nasc-cufes-trawl,fig.cap='(ref:nasc-cufes-trawl)',out.width='7in',fig.pos='H'}
include_graphics(here("Figs/fig_nasc_cufes_trawl.png"))
``` 

\elandscape
\newpage

# Disposition of Data {#data-disposition}
```{r calc-raw-size, eval=FALSE}
if (calc.raw.size) {
  # calculate sizes of ER60, EK80, ME70, MS70, and SX90 .RAW files
  ek60.file.size <- sum(dir_info(file.path(survey.dir,'DATA/EK60/RAW'), 
                                 regexp = '.raw$', recursive = T)$size)
  ek80.file.size <- sum(dir_info(file.path(survey.dir,'DATA/EK80/RAW'), 
                                 regexp = '.raw$', recursive = T)$size)
  me70.file.size <- sum(dir_info(file.path(survey.dir,'DATA/ME70/RAW'), 
                                 regexp = '.raw$', recursive = T)$size)
  ms70.file.size <- sum(dir_info(file.path(survey.dir,'DATA/MS70/RAW'), 
                                 regexp = '.raw$', recursive = T)$size)
  sx90.file.size <- sum(dir_info(file.path(survey.dir,'DATA/SX90/RAW'), 
                                 regexp = '.raw$', recursive = T)$size)

  save(ek60.file.size, ek80.file.size, me70.file.size, ms70.file.size, sx90.file.size,
       file = here("Output/raw_file_info.Rdata"))
} else {
  load(here("Output/raw_file_info.Rdata"))
}
```  

Approximately `as.character(ek60.file.size)` of raw EK60 data, `as.character(ek80.file.size)` of raw EK80 data, 887G of raw ME70 data, 2.69T of raw MS70 data, and `as.character(sx90.file.size)` of raw SX90 data are archived on the SWFSC data server. For more information, contact: David Demer (Southwest Fisheries Science Center, 8901 La Jolla Shores Drive, La Jolla, California, 92037, U.S.A.; phone: 858-546-5603; email: <david.demer@noaa.gov>).

# Acknowledgements {#acknowledgements}
We thank the crew members of NOAA Ship _`r survey.vessel`_, as well as the scientists and technicians that participated in the sampling operations at sea. Critical reviews by **Reviewer A** and **[Others?]** improved this report.  

# References {-}  

<div id = 'refs'></div>  

\newpage  

# (APPENDIX) Appendix {-}
# Appendix {-} 
# Centerboard positions {#appendix-cb-pos}

Date, time, and location associated with changes to the position of the centerboard and transducer depth (retracted ~5-m, intermediate ~7-m, extended ~9-m).

```{r cb-pos,results='asis'}
# get centerboard position records
cb.pos <- filter(bridge.snap, 
                 Button %in% c("Retracted (5 m)","Intermediate (7 m)","Extended (9 m)")) %>% 
  select(datetime, Button, Lat, Lon) %>% 
  rename(`Date Time` = datetime, `Position (depth)` = Button, 
         `Latitude (deg)` = Lat, `Longitude (deg)` = Lon)

if (doc.type == "docx") {
  # create kable object (for Word)
  regulartable(cb.pos)
} else {
  # print LaTeX table for HTML or PDF
  kable(cb.pos, align = c("l","l","c","c"), digits = c(0,0,4,4),
        escape = F, booktabs = T, linesep = "") %>% 
    kable_styling(position = "center",
                  latex_options = c("striped", "hold_position"))
}
```   

\newpage

# Echosounder transducer impedance measurements {#appendix-impedance-plots}  

The magnitude of impedance ($|Z|$, $\Omega$; panel a), phase ($\theta$, $^\circ$; panel b), and conductance ($G$, $S$; panel c) versus frequency, and susceptance ($B$, $S$) versus $G$ (admittance circle; panel d), for each transducer quadrant (various colors).

**Create impedance plots.**

```{r impedance-plots,out.height="7.5in",fig.pos='H', eval = FALSE}
if (process.zmux) {
  source(here("Code/zmux_plot.R"))
}

include_graphics(here("Figs/fig_impedance_plot_all.png"))
```

\newpage

# CTD and UCTD sampling locations {#appendix-ctd-sampling}

Times and locations of conductivity and temperature versus depth casts while on station (CTD) and underway (UCTD). **Use processed CTD/UCTD cast files to generate this table.**

```{r SampleSummTable,results='asis'}
# Rename
all.ctds.table <- all.ctds %>% 
  mutate(Button = str_replace(Button, " Cast","")) %>% 
  rename(`Date Time` = Date, `Cast Type` = Button,
         `Latitude (deg)` = Latitude, `Longitude (deg)` = Longitude)

if (doc.type == "docx") {
  # create flextable object (for Word)
  regulartable(all.ctds.table)
} else {
  # print LaTeX table for HTML or PDF
  kable(all.ctds.table,
        align = c("l","l","c","c"), 
        digits = c(0,0,4,4),
        escape = F, longtable = T, 
        booktabs = T) %>% 
    kable_styling(position = "center", 
                  latex_options = c("repeat_header","hold_position"))
}
```  

\newpage
\blandscape

# Trawl sample summary {#appendix-trawl-sampling}

Date, time, location at the start of trawling (i.e., at net equilibrium), and biomasses (kg) of CPS collected for each trawl haul.

```{r trawl-catch-summary,results='asis'}
# Summarize catch by species, used to subset columns
pos.cps <- trawl.summ %>% 
  select(-Haul, -Date, -Latitude, -Longitude, -"All CPS") %>% 
  gather(key = "scientificName", value = "weight") %>% 
  group_by(scientificName) %>% 
  summarise(weight = sum(weight, na.rm = T)) %>% 
  filter(weight != 0) %>% 
  pull(scientificName)

# Select only species with positive catch weight
trawl.summ <- trawl.summ %>% 
  select(Haul, `Date Time` = Date, `Latitude (deg)` = Latitude, 
         `Longitude (deg)` = Longitude,
         pos.cps, "All CPS") 

# Replace zeros with NA
trawl.summ[trawl.summ == 0] <- NA

if (doc.type == "docx") {
  # create kable object (for Word)
  regulartable(trawl.summ)
} else {
  # print LaTeX table for HTML or PDF
  kable(trawl.summ,escape = F,longtable = T,booktabs = T,
        align = c("r","c",rep("r",ncol(trawl.summ) - 2)),
        digits = c(0,0,4,4,rep(2, ncol(trawl.summ) - 4))) %>% 
    kable_styling(position = "center", 
                  latex_options = c("repeat_header","hold_position"))
}
```  

\elandscape
\newpage
